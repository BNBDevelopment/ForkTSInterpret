
@article{ye_time_2011,
	title = {Time series shapelets: a novel technique that allows accurate, interpretable and fast classification},
	volume = {22},
	issn = {1384-5810, 1573-756X},
	shorttitle = {Time series shapelets},
	url = {http://link.springer.com/10.1007/s10618-010-0179-5},
	doi = {10.1007/s10618-010-0179-5},
	language = {en},
	number = {1-2},
	urldate = {2021-10-20},
	journal = {Data Mining and Knowledge Discovery},
	author = {Ye, Lexiang and Keogh, Eamonn},
	month = jan,
	year = {2011},
	pages = {149--182},
	file = {Full Text:/home/jacqueline/Zotero/storage/BVU6EX2T/Ye and Keogh - 2011 - Time series shapelets a novel technique that allo.pdf:application/pdf},
}

@article{ye_time_2011-1,
	title = {Time series shapelets: a novel technique that allows accurate, interpretable and fast classification},
	volume = {22},
	issn = {1384-5810, 1573-756X},
	shorttitle = {Time series shapelets},
	url = {http://link.springer.com/10.1007/s10618-010-0179-5},
	doi = {10.1007/s10618-010-0179-5},
	language = {en},
	number = {1-2},
	urldate = {2021-10-20},
	journal = {Data Min Knowl Disc},
	author = {Ye, Lexiang and Keogh, Eamonn},
	month = jan,
	year = {2011},
	pages = {149--182},
	file = {Full Text:/home/jacqueline/Zotero/storage/8A9LDQVT/Ye and Keogh - 2011 - Time series shapelets a novel technique that allo.pdf:application/pdf},
}

@article{bagnall_great_2017,
	title = {The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances},
	volume = {31},
	issn = {1384-5810, 1573-756X},
	shorttitle = {The great time series classification bake off},
	url = {http://link.springer.com/10.1007/s10618-016-0483-9},
	doi = {10.1007/s10618-016-0483-9},
	language = {en},
	number = {3},
	urldate = {2021-10-20},
	journal = {Data Min Knowl Disc},
	author = {Bagnall, Anthony and Lines, Jason and Bostrom, Aaron and Large, James and Keogh, Eamonn},
	month = may,
	year = {2017},
	pages = {606--660},
	file = {Full Text:/home/jacqueline/Zotero/storage/5R7G9EKL/Bagnall et al. - 2017 - The great time series classification bake off a r.pdf:application/pdf},
}

@article{noauthor_great_2017,
	title = {The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances},
	volume = {31},
	issn = {1384-5810, 1573-756X},
	shorttitle = {The great time series classification bake off},
	url = {http://link.springer.com/10.1007/s10618-016-0483-9},
	doi = {10.1007/s10618-016-0483-9},
	language = {en},
	number = {3},
	urldate = {2021-10-20},
	journal = {Data Mining and Knowledge Discovery},
	month = may,
	year = {2017},
	pages = {606--660},
	file = {Full Text:/home/jacqueline/Zotero/storage/RWE79JQ6/2017 - The great time series classification bake off a r.pdf:application/pdf},
}

@article{alvarez-melis_towards_2018,
	title = {Towards {Robust} {Interpretability} with {Self}-{Explaining} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1806.07538},
	abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating \${\textbackslash}textit\{a posteriori\}\$ explanations for previously trained models around specific predictions. \${\textbackslash}textit\{Self-explaining\}\$ models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
	urldate = {2021-10-22},
	journal = {arXiv:1806.07538 [cs, stat]},
	author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
	month = dec,
	year = {2018},
	note = {arXiv: 1806.07538},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/EEV26GTD/Alvarez-Melis and Jaakkola - 2018 - Towards Robust Interpretability with Self-Explaini.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/8PZSGMZT/1806.html:text/html},
}

@article{mujkanovic_timexplain_2020,
	title = {{timeXplain} -- {A} {Framework} for {Explaining} the {Predictions} of {Time} {Series} {Classifiers}},
	url = {http://arxiv.org/abs/2007.07606},
	abstract = {Modern time series classifiers display impressive predictive capabilities, yet their decision-making processes mostly remain black boxes to the user. At the same time, model-agnostic explainers, such as the recently proposed SHAP, promise to make the predictions of machine learning models interpretable, provided there are well-designed domain mappings. We bring both worlds together in our timeXplain framework, extending the reach of explainable artificial intelligence to time series classification and value prediction. We present novel domain mappings for the time and the frequency domain as well as series statistics and analyze their explicative power as well as their limits. We employ timeXplain in a large-scale experimental comparison of several state-of-the-art time series classifiers and discover similarities between seemingly distinct classification concepts such as residual neural networks and elastic ensembles.},
	urldate = {2021-11-10},
	journal = {arXiv:2007.07606 [cs, stat]},
	author = {Mujkanovic, Felix and Doskoč, Vanja and Schirneck, Martin and Schäfer, Patrick and Friedrich, Tobias},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.07606},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/BNHQDI9D/Mujkanovic et al. - 2020 - timeXplain -- A Framework for Explaining the Predi.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/2I8UUE4Q/2007.html:text/html},
}

@inproceedings{guilleme_agnostic_2019,
	address = {Portland, OR, USA},
	title = {Agnostic {Local} {Explanation} for {Time} {Series} {Classification}},
	isbn = {978-1-72813-798-8},
	url = {https://ieeexplore.ieee.org/document/8995349/},
	doi = {10.1109/ICTAI.2019.00067},
	urldate = {2021-11-10},
	booktitle = {2019 {IEEE} 31st {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	publisher = {IEEE},
	author = {Guilleme, Mael and Masson, Veronique and Roze, Laurence and Termier, Alexandre},
	month = nov,
	year = {2019},
	pages = {432--439},
}

@article{schlegel_ts-mule_2021,
	title = {{TS}-{MULE}: {Local} {Interpretable} {Model}-{Agnostic} {Explanations} for {Time} {Series} {Forecast} {Models}},
	shorttitle = {{TS}-{MULE}},
	url = {http://arxiv.org/abs/2109.08438},
	abstract = {Time series forecasting is a demanding task ranging from weather to failure forecasting with black-box models achieving state-of-the-art performances. However, understanding and debugging are not guaranteed. We propose TS-MULE, a local surrogate model explanation method specialized for time series extending the LIME approach. Our extended LIME works with various ways to segment and perturb the time series data. In our extension, we present six sampling segmentation approaches for time series to improve the quality of surrogate attributions and demonstrate their performances on three deep learning model architectures and three common multivariate time series datasets.},
	urldate = {2021-11-10},
	journal = {arXiv:2109.08438 [cs]},
	author = {Schlegel, Udo and Lam, Duy Vo and Keim, Daniel A. and Seebacher, Daniel},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.08438},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/VUCC5SDQ/Schlegel et al. - 2021 - TS-MULE Local Interpretable Model-Agnostic Explan.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/99KNYEFJ/2109.html:text/html},
}

@article{parvatharaju_learning_2021,
	title = {Learning {Saliency} {Maps} to {Explain} {Deep} {Time} {Series} {Classifiers}},
	abstract = {Explainable classi�cation is essential to high-impact settings where practitioners require evidence to support their decisions. However, state-of-the-art deep learning models lack transparency in how they make their predictions. One increasingly popular solution is attribution-based explainability, which� nds the impact of input features on the model’s predictions. While this is popular for computer vision, little has been done to explain deep time series classi�ers. In this work, we study this problem and propose PERT, a novel perturbation-based explainability method designed to explain deep classi�ers’ decisions on time series. PERT extends beyond recent perturbation methods to generate a saliency map that assigns importance values to the timesteps of the instance-of-interest. First, PERT uses a novel Prioritized Replacement Selector to learn which alternative time series from a larger dataset are most useful to perform this perturbation. Second, PERT mixes the instance with the replacements using a Guided Perturbation Strategy, which learns to what degree each timestep can be perturbed without altering the classi�er’s� nal prediction. These two steps jointly learn to identify the fewest and most impactful timesteps that explain the classi�er’s prediction. We evaluate PERT using three metrics on nine popular datasets with two black-box models. We� nd that PERT consistently outperforms all� ve state-of-the-art methods. Using a case study, we also demonstrate that PERT succeeds in �nding the relevant regions of the input time series.},
	language = {en},
	author = {Parvatharaju, Prathyush S and Hartvigsen, Thomas and Doddaiah, Ramesh and Rundensteiner, Elke A},
	year = {2021},
	pages = {10},
	file = {3459637.3482446.pdf:/home/jacqueline/Downloads/3459637.3482446.pdf:application/pdf},
}

@book{oppenheim_discrete-time_2014,
	address = {Harlow},
	edition = {Third edition, Pearson new international edition},
	series = {Always learning},
	title = {Discrete-time signal processing},
	isbn = {978-1-292-02572-8},
	language = {eng},
	publisher = {Pearson Education Limited},
	author = {Oppenheim, Alan V. and Schafer, Ronald W.},
	year = {2014},
	file = {Discrete-Time Signal Processing:/home/jacqueline/Downloads/UploadFile_2230.pdf:application/pdf},
}

@article{harford_generating_2021,
	title = {Generating {Adversarial} {Samples} on {Multivariate} {Time} {Series} using {Variational} {Autoencoders}},
	volume = {8},
	issn = {2329-9266, 2329-9274},
	url = {https://ieeexplore.ieee.org/document/9479867/},
	doi = {10.1109/JAS.2021.1004108},
	abstract = {Classification models for multivariate time series have drawn the interest of many researchers to the field with the objective of developing accurate and efficient models. However, limited research has been conducted on generating adversarial samples for multivariate time series classification models. Adversarial samples could become a security concern in systems with complex sets of sensors. This study proposes extending the existing gradient adversarial transformation network (GATN) in combination with adversarial autoencoders to attack multivariate time series classification models. The proposed model attacks classification models by utilizing a distilled model to imitate the output of the multivariate time series classification model. In addition, the adversarial generator function is replaced with a variational autoencoder to enhance the adversarial samples. The developed methodology is tested on two multivariate time series classification models: 1-nearest neighbor dynamic time warping (1-NN DTW) and a fully convolutional network (FCN). This study utilizes 30 multivariate time series benchmarks provided by the University of East Anglia (UEA) and University of California Riverside (UCR). The use of adversarial autoencoders shows an increase in the fraction of successful adversaries generated on multivariate time series. To the best of our knowledge, this is the first study to explore adversarial attacks on multivariate time series. Additionally, we recommend future research utilizing the generated latent space from the variational autoencoders.},
	language = {en},
	number = {9},
	urldate = {2021-11-16},
	journal = {IEEE/CAA J. Autom. Sinica},
	author = {Harford, Samuel and Karim, Fazle and Darabi, Houshang},
	month = sep,
	year = {2021},
	pages = {1523--1538},
	file = {Generating_Adversarial_Samples_on_Multivariate_Time_Series_using_Variational_Autoencoders.pdf:/home/jacqueline/Downloads/Generating_Adversarial_Samples_on_Multivariate_Time_Series_using_Variational_Autoencoders.pdf:application/pdf},
}

@inproceedings{lin_anomaly_2020,
	address = {Barcelona, Spain},
	title = {Anomaly {Detection} for {Time} {Series} {Using} {VAE}-{LSTM} {Hybrid} {Model}},
	isbn = {978-1-5090-6631-5},
	url = {https://ieeexplore.ieee.org/document/9053558/},
	doi = {10.1109/ICASSP40776.2020.9053558},
	urldate = {2021-11-16},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Lin, Shuyu and Clark, Ronald and Birke, Robert and Schonborn, Sandro and Trigoni, Niki and Roberts, Stephen},
	month = may,
	year = {2020},
	pages = {4322--4326},
}

@article{fortuin_gp-vae_nodate,
	title = {{GP}-{VAE}: {Deep} {Probabilistic} {Multivariate} {Time} {Series} {Imputation}},
	abstract = {Multivariate time series with missing values are common in areas such as healthcare and ﬁnance, and have grown in number and complexity over the years. This raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain. However, naïve applications of deep learning fall short in giving reliable conﬁdence estimates and lack interpretability. We propose a new deep sequential latent variable model for dimensionality reduction and data imputation. Our modeling assumption is simple and interpretable: the high dimensional time series has a lower-dimensional representation which evolves smoothly in time according to a Gaussian process. The nonlinear dimensionality reduction in the presence of missing data is achieved using a VAE approach with a novel structured variational approximation. We demonstrate that our approach outperforms several classical and deep learning-based data imputation methods on high-dimensional data from the domains of computer vision and healthcare, while additionally improving the smoothness of the imputations and providing interpretable uncertainty estimates.},
	language = {en},
	author = {Fortuin, Vincent and Baranchuk, Dmitry and Rätsch, Gunnar and Mandt, Stephan},
	pages = {10},
	file = {Fortuin et al. - GP-VAE Deep Probabilistic Multivariate Time Serie.pdf:/home/jacqueline/Zotero/storage/GWWDV5SG/Fortuin et al. - GP-VAE Deep Probabilistic Multivariate Time Serie.pdf:application/pdf},
}

@article{niu_lstm-based_2020,
	title = {{LSTM}-{Based} {VAE}-{GAN} for {Time}-{Series} {Anomaly} {Detection}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/13/3738},
	doi = {10.3390/s20133738},
	abstract = {Time series anomaly detection is widely used to monitor the equipment sates through the data collected in the form of time series. At present, the deep learning method based on generative adversarial networks (GAN) has emerged for time series anomaly detection. However, this method needs to find the best mapping from real-time space to the latent space at the anomaly detection stage, which brings new errors and takes a long time. In this paper, we propose a long short-term memory-based variational autoencoder generation adversarial networks (LSTM-based VAE-GAN) method for time series anomaly detection, which effectively solves the above problems. Our method jointly trains the encoder, the generator and the discriminator to take advantage of the mapping ability of the encoder and the discrimination ability of the discriminator simultaneously. The long short-term memory (LSTM) networks are used as the encoder, the generator and the discriminator. At the anomaly detection stage, anomalies are detected based on reconstruction difference and discrimination results. Experimental results show that the proposed method can quickly and accurately detect anomalies.},
	language = {en},
	number = {13},
	urldate = {2021-11-16},
	journal = {Sensors},
	author = {Niu, Zijian and Yu, Ke and Wu, Xiaofei},
	month = jul,
	year = {2020},
	pages = {3738},
	file = {Full Text:/home/jacqueline/Zotero/storage/FE2GI9AZ/Niu et al. - 2020 - LSTM-Based VAE-GAN for Time-Series Anomaly Detecti.pdf:application/pdf},
}

@article{li_stacking_2021,
	title = {Stacking {VAE} with {Graph} {Neural} {Networks} for {Effective} and {Interpretable} {Time} {Series} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2105.08397},
	abstract = {In real-world maintenance applications, deep generative models have shown promising performance in detecting anomalous events of entities from time-series signals collected from multiple sensors. Nevertheless, we outline two important challenges of leveraging such models for times-series anomaly detection: 1) developing effective and efficient reconstruction models and 2) exploiting the similarity and interrelation structures among the multivariate time series data channels. To address these challenges, in this paper we propose a stacking variational auto-encoder (VAE) model with graph neural networks for the effective and interpretable time-series anomaly detection. Specifically, we propose a stacking block-wise reconstruction framework with a weight-sharing scheme for the multivariate time series data with similarities among channels. Moreover, with a graph learning module, our model learns a sparse adjacency matrix to explicitly capture the stable interrelation structure information among multiple time series data channels for interpretable reconstruction of series patterns. Experimental results show that our proposed model outperforms the strong baselines on three public datasets with considerable improvements and meanwhile still maintains the training efficiency. Furthermore, we demonstrate that the intuitive stable structure learned by our model significantly improves the interpretability of our detection results.},
	urldate = {2021-11-16},
	journal = {arXiv:2105.08397 [cs]},
	author = {Li, Wenkai and Hu, Wenbo and Chen, Ning and Feng, Cheng},
	month = may,
	year = {2021},
	note = {arXiv: 2105.08397},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/JDIZSFCE/Li et al. - 2021 - Stacking VAE with Graph Neural Networks for Effect.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/C4K2PTGV/2105.html:text/html},
}

@article{li_learning_nodate,
	title = {Learning from {Irregularly}-{Sampled} {Time} {Series}: {A} {Missing} {Data} {Perspective}},
	abstract = {Irregularly-sampled time series occur in many domains including healthcare. They can be challenging to model because they do not naturally yield a ﬁxed-dimensional representation as required by many standard machine learning models. In this paper, we consider irregular sampling from the perspective of missing data. We model observed irregularly-sampled time series data as a sequence of index-value pairs sampled from a continuous but unobserved function. We introduce an encoder-decoder framework for learning from such generic indexed sequences. We propose learning methods for this framework based on variational autoencoders and generative adversarial networks. For continuous irregularly-sampled time series, we introduce continuous convolutional layers that can efﬁciently interface with existing neural network architectures. Experiments show that our models are able to achieve competitive or better classiﬁcation results on irregularlysampled multivariate time series compared to recent RNN models while offering signiﬁcantly faster training times.},
	language = {en},
	author = {Li, Steven Cheng-Xian and Marlin, Benjamin M},
	pages = {10},
	file = {Li and Marlin - Learning from Irregularly-Sampled Time Series A M.pdf:/home/jacqueline/Zotero/storage/SEXHWBMU/Li and Marlin - Learning from Irregularly-Sampled Time Series A M.pdf:application/pdf},
}

@article{esteban_real-valued_2017,
	title = {Real-valued ({Medical}) {Time} {Series} {Generation} with {Recurrent} {Conditional} {GANs}},
	url = {http://arxiv.org/abs/1706.02633},
	abstract = {Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data. In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. RGANs make use of recurrent neural networks in the generator and the discriminator. In the case of RCGANs, both of these RNNs are conditioned on auxiliary information. We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series. We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa. We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data. This is demonstrated on digit classification from 'serialised' MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit. We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data.},
	urldate = {2021-11-16},
	journal = {arXiv:1706.02633 [cs, stat]},
	author = {Esteban, Cristóbal and Hyland, Stephanie L. and Rätsch, Gunnar},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.02633},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/AT959R9W/Esteban et al. - 2017 - Real-valued (Medical) Time Series Generation with .pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/U9G5CA9M/1706.html:text/html},
}

@article{ismail_benchmarking_2020,
	title = {Benchmarking {Deep} {Learning} {Interpretability} in {Time} {Series} {Predictions}},
	url = {http://arxiv.org/abs/2010.13924},
	abstract = {Saliency methods are used extensively to highlight the importance of input features in model predictions. These methods are mostly used in vision and language tasks, and their applications to time series data is relatively unexplored. In this paper, we set out to extensively compare the performance of various saliency-based interpretability methods across diverse neural architectures, including Recurrent Neural Network, Temporal Convolutional Networks, and Transformers in a new benchmark of synthetic time series data. We propose and report multiple metrics to empirically evaluate the performance of saliency methods for detecting feature importance over time using both precision (i.e., whether identified features contain meaningful signals) and recall (i.e., the number of features with signal identified as important). Through several experiments, we show that (i) in general, network architectures and saliency methods fail to reliably and accurately identify feature importance over time in time series data, (ii) this failure is mainly due to the conflation of time and feature domains, and (iii) the quality of saliency maps can be improved substantially by using our proposed two-step temporal saliency rescaling (TSR) approach that first calculates the importance of each time step before calculating the importance of each feature at a time step.},
	urldate = {2021-11-24},
	journal = {arXiv:2010.13924 [cs, stat]},
	author = {Ismail, Aya Abdelsalam and Gunady, Mohamed and Bravo, Héctor Corrada and Feizi, Soheil},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.13924},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/E78XLXRN/Ismail et al. - 2020 - Benchmarking Deep Learning Interpretability in Tim.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/HVC8SWGT/2010.html:text/html},
}

@article{siddiqui_tsinsight_2021,
	title = {{TSInsight}: {A} {Local}-{Global} {Attribution} {Framework} for {Interpretability} in {Time} {Series} {Data}},
	volume = {21},
	issn = {1424-8220},
	shorttitle = {{TSInsight}},
	url = {https://www.mdpi.com/1424-8220/21/21/7373},
	doi = {10.3390/s21217373},
	abstract = {With the rise in the employment of deep learning methods in safety-critical scenarios, interpretability is more essential than ever before. Although many different directions regarding interpretability have been explored for visual modalities, time series data has been neglected, with only a handful of methods tested due to their poor intelligibility. We approach the problem of interpretability in a novel way by proposing TSInsight, where we attach an auto-encoder to the classifier with a sparsity-inducing norm on its output and fine-tune it based on the gradients from the classifier and a reconstruction penalty. TSInsight learns to preserve features that are important for prediction by the classifier and suppresses those that are irrelevant, i.e., serves as a feature attribution method to boost the interpretability. In contrast to most other attribution frameworks, TSInsight is capable of generating both instance-based and model-based explanations. We evaluated TSInsight along with nine other commonly used attribution methods on eight different time series datasets to validate its efficacy. The evaluation results show that TSInsight naturally achieves output space contraction; therefore, it is an effective tool for the interpretability of deep time series models.},
	language = {en},
	number = {21},
	urldate = {2021-11-24},
	journal = {Sensors},
	author = {Siddiqui, Shoaib Ahmed and Mercier, Dominique and Dengel, Andreas and Ahmed, Sheraz},
	month = nov,
	year = {2021},
	pages = {7373},
	file = {Full Text:/home/jacqueline/Zotero/storage/96HLX69V/Siddiqui et al. - 2021 - TSInsight A Local-Global Attribution Framework fo.pdf:application/pdf},
}

@article{moraffah_causal_2020,
	title = {Causal {Interpretability} for {Machine} {Learning} -- {Problems}, {Methods} and {Evaluation}},
	url = {http://arxiv.org/abs/2003.03934},
	abstract = {Machine learning models have had discernible achievements in a myriad of applications. However, most of these models are black-boxes, and it is obscure how the decisions are made by them. This makes the models unreliable and untrustworthy. To provide insights into the decision making processes of these models, a variety of traditional interpretable models have been proposed. Moreover, to generate more human-friendly explanations, recent work on interpretability tries to answer questions related to causality such as "Why does this model makes such decisions?" or "Was it a specific feature that caused the decision made by the model?". In this work, models that aim to answer causal questions are referred to as causal interpretable models. The existing surveys have covered concepts and methodologies of traditional interpretability. In this work, we present a comprehensive survey on causal interpretable models from the aspects of the problems and methods. In addition, this survey provides in-depth insights into the existing evaluation metrics for measuring interpretability, which can help practitioners understand for what scenarios each evaluation metric is suitable.},
	urldate = {2021-12-02},
	journal = {arXiv:2003.03934 [cs, stat]},
	author = {Moraffah, Raha and Karami, Mansooreh and Guo, Ruocheng and Raglin, Adrienne and Liu, Huan},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.03934},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/HAZKBW3C/Moraffah et al. - 2020 - Causal Interpretability for Machine Learning -- Pr.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/VYP7NUSR/2003.html:text/html},
}

@incollection{sanchez-ruiz_instance-based_2021,
	address = {Cham},
	title = {Instance-{Based} {Counterfactual} {Explanations} for {Time} {Series} {Classification}},
	volume = {12877},
	isbn = {978-3-030-86956-4 978-3-030-86957-1},
	url = {https://link.springer.com/10.1007/978-3-030-86957-1_3},
	abstract = {In recent years, there has been a rapidly expanding focus on explaining the predictions made by black-box AI systems that handle image and tabular data. However, considerably less attention has been paid to explaining the predictions of opaque AI systems handling time series data. In this paper, we advance a novel model-agnostic, case-based technique – Native Guide – that generates counterfactual explanations for time series classiﬁers. Given a query time series, Tq, for which a black-box classiﬁcation system predicts class, c, a counterfactual time series explanation shows how Tq could change, such that the system predicts an alternative class, c . The proposed instance-based technique adapts existing counterfactual instances in the case-base by highlighting and modifying discriminative areas of the time series that underlie the classiﬁcation. Quantitative and qualitative results from two comparative experiments indicate that Native Guide generates plausible, proximal, sparse and diverse explanations that are better than those produced by key benchmark counterfactual methods.},
	language = {en},
	urldate = {2022-03-14},
	booktitle = {Case-{Based} {Reasoning} {Research} and {Development}},
	publisher = {Springer International Publishing},
	author = {Delaney, Eoin and Greene, Derek and Keane, Mark T.},
	editor = {Sánchez-Ruiz, Antonio A. and Floyd, Michael W.},
	year = {2021},
	doi = {10.1007/978-3-030-86957-1_3},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {32--47},
	file = {Delaney et al. - 2021 - Instance-Based Counterfactual Explanations for Tim.pdf:/home/jacqueline/Zotero/storage/XIAIBPNB/Delaney et al. - 2021 - Instance-Based Counterfactual Explanations for Tim.pdf:application/pdf},
}

@inproceedings{keane_if_2021,
	address = {Montreal, Canada},
	title = {If {Only} {We} {Had} {Better} {Counterfactual} {Explanations}: {Five} {Key} {Deficits} to {Rectify} in the {Evaluation} of {Counterfactual} {XAI} {Techniques}},
	isbn = {978-0-9992411-9-6},
	shorttitle = {If {Only} {We} {Had} {Better} {Counterfactual} {Explanations}},
	url = {https://www.ijcai.org/proceedings/2021/609},
	doi = {10.24963/ijcai.2021/609},
	abstract = {In recent years, there has been an explosion of AI research on counterfactual explanations as a solution to the problem of eXplainable AI (XAI). These explanations seem to offer technical, psychological and legal benefits over other explanation techniques. We survey 100 distinct counterfactual explanation methods reported in the literature. This survey addresses the extent to which these methods have been adequately evaluated, both psychologically and computationally, and quantifies the shortfalls occurring. For instance, only 21\% of these methods have been user tested. Five key deficits in the evaluation of these methods are detailed and a roadmap, with standardized benchmark evaluations, is proposed to resolve the issues arising; issues, that currently effectively block scientific progress in this field.},
	language = {en},
	urldate = {2022-03-14},
	booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Keane, Mark T. and Kenny, Eoin M. and Delaney, Eoin and Smyth, Barry},
	month = aug,
	year = {2021},
	pages = {4466--4474},
	file = {Keane et al. - 2021 - If Only We Had Better Counterfactual Explanations.pdf:/home/jacqueline/Zotero/storage/YP2DURB2/Keane et al. - 2021 - If Only We Had Better Counterfactual Explanations.pdf:application/pdf},
}

@incollection{soares_learning_2021,
	address = {Cham},
	title = {Learning {Time} {Series} {Counterfactuals} via {Latent} {Space} {Representations}},
	volume = {12986},
	isbn = {978-3-030-88941-8 978-3-030-88942-5},
	url = {https://link.springer.com/10.1007/978-3-030-88942-5_29},
	abstract = {Counterfactual explanations can provide sample-based explanations of features required to modify from the original sample to change the classiﬁcation result from an undesired state to a desired state; hence it provides interpretability of the model. Previous work of LatentCF presents an algorithm for image data that employs autoencoder models to directly transform original samples into counterfactuals in a latent space representation. In our paper, we adapt the approach to time series classiﬁcation and propose an improved algorithm named LatentCF++ which introduces additional constraints in the counterfactual generation process. We conduct an extensive experiment on a total of 40 datasets from the UCR archive, comparing to current state-of-the-art methods. Based on our evaluation metrics, we show that the LatentCF++ framework can with high probability generate valid counterfactuals and achieve comparable explanations to current state-of-the-art. Our proposed approach can also generate counterfactuals that are considerably closer to the decision boundary in terms of margin diﬀerence.},
	language = {en},
	urldate = {2022-03-14},
	booktitle = {Discovery {Science}},
	publisher = {Springer International Publishing},
	author = {Wang, Zhendong and Samsten, Isak and Mochaourab, Rami and Papapetrou, Panagiotis},
	editor = {Soares, Carlos and Torgo, Luis},
	year = {2021},
	doi = {10.1007/978-3-030-88942-5_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {369--384},
	file = {Wang et al. - 2021 - Learning Time Series Counterfactuals via Latent Sp.pdf:/home/jacqueline/Zotero/storage/QZ22RWCM/Wang et al. - 2021 - Learning Time Series Counterfactuals via Latent Sp.pdf:application/pdf},
}

@article{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://arxiv.org/abs/1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	language = {en},
	urldate = {2022-03-23},
	journal = {arXiv:1602.04938 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv: 1602.04938},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:/home/jacqueline/Zotero/storage/E4W5IXKZ/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf},
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable.},
	language = {en},
	number = {2},
	urldate = {2022-03-23},
	journal = {Int J Comput Vis},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	note = {arXiv: 1610.02391},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	pages = {336--359},
	file = {Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:/home/jacqueline/Zotero/storage/36YHTMDN/Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:application/pdf},
}

@article{lundberg_unified_nodate,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a uniﬁed framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identiﬁcation of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class uniﬁes six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this uniﬁcation, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	language = {en},
	author = {Lundberg, Scott M and Lee, Su-In},
	pages = {10},
	file = {Lundberg and Lee - A Unified Approach to Interpreting Model Predictio.pdf:/home/jacqueline/Zotero/storage/YWXAF93Y/Lundberg and Lee - A Unified Approach to Interpreting Model Predictio.pdf:application/pdf},
}

@article{deb_introduction_1999,
	title = {An introduction to genetic algorithms},
	volume = {24},
	issn = {0256-2499, 0973-7677},
	url = {http://link.springer.com/10.1007/BF02823145},
	doi = {10.1007/BF02823145},
	language = {en},
	number = {4-5},
	urldate = {2022-03-24},
	journal = {Sadhana},
	author = {Deb, Kalyanmoy},
	month = aug,
	year = {1999},
	pages = {293--315},
}

@article{deb_fast_2002,
	title = {A fast and elitist multiobjective genetic algorithm: {NSGA}-{II}},
	volume = {6},
	issn = {1089778X},
	shorttitle = {A fast and elitist multiobjective genetic algorithm},
	url = {http://ieeexplore.ieee.org/document/996017/},
	doi = {10.1109/4235.996017},
	number = {2},
	urldate = {2022-03-24},
	journal = {IEEE Trans. Evol. Computat.},
	author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
	month = apr,
	year = {2002},
	pages = {182--197},
}

@article{ishibuchi_performance_2017,
	title = {Performance of {Decomposition}-{Based} {Many}-{Objective} {Algorithms} {Strongly} {Depends} on {Pareto} {Front} {Shapes}},
	volume = {21},
	issn = {1089-778X, 1089-778X, 1941-0026},
	url = {http://ieeexplore.ieee.org/document/7509682/},
	doi = {10.1109/TEVC.2016.2587749},
	number = {2},
	urldate = {2022-03-24},
	journal = {IEEE Trans. Evol. Computat.},
	author = {Ishibuchi, Hisao and Setoguchi, Yu and Masuda, Hiroyuki and Nojima, Yusuke},
	month = apr,
	year = {2017},
	pages = {169--190},
}

@article{rojat_explainable_2021,
	title = {Explainable {Artificial} {Intelligence} ({XAI}) on {TimeSeries} {Data}: {A} {Survey}},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI}) on {TimeSeries} {Data}},
	url = {http://arxiv.org/abs/2104.00950},
	abstract = {Most of state of the art methods applied on time series consist of deep learning methods that are too complex to be interpreted. This lack of interpretability is a major drawback, as several applications in the real world are critical tasks, such as the medical field or the autonomous driving field. The explainability of models applied on time series has not gather much attention compared to the computer vision or the natural language processing fields. In this paper, we present an overview of existing explainable AI (XAI) methods applied on time series and illustrate the type of explanations they produce. We also provide a reflection on the impact of these explanation methods to provide confidence and trust in the AI systems.},
	urldate = {2022-03-24},
	journal = {arXiv:2104.00950 [cs]},
	author = {Rojat, Thomas and Puget, Raphaël and Filliat, David and Del Ser, Javier and Gelin, Rodolphe and Díaz-Rodríguez, Natalia},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.00950},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/NCLP4BG7/Rojat et al. - 2021 - Explainable Artificial Intelligence (XAI) on TimeS.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/IDK7WGVI/2104.html:text/html},
}

@article{miller_explanation_2018,
	title = {Explanation in {Artificial} {Intelligence}: {Insights} from the {Social} {Sciences}},
	shorttitle = {Explanation in {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/1706.07269},
	abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	urldate = {2022-03-24},
	journal = {arXiv:1706.07269 [cs]},
	author = {Miller, Tim},
	month = aug,
	year = {2018},
	note = {arXiv: 1706.07269},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/6WPV2N8M/Miller - 2018 - Explanation in Artificial Intelligence Insights f.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/39C4T9YB/1706.html:text/html},
}

@article{nguyen_interpretable_2020,
	title = {Interpretable {Time} {Series} {Classification} using {Linear} {Models} and {Multi}-resolution {Multi}-domain {Symbolic} {Representations}},
	url = {http://arxiv.org/abs/2006.01667},
	abstract = {The time series classiﬁcation literature has expanded rapidly over the last decade, with many new classiﬁcation approaches published each year. Prior research has mostly focused on improving the accuracy and eﬃciency of classiﬁers, with interpretability being somewhat neglected. This aspect of classiﬁers has become critical for many application domains and the introduction of the EU GDPR legislation in 2018 is likely to further emphasize the importance of interpretable learning algorithms. Currently, state-of-the-art classiﬁcation accuracy is achieved with very complex models based on large ensembles (COTE) or deep neural networks (FCN). These approaches are not eﬃcient with regard to either time or space, are diﬃcult to interpret and cannot be applied to variable-length time series, requiring pre-processing of the original series to a set ﬁxedlength. In this paper we propose new time series classiﬁcation algorithms to address these gaps. Our approach is based on symbolic representations of time series, eﬃcient sequence mining algorithms and linear classiﬁcation models. Our linear models are as accurate as deep learning models but are more eﬃcient regarding running time and memory, can work with variable-length time series and can be interpreted by highlighting the discriminative symbolic features on the original time series. We advance the state-of-the-art in time series classiﬁcation by proposing new algorithms built using the following three key ideas: (1) Multiple resolutions of symbolic representations: we combine symbolic representations obtained using diﬀerent parameters, rather than one ﬁxed representation (e.g., multiple SAX representations); (2) Multiple domain representations: we combine symbolic representations in time (e.g., SAX) and frequency (e.g., SFA) domains, to be more robust across problem types; (3) Eﬃcient navigation in a huge symbolic-words space: we extend a symbolic sequence classiﬁer (SEQL) to work with multiple symbolic representations and use its greedy feature selection strategy to eﬀectively ﬁlter the best features for each representation. We show that our multi-resolution multi-domain linear classiﬁer (mtSS-SEQL+LR) achieves a similar accuracy to the state-of-the-art COTE ensemble, and to recent deep learning methods (FCN, ResNet), but uses a fraction of the time and memory required by either COTE or deep models. To further analyse the interpretability of our classiﬁer, we present a case study on a human motion dataset collected by the authors. We discuss the accuracy, eﬃciency and interpretability of our proposed algorithms and release all the results, source code and data to encourage reproducibility.},
	language = {en},
	urldate = {2022-03-24},
	journal = {arXiv:2006.01667 [cs, eess, stat]},
	author = {Nguyen, Thach Le and Gsponer, Severin and Ilie, Iulia and O'Reilly, Martin and Ifrim, Georgiana},
	month = may,
	year = {2020},
	note = {arXiv: 2006.01667},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	file = {Nguyen et al. - 2020 - Interpretable Time Series Classification using Lin.pdf:/home/jacqueline/Zotero/storage/3UXE8IYA/Nguyen et al. - 2020 - Interpretable Time Series Classification using Lin.pdf:application/pdf},
}

@article{fouladgar_metrics_2022,
	title = {Metrics and {Evaluations} of {Time} {Series} {Explanations}: {An} {Application} in {Affect} {Computing}},
	volume = {10},
	abstract = {Explainable artiﬁcial intelligence (XAI) has shed light on enormous applications by clarifying why neural models make speciﬁc decisions. However, it remains challenging to measure how sensitive XAI solutions are to the explanations of neural models. Although different evaluation metrics have been proposed to measure sensitivity, the main focus has been on the visual and textual data. There is insufﬁcient attention devoted to the sensitivity metrics tailored for time series data. In this paper, we formulate several metrics, including max short-term sensitivity (MSS), max long-term sensitivity (MLS), average short-term sensitivity (ASS) and average long-term sensitivity (ALS), that target the sensitivity of XAI models with respect to the generated and real time series. Our hypothesis is that for close series with the same labels, we obtain similar explanations. We evaluate three XAI models, LIME, integrated gradient (IG), and SmoothGrad (SG), on CN-Waterfall, a deep convolutional network. This network is a highly accurate time series classiﬁer in affect computing. Our experiments rely on data-, metric- and XAI hyperparameter-related settings on the WESAD and MAHNOB-HCI datasets. The results reveal that (i) IG and LIME provide a lower sensitivity scale than SG in all the metrics and settings, potentially due to the lower scale of important scores generated by IG and LIME, (ii) the XAI models show higher sensitivities for a smaller window of data, (iii) the sensitivities of XAI models ﬂuctuate when the network parameters and data properties change, and (iv) the XAI models provide unstable sensitivities under different settings of hyperparameters.},
	language = {en},
	author = {Fouladgar, Nazanin and Alirezaie, Marjan and Främling, Kary},
	year = {2022},
	pages = {15},
	file = {Fouladgar et al. - 2022 - Metrics and Evaluations of Time Series Explanation.pdf:/home/jacqueline/Zotero/storage/2IYACWBX/Fouladgar et al. - 2022 - Metrics and Evaluations of Time Series Explanation.pdf:application/pdf},
}

@article{ye_time_nodate,
	title = {Time series shapelets: a novel technique that allows accurate, interpretable and fast classiﬁcation},
	abstract = {Classiﬁcation of time series has been attracting great interest over the past decade. While dozens of techniques have been introduced, recent empirical evidence has strongly suggested that the simple nearest neighbor algorithm is very difﬁcult to beat for most time series problems, especially for large-scale datasets. While this may be considered good news, given the simplicity of implementing the nearest neighbor algorithm, there are some negative consequences of this. First, the nearest neighbor algorithm requires storing and searching the entire dataset, resulting in a high time and space complexity that limits its applicability, especially on resource-limited sensors. Second, beyond mere classiﬁcation accuracy, we often wish to gain some insight into the data and to make the classiﬁcation result more explainable, which global characteristics of the nearest neighbor cannot provide. In this work we introduce a new time series primitive, time series shapelets, which addresses these limitations. Informally, shapelets are time series subsequences which are in some sense maximally representative of a class. We can use the distance to the shapelet, rather than the distance to the nearest neighbor to classify objects. As we shall show with extensive empirical evaluations in diverse domains, classiﬁcation algorithms based on the time series shapelet primitives can be interpretable, more accurate, and significantly faster than state-of-the-art classiﬁers.},
	language = {en},
	author = {Ye, Lexiang and Keogh, Eamonn},
	pages = {34},
	file = {Ye and Keogh - Time series shapelets a novel technique that allo.pdf:/home/jacqueline/Zotero/storage/9636FX42/Ye and Keogh - Time series shapelets a novel technique that allo.pdf:application/pdf},
}

@article{siddiqui_tsviz_2019,
	title = {{TSViz}: {Demystification} of {Deep} {Learning} {Models} for {Time}-{Series} {Analysis}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {{TSViz}},
	url = {http://arxiv.org/abs/1802.02952},
	doi = {10.1109/ACCESS.2019.2912823},
	abstract = {This paper presents a novel framework for demystiﬁcation of convolutional deep learning models for time-series analysis. This is a step towards making informed/explainable decisions in the domain of time-series, powered by deep learning. There have been numerous efforts to increase the interpretability of image-centric deep neural network models, where the learned features are more intuitive to visualize. Visualization in time-series domain is much more complicated as there is no direct interpretation of the ﬁlters and inputs as compared to the image modality. In addition, little or no concentration has been devoted for the development of such tools in the domain of time-series in the past. TSViz provides possibilities to explore and analyze a network from different dimensions at different levels of abstraction which includes identiﬁcation of parts of the input that were responsible for a prediction (including per ﬁlter saliency), importance of different ﬁlters present in the network for a particular prediction, notion of diversity present in the network through ﬁlter clustering, understanding of the main sources of variation learnt by the network through inverse optimization, and analysis of the network’s robustness against adversarial noise. As a sanity check for the computed inﬂuence values, we demonstrate results regarding pruning of neural networks based on the computed inﬂuence information. These representations allow to understand the network features so that the acceptability of deep networks for time-series data can be enhanced. This is extremely important in domains like ﬁnance, industry 4.0, self-driving cars, health-care, counter-terrorism etc., where reasons for reaching a particular prediction are equally important as the prediction itself. We assess the proposed framework for interpretability with a set of desirable properties essential for any method in this direction.},
	language = {en},
	urldate = {2022-03-25},
	journal = {IEEE Access},
	author = {Siddiqui, Shoaib Ahmed and Mercier, Dominik and Munir, Mohsin and Dengel, Andreas and Ahmed, Sheraz},
	year = {2019},
	note = {arXiv: 1802.02952},
	keywords = {Computer Science - Machine Learning, Computer Science - Human-Computer Interaction},
	pages = {67027--67040},
	file = {Siddiqui et al. - 2019 - TSViz Demystification of Deep Learning Models for.pdf:/home/jacqueline/Zotero/storage/VFD5WXLH/Siddiqui et al. - 2019 - TSViz Demystification of Deep Learning Models for.pdf:application/pdf},
}

@incollection{back_multi-objective_2020,
	address = {Cham},
	title = {Multi-{Objective} {Counterfactual} {Explanations}},
	volume = {12269},
	isbn = {978-3-030-58111-4 978-3-030-58112-1},
	url = {http://link.springer.com/10.1007/978-3-030-58112-1_31},
	language = {en},
	urldate = {2022-03-25},
	booktitle = {Parallel {Problem} {Solving} from {Nature} – {PPSN} {XVI}},
	publisher = {Springer International Publishing},
	author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
	editor = {Bäck, Thomas and Preuss, Mike and Deutz, André and Wang, Hao and Doerr, Carola and Emmerich, Michael and Trautmann, Heike},
	year = {2020},
	doi = {10.1007/978-3-030-58112-1_31},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {448--469},
	file = {Full Text:/home/jacqueline/Zotero/storage/DXU2MP5F/Dandl et al. - 2020 - Multi-Objective Counterfactual Explanations.pdf:application/pdf},
}

@article{wachter_counterfactual_2018,
	title = {Counterfactual {Explanations} without {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	shorttitle = {Counterfactual {Explanations} without {Opening} the {Black} {Box}},
	url = {http://arxiv.org/abs/1711.00399},
	abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
	urldate = {2022-03-25},
	journal = {arXiv:1711.00399 [cs]},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	month = mar,
	year = {2018},
	note = {arXiv: 1711.00399},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/QIIKBHHP/Wachter et al. - 2018 - Counterfactual Explanations without Opening the Bl.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/5DJCHTIG/1711.html:text/html},
}

@incollection{oliver_interpretable_2021,
	address = {Cham},
	title = {Interpretable {Counterfactual} {Explanations} {Guided} by {Prototypes}},
	volume = {12976},
	isbn = {978-3-030-86519-1 978-3-030-86520-7},
	url = {https://link.springer.com/10.1007/978-3-030-86520-7_40},
	language = {en},
	urldate = {2022-03-25},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}. {Research} {Track}},
	publisher = {Springer International Publishing},
	author = {Van Looveren, Arnaud and Klaise, Janis},
	editor = {Oliver, Nuria and Pérez-Cruz, Fernando and Kramer, Stefan and Read, Jesse and Lozano, Jose A.},
	year = {2021},
	doi = {10.1007/978-3-030-86520-7_40},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {650--665},
	file = {Submitted Version:/home/jacqueline/Zotero/storage/H745BMFX/Van Looveren and Klaise - 2021 - Interpretable Counterfactual Explanations Guided b.pdf:application/pdf},
}

@article{goyal_counterfactual_2019,
	title = {Counterfactual {Visual} {Explanations}},
	url = {http://arxiv.org/abs/1904.07451},
	abstract = {In this work, we develop a technique to produce counterfactual visual explanations. Given a 'query' image \$I\$ for which a vision system predicts class \$c\$, a counterfactual visual explanation identifies how \$I\$ could change such that the system would output a different specified class \$c'\$. To do this, we select a 'distractor' image \$I'\$ that the system predicts as class \$c'\$ and identify spatial regions in \$I\$ and \$I'\$ such that replacing the identified region in \$I\$ with the identified region in \$I'\$ would push the system towards classifying \$I\$ as \$c'\$. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples.},
	urldate = {2022-03-25},
	journal = {arXiv:1904.07451 [cs, stat]},
	author = {Goyal, Yash and Wu, Ziyan and Ernst, Jan and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
	month = jun,
	year = {2019},
	note = {arXiv: 1904.07451},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/5KXZHKTT/Goyal et al. - 2019 - Counterfactual Visual Explanations.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/RIYYCYIB/1904.html:text/html},
}

@article{ates_counterfactual_2021,
	title = {Counterfactual {Explanations} for {Machine} {Learning} on {Multivariate} {Time} {Series} {Data}},
	url = {http://arxiv.org/abs/2008.10781},
	doi = {10.1109/ICAPAI49758.2021.9462056},
	abstract = {Applying machine learning (ML) on multivariate time series data has growing popularity in many application domains, including in computer system management. For example, recent high performance computing (HPC) research proposes a variety of ML frameworks that use system telemetry data in the form of multivariate time series so as to detect performance variations, perform intelligent scheduling or node allocation, and improve system security. Common barriers for adoption for these ML frameworks include the lack of user trust and the difficulty of debugging. These barriers need to be overcome to enable the widespread adoption of ML frameworks in production systems. To address this challenge, this paper proposes a novel explainability technique for providing counterfactual explanations for supervised ML frameworks that use multivariate time series data. The proposed method outperforms state-of-the-art explainability methods on several different ML frameworks and data sets in metrics such as faithfulness and robustness. The paper also demonstrates how the proposed method can be used to debug ML frameworks and gain a better understanding of HPC system telemetry data.},
	urldate = {2022-03-25},
	journal = {2021 International Conference on Applied Artificial Intelligence (ICAPAI)},
	author = {Ates, Emre and Aksar, Burak and Leung, Vitus J. and Coskun, Ayse K.},
	month = may,
	year = {2021},
	note = {arXiv: 2008.10781},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {1--8},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/T26TYK8H/Ates et al. - 2021 - Counterfactual Explanations for Machine Learning o.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/AWI6C3C4/2008.html:text/html},
}

@article{dau_ucr_2019,
	title = {The {UCR} {Time} {Series} {Archive}},
	url = {http://arxiv.org/abs/1810.07758},
	abstract = {The UCR Time Series Archive - introduced in 2002, has become an important resource in the time series data mining community, with at least one thousand published papers making use of at least one data set from the archive. The original incarnation of the archive had sixteen data sets but since that time, it has gone through periodic expansions. The last expansion took place in the summer of 2015 when the archive grew from 45 to 85 data sets. This paper introduces and will focus on the new data expansion from 85 to 128 data sets. Beyond expanding this valuable resource, this paper offers pragmatic advice to anyone who may wish to evaluate a new algorithm on the archive. Finally, this paper makes a novel and yet actionable claim: of the hundreds of papers that show an improvement over the standard baseline (1-nearest neighbor classiﬁcation), a large fraction may be mis-attributing the reasons for their improvement. Moreover, they may have been able to achieve the same improvement with a much simpler modiﬁcation, requiring just a single line of code.},
	language = {en},
	urldate = {2022-03-27},
	journal = {arXiv:1810.07758 [cs, stat]},
	author = {Dau, Hoang Anh and Bagnall, Anthony and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and Keogh, Eamonn},
	month = sep,
	year = {2019},
	note = {arXiv: 1810.07758},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Dau et al. - 2019 - The UCR Time Series Archive.pdf:/home/jacqueline/Zotero/storage/SRJAUP6X/Dau et al. - 2019 - The UCR Time Series Archive.pdf:application/pdf},
}

@article{bagnall_uea_2018,
	title = {The {UEA} multivariate time series classification archive, 2018},
	url = {http://arxiv.org/abs/1811.00075},
	abstract = {In 2002, the UCR time series classification archive was first released with sixteen datasets. It gradually expanded, until 2015 when it increased in size from 45 datasets to 85 datasets. In October 2018 more datasets were added, bringing the total to 128. The new archive contains a wide range of problems, including variable length series, but it still only contains univariate time series classification problems. One of the motivations for introducing the archive was to encourage researchers to perform a more rigorous evaluation of newly proposed time series classification (TSC) algorithms. It has worked: most recent research into TSC uses all 85 datasets to evaluate algorithmic advances. Research into multivariate time series classification, where more than one series are associated with each class label, is in a position where univariate TSC research was a decade ago. Algorithms are evaluated using very few datasets and claims of improvement are not based on statistical comparisons. We aim to address this problem by forming the first iteration of the MTSC archive, to be hosted at the website www.timeseriesclassification.com. Like the univariate archive, this formulation was a collaborative effort between researchers at the University of East Anglia (UEA) and the University of California, Riverside (UCR). The 2018 vintage consists of 30 datasets with a wide range of cases, dimensions and series lengths. For this first iteration of the archive we format all data to be of equal length, include no series with missing data and provide train/test splits.},
	language = {en},
	urldate = {2022-03-27},
	journal = {arXiv:1811.00075 [cs, stat]},
	author = {Bagnall, Anthony and Dau, Hoang Anh and Lines, Jason and Flynn, Michael and Large, James and Bostrom, Aaron and Southam, Paul and Keogh, Eamonn},
	month = oct,
	year = {2018},
	note = {arXiv: 1811.00075},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Bagnall et al. - 2018 - The UEA multivariate time series classification ar.pdf:/home/jacqueline/Zotero/storage/CWCMUDB3/Bagnall et al. - 2018 - The UEA multivariate time series classification ar.pdf:application/pdf},
}

@article{fawaz_deep_2019,
	title = {Deep learning for time series classification: a review},
	volume = {33},
	issn = {1384-5810, 1573-756X},
	shorttitle = {Deep learning for time series classification},
	url = {http://arxiv.org/abs/1809.04356},
	doi = {10.1007/s10618-019-00619-1},
	abstract = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8,730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
	number = {4},
	urldate = {2022-03-28},
	journal = {Data Min Knowl Disc},
	author = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
	month = jul,
	year = {2019},
	note = {arXiv: 1809.04356},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {917--963},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/GWE39EYP/Fawaz et al. - 2019 - Deep learning for time series classification a re.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/VUT4NG92/1809.html:text/html},
}

@article{mothilal_explaining_2020,
	title = {Explaining {Machine} {Learning} {Classifiers} through {Diverse} {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/1905.07697},
	doi = {10.1145/3351095.3372850},
	abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
	urldate = {2022-03-28},
	journal = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
	month = jan,
	year = {2020},
	note = {arXiv: 1905.07697},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computers and Society},
	pages = {607--617},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/CQ8CAPAH/Mothilal et al. - 2020 - Explaining Machine Learning Classifiers through Di.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/DJKUERYV/1905.html:text/html},
}

@article{pawelczyk_carla_2021,
	title = {{CARLA}: {A} {Python} {Library} to {Benchmark} {Algorithmic} {Recourse} and {Counterfactual} {Explanation} {Algorithms}},
	shorttitle = {{CARLA}},
	url = {http://arxiv.org/abs/2108.00783},
	abstract = {Counterfactual explanations provide means for prescriptive model explanations by suggesting actionable feature changes (e.g., increase income) that allow individuals to achieve favorable outcomes in the future (e.g., insurance approval). Choosing an appropriate method is a crucial aspect for meaningful counterfactual explanations. As documented in recent reviews, there exists a quickly growing literature with available methods. Yet, in the absence of widely available opensource implementations, the decision in favor of certain models is primarily based on what is readily available. Going forward - to guarantee meaningful comparisons across explanation methods - we present CARLA (Counterfactual And Recourse LibrAry), a python library for benchmarking counterfactual explanation methods across both different data sets and different machine learning models. In summary, our work provides the following contributions: (i) an extensive benchmark of 11 popular counterfactual explanation methods, (ii) a benchmarking framework for research on future counterfactual explanation methods, and (iii) a standardized set of integrated evaluation measures and data sets for transparent and extensive comparisons of these methods. We have open-sourced CARLA and our experimental results on Github, making them available as competitive baselines. We welcome contributions from other research groups and practitioners.},
	urldate = {2022-03-28},
	journal = {arXiv:2108.00783 [cs]},
	author = {Pawelczyk, Martin and Bielawski, Sascha and Heuvel, Johannes van den and Richter, Tobias and Kasneci, Gjergji},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.00783},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/YVDLMBSV/Pawelczyk et al. - 2021 - CARLA A Python Library to Benchmark Algorithmic R.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/QDS74ZKP/2108.html:text/html},
}

@article{deb_evolutionary_2014,
	title = {An {Evolutionary} {Many}-{Objective} {Optimization} {Algorithm} {Using} {Reference}-{Point}-{Based} {Nondominated} {Sorting} {Approach}, {Part} {I}: {Solving} {Problems} {With} {Box} {Constraints}},
	volume = {18},
	issn = {1089-778X, 1089-778X, 1941-0026},
	shorttitle = {An {Evolutionary} {Many}-{Objective} {Optimization} {Algorithm} {Using} {Reference}-{Point}-{Based} {Nondominated} {Sorting} {Approach}, {Part} {I}},
	url = {http://ieeexplore.ieee.org/document/6600851/},
	doi = {10.1109/TEVC.2013.2281535},
	number = {4},
	urldate = {2022-03-28},
	journal = {IEEE Trans. Evol. Computat.},
	author = {Deb, Kalyanmoy and Jain, Himanshu},
	month = aug,
	year = {2014},
	pages = {577--601},
}

@incollection{hutchison_emo_2005,
	address = {Berlin, Heidelberg},
	title = {An {EMO} {Algorithm} {Using} the {Hypervolume} {Measure} as {Selection} {Criterion}},
	volume = {3410},
	isbn = {978-3-540-24983-2 978-3-540-31880-4},
	url = {http://link.springer.com/10.1007/978-3-540-31880-4_5},
	urldate = {2022-03-28},
	booktitle = {Evolutionary {Multi}-{Criterion} {Optimization}},
	publisher = {Springer Berlin Heidelberg},
	author = {Emmerich, Michael and Beume, Nicola and Naujoks, Boris},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Coello Coello, Carlos A. and Hernández Aguirre, Arturo and Zitzler, Eckart},
	year = {2005},
	doi = {10.1007/978-3-540-31880-4_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {62--76},
}

@article{qingfu_zhang_moead_2007,
	title = {{MOEA}/{D}: {A} {Multiobjective} {Evolutionary} {Algorithm} {Based} on {Decomposition}},
	volume = {11},
	issn = {1941-0026, 1089-778X},
	shorttitle = {{MOEA}/{D}},
	url = {http://ieeexplore.ieee.org/document/4358754/},
	doi = {10.1109/TEVC.2007.892759},
	number = {6},
	urldate = {2022-03-28},
	journal = {IEEE Trans. Evol. Computat.},
	author = {{Qingfu Zhang} and {Hui Li}},
	month = dec,
	year = {2007},
	pages = {712--731},
}

@incollection{hutchison_indicator-based_2004,
	address = {Berlin, Heidelberg},
	title = {Indicator-{Based} {Selection} in {Multiobjective} {Search}},
	volume = {3242},
	isbn = {978-3-540-23092-2 978-3-540-30217-9},
	url = {http://link.springer.com/10.1007/978-3-540-30217-9_84},
	urldate = {2022-03-28},
	booktitle = {Parallel {Problem} {Solving} from {Nature} - {PPSN} {VIII}},
	publisher = {Springer Berlin Heidelberg},
	author = {Zitzler, Eckart and Künzli, Simon},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Yao, Xin and Burke, Edmund K. and Lozano, José A. and Smith, Jim and Merelo-Guervós, Juan Julián and Bullinaria, John A. and Rowe, Jonathan E. and Tiňo, Peter and Kabán, Ata and Schwefel, Hans-Paul},
	year = {2004},
	doi = {10.1007/978-3-540-30217-9_84},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {832--842},
}

@article{lin_medical_2019,
	title = {Medical {Time} {Series} {Classification} with {Hierarchical} {Attention}-based {Temporal} {Convolutional} {Networks}: {A} {Case} {Study} of {Myotonic} {Dystrophy} {Diagnosis}},
	shorttitle = {Medical {Time} {Series} {Classification} with {Hierarchical} {Attention}-based {Temporal} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1903.11748},
	abstract = {Myotonia, which refers to delayed muscle relaxation after contraction, is the main symptom of myotonic dystrophy patients. We propose a hierarchical attention-based temporal convolutional network (HA-TCN) for myotonic dystrohpy diagnosis from handgrip time series data, and introduce mechanisms that enable model explainability. We compare the performance of the HA-TCN model against that of benchmark TCN models, LSTM models with and without attention mechanisms, and SVM approaches with handcrafted features. In terms of classification accuracy and F1 score, we found all deep learning models have similar levels of performance, and they all outperform SVM. Further, the HA-TCN model outperforms its TCN counterpart with regards to computational efficiency regardless of network depth, and in terms of performance particularly when the number of hidden layers is small. Lastly, HA-TCN models can consistently identify relevant time series segments in the relaxation phase of the handgrip time series, and exhibit increased robustness to noise when compared to attention-based LSTM models.},
	urldate = {2022-03-28},
	journal = {arXiv:1903.11748 [cs]},
	author = {Lin, Lei and Xu, Beilei and Wu, Wencheng and Richardson, Trevor and Bernal, Edgar A.},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.11748},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/HGW4XCJT/Lin et al. - 2019 - Medical Time Series Classification with Hierarchic.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/6LZCJ7TM/1903.html:text/html},
}

@article{guo_exploring_2019,
	title = {Exploring {Interpretable} {LSTM} {Neural} {Networks} over {Multi}-{Variable} {Data}},
	url = {http://arxiv.org/abs/1905.12034},
	abstract = {For recurrent neural networks trained on time series with target and exogenous variables, in addition to accurate prediction, it is also desired to provide interpretable insights into the data. In this paper, we explore the structure of LSTM recurrent neural networks to learn variable-wise hidden states, with the aim to capture different dynamics in multi-variable time series and distinguish the contribution of variables to the prediction. With these variable-wise hidden states, a mixture attention mechanism is proposed to model the generative process of the target. Then we develop associated training methods to jointly learn network parameters, variable and temporal importance w.r.t the prediction of the target variable. Extensive experiments on real datasets demonstrate enhanced prediction performance by capturing the dynamics of different variables. Meanwhile, we evaluate the interpretation results both qualitatively and quantitatively. It exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variable data.},
	urldate = {2022-03-28},
	journal = {arXiv:1905.12034 [cs, stat]},
	author = {Guo, Tian and Lin, Tao and Antulov-Fantulin, Nino},
	month = may,
	year = {2019},
	note = {arXiv: 1905.12034},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/N4MFZK8H/Guo et al. - 2019 - Exploring Interpretable LSTM Neural Networks over .pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/5L6R2QZX/1905.html:text/html},
}

@article{karlsson_explainable_2018,
	title = {Explainable time series tweaking via irreversible and reversible temporal transformations},
	url = {http://arxiv.org/abs/1809.05183},
	abstract = {Time series classiﬁcation has received great attention over the past decade with a wide range of methods focusing on predictive performance by exploiting various types of temporal features. Nonetheless, little emphasis has been placed on interpretability and explainability. In this paper, we formulate the novel problem of explainable time series tweaking, where, given a time series and an opaque classiﬁer that provides a particular classiﬁcation decision for the time series, we want to ﬁnd the minimum number of changes to be performed to the given time series so that the classiﬁer changes its decision to another class. We show that the problem is NP-hard, and focus on two instantiations of the problem, which we refer to as reversible and irreversible time series tweaking. The classiﬁer under investigation is the random shapelet forest classiﬁer. Moreover, we propose two algorithmic solutions for the two problems along with simple optimizations, as well as a baseline solution using the nearest neighbor classiﬁer. An extensive experimental evaluation on a variety of real datasets demonstrates the usefulness and effectiveness of our problem formulation and solutions.},
	language = {en},
	urldate = {2022-03-29},
	journal = {arXiv:1809.05183 [cs, stat]},
	author = {Karlsson, Isak and Rebane, Jonathan and Papapetrou, Panagiotis and Gionis, Aristides},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.05183},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Karlsson et al. - 2018 - Explainable time series tweaking via irreversible .pdf:/home/jacqueline/Zotero/storage/9DFQG7FX/Karlsson et al. - 2018 - Explainable time series tweaking via irreversible .pdf:application/pdf},
}

@article{karlsson_explainable_2018-1,
	title = {Explainable time series tweaking via irreversible and reversible temporal transformations},
	url = {http://arxiv.org/abs/1809.05183},
	abstract = {Time series classiﬁcation has received great attention over the past decade with a wide range of methods focusing on predictive performance by exploiting various types of temporal features. Nonetheless, little emphasis has been placed on interpretability and explainability. In this paper, we formulate the novel problem of explainable time series tweaking, where, given a time series and an opaque classiﬁer that provides a particular classiﬁcation decision for the time series, we want to ﬁnd the minimum number of changes to be performed to the given time series so that the classiﬁer changes its decision to another class. We show that the problem is NP-hard, and focus on two instantiations of the problem, which we refer to as reversible and irreversible time series tweaking. The classiﬁer under investigation is the random shapelet forest classiﬁer. Moreover, we propose two algorithmic solutions for the two problems along with simple optimizations, as well as a baseline solution using the nearest neighbor classiﬁer. An extensive experimental evaluation on a variety of real datasets demonstrates the usefulness and effectiveness of our problem formulation and solutions.},
	language = {en},
	urldate = {2022-03-29},
	journal = {arXiv:1809.05183 [cs, stat]},
	author = {Karlsson, Isak and Rebane, Jonathan and Papapetrou, Panagiotis and Gionis, Aristides},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.05183},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Karlsson et al. - 2018 - Explainable time series tweaking via irreversible .pdf:/home/jacqueline/Zotero/storage/9TSLTXQW/Karlsson et al. - 2018 - Explainable time series tweaking via irreversible .pdf:application/pdf},
}

@article{sulem_diverse_2022,
	title = {Diverse {Counterfactual} {Explanations} for {Anomaly} {Detection} in {Time} {Series}},
	url = {http://arxiv.org/abs/2203.11103},
	abstract = {Data-driven methods that detect anomalies in times series data are ubiquitous in practice, but they are in general unable to provide helpful explanations for the predictions they make. In this work we propose a model-agnostic algorithm that generates counterfactual ensemble explanations for time series anomaly detection models. Our method generates a set of diverse counterfactual examples, i.e, multiple perturbed versions of the original time series that are not considered anomalous by the detection model. Since the magnitude of the perturbations is limited, these counterfactuals represent an ensemble of inputs similar to the original time series that the model would deem normal. Our algorithm is applicable to any differentiable anomaly detection model. We investigate the value of our method on univariate and multivariate real-world datasets and two deep-learning-based anomaly detection models, under several explainability criteria previously proposed in other data domains such as Validity, Plausibility, Closeness and Diversity. We show that our algorithm can produce ensembles of counterfactual examples that satisfy these criteria and thanks to a novel type of visualization, can convey a richer interpretation of a model’s internal mechanism than existing methods. Moreover, we design a sparse variant of our method to improve the interpretability of counterfactual explanations for high-dimensional time series anomalies. In this setting, our explanation is localized on only a few dimensions and can therefore be communicated more efﬁciently to the model’s user.},
	language = {en},
	urldate = {2022-03-29},
	journal = {arXiv:2203.11103 [cs, stat]},
	author = {Sulem, Deborah and Donini, Michele and Zafar, Muhammad Bilal and Aubet, Francois-Xavier and Gasthaus, Jan and Januschowski, Tim and Das, Sanjiv and Kenthapadi, Krishnaram and Archambeau, Cedric},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.11103},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Sulem et al. - 2022 - Diverse Counterfactual Explanations for Anomaly De.pdf:/home/jacqueline/Zotero/storage/IUMENU5U/Sulem et al. - 2022 - Diverse Counterfactual Explanations for Anomaly De.pdf:application/pdf},
}

@article{guidotti_factual_2019,
	title = {Factual and {Counterfactual} {Explanations} for {Black} {Box} {Decision} {Making}},
	volume = {34},
	issn = {1541-1672, 1941-1294},
	url = {https://ieeexplore.ieee.org/document/8920138/},
	doi = {10.1109/MIS.2019.2957223},
	abstract = {The rise of sophisticated machine learning models has brought accurate but obscure decision systems, which hide their logic, thus undermining transparency, trust, and the adoption of artiﬁcial intelligence (AI) in socially sensitive and safety-critical contexts. We introduce a local rule-based explanation method, providing faithful explanations of the decision made by a black box classiﬁer on a speciﬁc instance. The proposed method ﬁrst learns an interpretable, local classiﬁer on a synthetic neighborhood of the instance under investigation, generated by a genetic algorithm. Then, it derives from the interpretable classiﬁer an explanation consisting of a decision rule, explaining the factual reasons of the decision, and a set of counterfactuals, suggesting the changes in the instance features that would lead to a different outcome. Experimental results show that the proposed method outperforms existing approaches in terms of the quality of the explanations and of the accuracy in mimicking the black box.},
	language = {en},
	number = {6},
	urldate = {2022-03-29},
	journal = {IEEE Intell. Syst.},
	author = {Guidotti, Riccardo and Monreale, Anna and Giannotti, Fosca and Pedreschi, Dino and Ruggieri, Salvatore and Turini, Franco},
	month = nov,
	year = {2019},
	pages = {14--23},
	file = {Guidotti et al. - 2019 - Factual and Counterfactual Explanations for Black .pdf:/home/jacqueline/Zotero/storage/C5GWU58K/Guidotti et al. - 2019 - Factual and Counterfactual Explanations for Black .pdf:application/pdf},
}

@article{poyiadzi_face_2020,
	title = {{FACE}: {Feasible} and {Actionable} {Counterfactual} {Explanations}},
	shorttitle = {{FACE}},
	url = {http://arxiv.org/abs/1909.09369},
	doi = {10.1145/3375627.3375850},
	abstract = {Work in Counterfactual Explanations tends to focus on the principle of "the closest possible world" that identifies small changes leading to the desired outcome. In this paper we argue that while this approach might initially seem intuitively appealing it exhibits shortcomings not addressed in the current literature. First, a counterfactual example generated by the state-of-the-art systems is not necessarily representative of the underlying data distribution, and may therefore prescribe unachievable goals(e.g., an unsuccessful life insurance applicant with severe disability may be advised to do more sports). Secondly, the counterfactuals may not be based on a "feasible path" between the current state of the subject and the suggested one, making actionable recourse infeasible (e.g., low-skilled unsuccessful mortgage applicants may be told to double their salary, which may be hard without first increasing their skill level). These two shortcomings may render counterfactual explanations impractical and sometimes outright offensive. To address these two major flaws, first of all, we propose a new line of Counterfactual Explanations research aimed at providing actionable and feasible paths to transform a selected instance into one that meets a certain goal. Secondly, we propose FACE: an algorithmically sound way of uncovering these "feasible paths" based on the shortest path distances defined via density-weighted metrics. Our approach generates counterfactuals that are coherent with the underlying data distribution and supported by the "feasible paths" of change, which are achievable and can be tailored to the problem at hand.},
	urldate = {2022-03-29},
	journal = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
	author = {Poyiadzi, Rafael and Sokol, Kacper and Santos-Rodriguez, Raul and De Bie, Tijl and Flach, Peter},
	month = feb,
	year = {2020},
	note = {arXiv: 1909.09369},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {344--350},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/UTRKJSUM/Poyiadzi et al. - 2020 - FACE Feasible and Actionable Counterfactual Expla.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/89VW74W4/1909.html:text/html},
}

@book{sundararajan_discrete_2001,
	address = {Singapore ; River Edge, NJ},
	title = {The discrete fourier transform: theory, algorithms and applications},
	isbn = {978-981-02-4521-4},
	shorttitle = {The discrete fourier transform},
	publisher = {World Scientific},
	author = {Sundararajan, D.},
	year = {2001},
	keywords = {Fourier transformations, Mathematical physics},
}

@article{laugel_dangers_2019,
	title = {The {Dangers} of {Post}-hoc {Interpretability}: {Unjustified} {Counterfactual} {Explanations}},
	shorttitle = {The {Dangers} of {Post}-hoc {Interpretability}},
	url = {http://arxiv.org/abs/1907.09294},
	abstract = {Post-hoc interpretability approaches have been proven to be powerful tools to generate explanations for the predictions made by a trained blackbox model. However, they create the risk of having explanations that are a result of some artifacts learned by the model instead of actual knowledge from the data. This paper focuses on the case of counterfactual explanations and asks whether the generated instances can be justiﬁed, i.e. continuously connected to some ground-truth data. We evaluate the risk of generating unjustiﬁed counterfactual examples by investigating the local neighborhoods of instances whose predictions are to be explained and show that this risk is quite high for several datasets. Furthermore, we show that most state of the art approaches do not differentiate justiﬁed from unjustiﬁed counterfactual examples, leading to less useful explanations.},
	language = {en},
	urldate = {2022-03-31},
	journal = {arXiv:1907.09294 [cs, stat]},
	author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.09294},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Laugel et al. - 2019 - The Dangers of Post-hoc Interpretability Unjustif.pdf:/home/jacqueline/Zotero/storage/29RDMLSU/Laugel et al. - 2019 - The Dangers of Post-hoc Interpretability Unjustif.pdf:application/pdf},
}

@article{lipton_mythos_2017,
	title = {The {Mythos} of {Model} {Interpretability}},
	url = {http://arxiv.org/abs/1606.03490},
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	urldate = {2022-04-01},
	journal = {arXiv:1606.03490 [cs, stat]},
	author = {Lipton, Zachary C.},
	month = mar,
	year = {2017},
	note = {arXiv: 1606.03490},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/S3SFI49J/Lipton - 2017 - The Mythos of Model Interpretability.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/TH4CGX72/1606.html:text/html},
}

@article{adadi_peeking_2018,
	title = {Peeking {Inside} the {Black}-{Box}: {A} {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Peeking {Inside} the {Black}-{Box}},
	url = {https://ieeexplore.ieee.org/document/8466590/},
	doi = {10.1109/ACCESS.2018.2870052},
	abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artiﬁcial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research ﬁeld holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
	language = {en},
	urldate = {2022-05-24},
	journal = {IEEE Access},
	author = {Adadi, Amina and Berrada, Mohammed},
	year = {2018},
	pages = {52138--52160},
	file = {Adadi and Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explaina.pdf:/home/jacqueline/Zotero/storage/IUH54UWI/Adadi and Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explaina.pdf:application/pdf},
}

@article{tjoa_survey_2021,
	title = {A {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI}): {Toward} {Medical} {XAI}},
	volume = {32},
	issn = {2162-237X, 2162-2388},
	shorttitle = {A {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	url = {https://ieeexplore.ieee.org/document/9233366/},
	doi = {10.1109/TNNLS.2020.3027314},
	abstract = {Recently, artiﬁcial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different ﬁelds and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.},
	language = {en},
	number = {11},
	urldate = {2022-05-24},
	journal = {IEEE Trans. Neural Netw. Learning Syst.},
	author = {Tjoa, Erico and Guan, Cuntai},
	month = nov,
	year = {2021},
	pages = {4793--4813},
	file = {Tjoa and Guan - 2021 - A Survey on Explainable Artificial Intelligence (X.pdf:/home/jacqueline/Zotero/storage/H84FZNQW/Tjoa and Guan - 2021 - A Survey on Explainable Artificial Intelligence (X.pdf:application/pdf},
}

@misc{vilone_explainable_2020,
	title = {Explainable {Artificial} {Intelligence}: a {Systematic} {Review}},
	shorttitle = {Explainable {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2006.00093},
	abstract = {Explainable Artiﬁcial Intelligence (XAI) has experienced a signiﬁcant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classiﬁcation system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.},
	language = {en},
	urldate = {2022-05-24},
	publisher = {arXiv},
	author = {Vilone, Giulia and Longo, Luca},
	month = oct,
	year = {2020},
	note = {Number: arXiv:2006.00093
arXiv:2006.00093 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, I.2.0, I.2.6, I.2.m},
	file = {Vilone and Longo - 2020 - Explainable Artificial Intelligence a Systematic .pdf:/home/jacqueline/Zotero/storage/HJKEVW9P/Vilone and Longo - 2020 - Explainable Artificial Intelligence a Systematic .pdf:application/pdf},
}

@article{darias_systematic_nodate,
	title = {A {Systematic} {Review} on {Model}-agnostic {XAI} {Libraries}},
	abstract = {During the last few years, the topic of explainable artiﬁcial intelligence (XAI) has become a hotspot in the ML research community. Model-agnostic interpretation methods propose separating the explanations from the ML model, making these explanation methods reusable through XAI libraries. In this paper, we have reviewed some selected XAI libraries and provide examples of diﬀerent model agnostic explanations. The context of the research conducted in this paper is the iSee project 1 that will show how users of Artiﬁcial Intelligence (AI) can capture, share and re-use their experiences of AI explanations with other users who have similar explanation needs.},
	language = {en},
	author = {Darias, Jesus M and Dıaz-Agudo, Belen and Recio-Garcia, Juan A},
	pages = {12},
	file = {Darias et al. - A Systematic Review on Model-agnostic XAI Librarie.pdf:/home/jacqueline/Zotero/storage/S99A572W/Darias et al. - A Systematic Review on Model-agnostic XAI Librarie.pdf:application/pdf},
}

@misc{meudec_tf-explain_nodate,
	title = {tf-explain},
	abstract = {Interpretability Methods for tf.keras models with TensorFlow 2.x},
	author = {Meudec, Raphael},
}

@misc{meudec_raphael_tf-explain_2021,
	title = {tf-explain},
	copyright = {Open Access},
	url = {https://zenodo.org/record/5711704},
	abstract = {Interpretability Methods for tf.keras models with TensorFlow 2.x},
	urldate = {2022-05-24},
	publisher = {Zenodo},
	author = {Meudec, Raphael},
	month = feb,
	year = {2021},
	doi = {10.5281/ZENODO.5711704},
}

@article{klaise_alibi_2021,
	title = {Alibi {Explain}: {Algorithms} for {Explaining} {Machine} {Learning} {Models}},
	volume = {22},
	url = {http://jmlr.org/papers/v22/21-0017.html},
	number = {181},
	journal = {Journal of Machine Learning Research},
	author = {Klaise, Janis and Looveren, Arnaud Van and Vacanti, Giovanni and Coca, Alexandru},
	year = {2021},
	pages = {1--7},
}

@misc{kokhlikyan_captum_2020,
	title = {Captum: {A} unified and generic model interpretability library for {PyTorch}},
	shorttitle = {Captum},
	url = {http://arxiv.org/abs/2009.07896},
	abstract = {In this paper we introduce a novel, unified, open-source model interpretability library for PyTorch [12]. The library contains generic implementations of a number of gradient and perturbation-based attribution algorithms, also known as feature, neuron and layer importance algorithms, as well as a set of evaluation metrics for these algorithms. It can be used for both classification and non-classification models including graph-structured models built on Neural Networks (NN). In this paper we give a high-level overview of supported attribution algorithms and show how to perform memory-efficient and scalable computations. We emphasize that the three main characteristics of the library are multimodality, extensibility and ease of use. Multimodality supports different modality of inputs such as image, text, audio or video. Extensibility allows adding new algorithms and features. The library is also designed for easy understanding and use. Besides, we also introduce an interactive visualization tool called Captum Insights that is built on top of Captum library and allows sample-based model debugging and visualization using feature importance metrics.},
	urldate = {2022-05-24},
	publisher = {arXiv},
	author = {Kokhlikyan, Narine and Miglani, Vivek and Martin, Miguel and Wang, Edward and Alsallakh, Bilal and Reynolds, Jonathan and Melnikov, Alexander and Kliushkina, Natalia and Araya, Carlos and Yan, Siqi and Reblitz-Richardson, Orion},
	month = sep,
	year = {2020},
	note = {Number: arXiv:2009.07896
arXiv:2009.07896 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/LXVQBS3J/Kokhlikyan et al. - 2020 - Captum A unified and generic model interpretabili.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/E6ZRZDM5/2009.html:text/html},
}

@misc{ozbulak_pytorch_2019,
	title = {{PyTorch} {CNN} {Visualizations}},
	url = {https://github.com/utkuozbulak/pytorch-cnn-visualizations},
	publisher = {GitHub},
	author = {Ozbulak, Utku},
	year = {2019},
	note = {Publication Title: GitHub repository},
}

@article{alber_innvestigate_2019,
	title = {{iNNvestigate} {Neural} {Networks}!},
	volume = {20},
	url = {http://jmlr.org/papers/v20/18-540.html},
	number = {93},
	journal = {Journal of Machine Learning Research},
	author = {Alber, Maximilian and Lapuschkin, Sebastian and Seegerer, Philipp and Hägele, Miriam and Schütt, Kristof T. and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert and Dähne, Sven and Kindermans, Pieter-Jan},
	year = {2019},
	pages = {1--8},
}

@misc{fong_understanding_2019,
	title = {Understanding {Deep} {Networks} via {Extremal} {Perturbations} and {Smooth} {Masks}},
	url = {http://arxiv.org/abs/1910.08485},
	abstract = {The problem of attribution is concerned with identifying the parts of an input that are responsible for a model's output. An important family of attribution methods is based on measuring the effect of perturbations applied to the input. In this paper, we discuss some of the shortcomings of existing approaches to perturbation analysis and address them by introducing the concept of extremal perturbations, which are theoretically grounded and interpretable. We also introduce a number of technical innovations to compute extremal perturbations, including a new area constraint and a parametric family of smooth perturbations, which allow us to remove all tunable hyper-parameters from the optimization problem. We analyze the effect of perturbations as a function of their area, demonstrating excellent sensitivity to the spatial properties of the deep neural network under stimulation. We also extend perturbation analysis to the intermediate layers of a network. This application allows us to identify the salient channels necessary for classification, which, when visualized using feature inversion, can be used to elucidate model behavior. Lastly, we introduce TorchRay, an interpretability library built on PyTorch.},
	urldate = {2022-05-24},
	publisher = {arXiv},
	author = {Fong, Ruth and Patrick, Mandela and Vedaldi, Andrea},
	month = oct,
	year = {2019},
	note = {Number: arXiv:1910.08485
arXiv:1910.08485 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/JSR654J4/Fong et al. - 2019 - Understanding Deep Networks via Extremal Perturbat.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/NLFBF3SL/1910.html:text/html},
}

@article{nori_interpretml_2019,
	title = {{InterpretML}: {A} {Unified} {Framework} for {Machine} {Learning} {Interpretability}},
	journal = {arXiv preprint arXiv:1909.09223},
	author = {Nori, Harsha and Jenkins, Samuel and Koch, Paul and Caruana, Rich},
	year = {2019},
}

@article{wexler_what-if_2019,
	title = {The {What}-{If} {Tool}: {Interactive} {Probing} of {Machine} {Learning} {Models}},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {The {What}-{If} {Tool}},
	url = {https://ieeexplore.ieee.org/document/8807255/},
	doi = {10.1109/TVCG.2019.2934619},
	urldate = {2022-05-24},
	journal = {IEEE Trans. Visual. Comput. Graphics},
	author = {Wexler, James and Pushkarna, Mahima and Bolukbasi, Tolga and Wattenberg, Martin and Viegas, Fernanda and Wilson, Jimbo},
	year = {2019},
	pages = {1--1},
	file = {Full Text:/home/jacqueline/Zotero/storage/76DR9C8W/Wexler et al. - 2019 - The What-If Tool Interactive Probing of Machine L.pdf:application/pdf},
}

@misc{arya_one_2019,
	title = {One {Explanation} {Does} {Not} {Fit} {All}: {A} {Toolkit} and {Taxonomy} of {AI} {Explainability} {Techniques}},
	url = {https://arxiv.org/abs/1909.03012},
	author = {Arya, Vijay and Bellamy, Rachel K. E. and Chen, Pin-Yu and Dhurandhar, Amit and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Liao, Q. Vera and Luss, Ronny and Mojsilović, Aleksandra and Mourad, Sami and Pedemonte, Pablo and Raghavendra, Ramya and Richards, John and Sattigeri, Prasanna and Shanmugam, Karthikeyan and Singh, Moninder and Varshney, Kush R. and Wei, Dennis and Zhang, Yunfeng},
	month = sep,
	year = {2019},
}

@misc{singh_imodels_2021,
	title = {imodels: a python package for fitting interpretable models},
	url = {https://doi.org/10.21105/joss.03192},
	publisher = {The Open Journal},
	author = {Singh, Chandan and Nasseri, Keyan and Tan, Yan Shuo and Tang, Tiffany and Yu, Bin},
	year = {2021},
	doi = {10.21105/joss.03192},
	note = {Issue: 61
Pages: 3192
Publication Title: Journal of Open Source Software
Volume: 6},
}

@misc{samsten_isaksamstenwildboar_2020,
	title = {isaksamsten/wildboar: wildboar},
	copyright = {GNU General Public License v3.0 or later, Open Access},
	shorttitle = {isaksamsten/wildboar},
	url = {https://zenodo.org/record/4264063},
	abstract = {wildboar is a fast package for time series classification with Python The package can be installed from PyPi {\textless}pre{\textgreater}{\textless}code{\textgreater}pip install wildboar{\textless}/code{\textgreater}{\textless}/pre{\textgreater}},
	urldate = {2022-05-24},
	publisher = {Zenodo},
	author = {Samsten, Isak},
	month = nov,
	year = {2020},
	doi = {10.5281/ZENODO.4264063},
	note = {Language: en},
	keywords = {time series classification},
}

@misc{ancona_towards_2018,
	title = {Towards better understanding of gradient-based attribution methods for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.06104},
	abstract = {Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.},
	urldate = {2022-05-24},
	publisher = {arXiv},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	month = mar,
	year = {2018},
	note = {Number: arXiv:1711.06104
arXiv:1711.06104 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/ZXPZVNI9/Ancona et al. - 2018 - Towards better understanding of gradient-based att.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/QJFHVBPE/1711.html:text/html},
}

@article{baniecki_dalex_2021,
	title = {dalex: {Responsible} {Machine} {Learning} with {Interactive} {Explainability} and {Fairness} in {Python}},
	volume = {22},
	url = {http://jmlr.org/papers/v22/20-1473.html},
	number = {214},
	journal = {Journal of Machine Learning Research},
	author = {Baniecki, Hubert and Kretowicz, Wojciech and Piatyszek, Piotr and Wisniewski, Jakub and Biecek, Przemyslaw},
	year = {2021},
	pages = {1--7},
}

@inproceedings{agarwal_interpretable_2020,
	address = {Canberra, ACT, Australia},
	title = {Interpretable {Machine} {Learning} {Tools}: {A} {Survey}},
	isbn = {978-1-72812-547-3},
	shorttitle = {Interpretable {Machine} {Learning} {Tools}},
	url = {https://ieeexplore.ieee.org/document/9308260/},
	doi = {10.1109/SSCI47803.2020.9308260},
	abstract = {In recent years machine learning (ML) systems have been deployed extensively in various domains. But most MLbased frameworks lack transparency. To believe in ML models, an individual needs to understand the reasons behind the ML predictions. In this paper, we provide a survey of open-source software tools that help explore and understand the behavior of the ML models. Also, these tools include a variety of interpretable machine learning methods that assist people with understanding the connection between input and output variables through interpretation, validate the decision of a predictive model to enable lucidity, accountability, and fairness in the algorithmic decisionmaking policies. Furthermore, we provide the state-of-the-art of interpretable machine learning (IML) tools, along with a comparison and a brief discussion of the implementation of those IML tools in various programming languages.},
	language = {en},
	urldate = {2022-05-31},
	booktitle = {2020 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	publisher = {IEEE},
	author = {Agarwal, Namita and Das, Saikat},
	month = dec,
	year = {2020},
	pages = {1528--1534},
	file = {Agarwal and Das - 2020 - Interpretable Machine Learning Tools A Survey.pdf:/home/jacqueline/Zotero/storage/KIIQ47KZ/Agarwal and Das - 2020 - Interpretable Machine Learning Tools A Survey.pdf:application/pdf},
}

@inproceedings{arya_ai_2021,
	address = {Bangalore India},
	title = {{AI} {Explainability} 360 {Toolkit}},
	isbn = {978-1-4503-8817-7},
	url = {https://dl.acm.org/doi/10.1145/3430984.3430987},
	doi = {10.1145/3430984.3430987},
	language = {en},
	urldate = {2022-05-31},
	booktitle = {8th {ACM} {IKDD} {CODS} and 26th {COMAD}},
	publisher = {ACM},
	author = {Arya, Vijay and Bellamy, Rachel K. E. and Chen, Pin-Yu and Dhurandhar, Amit and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Liao, Q. Vera and Luss, Ronny and Mojsilović, Aleksandra and Mourad, Sami and Pedemonte, Pablo and Raghavendra, Ramya and Richards, John and Sattigeri, Prasanna and Shanmugam, Karthikeyan and Singh, Moninder and Varshney, Kush R. and Wei, Dennis and Zhang, Yunfeng},
	month = jan,
	year = {2021},
	pages = {376--379},
}

@article{hall_machine_nodate,
	title = {Machine {Learning} {Interpretability} with {H2O} {Driverless} {AI}},
	language = {en},
	author = {Hall, Patrick and Gill, Navdeep and Kurka, Megan and Phan, Wen and Bartz, Angela},
	pages = {40},
	file = {Hall et al. - Machine Learning Interpretability with H2O Driverl.pdf:/home/jacqueline/Zotero/storage/SI9JVS65/Hall et al. - Machine Learning Interpretability with H2O Driverl.pdf:application/pdf},
}

@misc{oracle_skater_nodate,
	title = {Skater},
	url = {https://github.com/oracle/Skater.},
	author = {Oracle},
}

@misc{eli5-org_eli5_nodate,
	title = {{ELI5}},
	url = {https://github.com/TeamHGMemex/eli5},
	author = {eli5-org},
}

@misc{ozbulak_pytorch_2019-1,
	title = {{PyTorch} {CNN} {Visualizations}},
	url = {https://github.com/utkuozbulak/pytorch-cnn-visualizations},
	publisher = {GitHub},
	author = {Ozbulak, Utku},
	year = {2019},
	note = {Publication Title: GitHub repository},
}

@inproceedings{mothilal_explaining_2020-1,
	title = {Explaining machine learning classifiers through diverse counterfactual explanations},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Mothilal, Ramaravind K and Sharma, Amit and Tan, Chenhao},
	year = {2020},
	pages = {607--617},
}

@misc{singh_imodels_2021-1,
	title = {imodels: a python package for fitting interpretable models},
	url = {https://doi.org/10.21105/joss.03192},
	publisher = {The Open Journal},
	author = {Singh, Chandan and Nasseri, Keyan and Tan, Yan Shuo and Tang, Tiffany and Yu, Bin},
	year = {2021},
	doi = {10.21105/joss.03192},
	note = {Issue: 61
Pages: 3192
Publication Title: Journal of Open Source Software
Volume: 6},
}

@misc{arrieta_explainable_2019,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, {Taxonomies}, {Opportunities} and {Challenges} toward {Responsible} {AI}},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {http://arxiv.org/abs/1910.10045},
	abstract = {In the last few years, Artiﬁcial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the ﬁeld. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) ﬁeld, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the ﬁeld of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to deﬁne explainability in Machine Learning, establishing a novel deﬁnition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this deﬁnition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artiﬁcial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the ﬁeld of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the beneﬁts of AI in their activity sectors, without any prior bias for its lack of interpretability.},
	language = {en},
	urldate = {2022-06-01},
	publisher = {arXiv},
	author = {Arrieta, Alejandro Barredo and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and García, Salvador and Gil-López, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	month = dec,
	year = {2019},
	note = {Number: arXiv:1910.10045
arXiv:1910.10045 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concept.pdf:/home/jacqueline/Zotero/storage/UD4L2HS5/Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf},
}

@article{linardatos_explainable_2020,
	title = {Explainable {AI}: {A} {Review} of {Machine} {Learning} {Interpretability} {Methods}},
	volume = {23},
	issn = {1099-4300},
	shorttitle = {Explainable {AI}},
	url = {https://www.mdpi.com/1099-4300/23/1/18},
	doi = {10.3390/e23010018},
	abstract = {Recent advances in artiﬁcial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a signiﬁcant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientiﬁc interest in the ﬁeld of Explainable Artiﬁcial Intelligence (XAI), a ﬁeld that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more speciﬁcally, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
	language = {en},
	number = {1},
	urldate = {2022-06-01},
	journal = {Entropy},
	author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
	month = dec,
	year = {2020},
	pages = {18},
	file = {Linardatos et al. - 2020 - Explainable AI A Review of Machine Learning Inter.pdf:/home/jacqueline/Zotero/storage/4VZCWMH7/Linardatos et al. - 2020 - Explainable AI A Review of Machine Learning Inter.pdf:application/pdf},
}

@misc{schwalbe_comprehensive_2022,
	title = {A {Comprehensive} {Taxonomy} for {Explainable} {Artificial} {Intelligence}: {A} {Systematic} {Survey} of {Surveys} on {Methods} and {Concepts}},
	shorttitle = {A {Comprehensive} {Taxonomy} for {Explainable} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2105.07190},
	abstract = {In the meantime, a wide variety of terminologies, motivations, approaches, and evaluation criteria have been developed within the research ﬁeld of explainable artiﬁcial intelligence (XAI). With the amount of XAI methods vastly growing, a taxonomy of methods is needed by researchers as well as practitioners: To grasp the breadth of the topic, compare methods, and to select the right XAI method based on traits required by a speciﬁc use-case context. Many taxonomies for XAI methods of varying level of detail and depth can be found in the literature. While they often have a diﬀerent focus, they also exhibit many points of overlap. This paper uniﬁes these eﬀorts and provides a complete taxonomy of XAI methods with respect to notions present in the current state of research. In a structured literature analysis and meta-study, we identiﬁed and reviewed more than 50 of the most cited and current surveys on XAI methods, metrics, and method traits. After summarizing them in a survey of surveys, we merge terminologies and concepts of the articles into a uniﬁed structured taxonomy. Single concepts therein are illustrated by more than 50 diverse selected example methods in total, which we categorize accordingly. The taxonomy may serve both beginners, researchers, and practitioners as a reference and wide-ranging overview of XAI method traits and aspects. Hence, it provides foundations for targeted, use-case-oriented, and context-sensitive future research.},
	language = {en},
	urldate = {2022-06-01},
	publisher = {arXiv},
	author = {Schwalbe, Gesina and Finzel, Bettina},
	month = may,
	year = {2022},
	note = {Number: arXiv:2105.07190
arXiv:2105.07190 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, I.2.0, I.2.6, I.2.m},
	file = {Schwalbe und Finzel - 2022 - A Comprehensive Taxonomy for Explainable Artificia.pdf:/home/jacqueline/Zotero/storage/LBZFHZC3/Schwalbe und Finzel - 2022 - A Comprehensive Taxonomy for Explainable Artificia.pdf:application/pdf},
}

@misc{chari_explanation_2020,
	title = {Explanation {Ontology}: {A} {Model} of {Explanations} for {User}-{Centered} {AI}},
	shorttitle = {Explanation {Ontology}},
	url = {http://arxiv.org/abs/2010.01479},
	abstract = {Explainability has been a goal for Artiﬁcial Intelligence (AI) systems since their conception, with the need for explainability growing as more complex AI models are increasingly used in critical, high-stakes settings such as healthcare. Explanations have often added to an AI system in a non-principled, post-hoc manner. With greater adoption of these systems and emphasis on user-centric explainability, there is a need for a structured representation that treats explainability as a primary consideration, mapping end user needs to speciﬁc explanation types and the system’s AI capabilities. We design an explanation ontology to model both the role of explanations, accounting for the system and user attributes in the process, and the range of diﬀerent literature-derived explanation types. We indicate how the ontology can support user requirements for explanations in the domain of healthcare. We evaluate our ontology with a set of competency questions geared towards a system designer who might use our ontology to decide which explanation types to include, given a combination of users’ needs and a system’s capabilities, both in system design settings and in real-time operations. Through the use of this ontology, system designers will be able to make informed choices on which explanations AI systems can and should provide.},
	language = {en},
	urldate = {2022-06-03},
	publisher = {arXiv},
	author = {Chari, Shruthi and Seneviratne, Oshani and Gruen, Daniel M. and Foreman, Morgan A. and Das, Amar K. and McGuinness, Deborah L.},
	month = oct,
	year = {2020},
	note = {Number: arXiv:2010.01479
arXiv:2010.01479 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Chari et al. - 2020 - Explanation Ontology A Model of Explanations for .pdf:/home/jacqueline/Zotero/storage/5828C4S2/Chari et al. - 2020 - Explanation Ontology A Model of Explanations for .pdf:application/pdf},
}

@article{riveiro_thats_2021,
	title = {“{That}'s (not) the output {I} expected!” {On} the role of end user expectations in creating explanations of {AI} systems},
	volume = {298},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370221000588},
	doi = {10.1016/j.artint.2021.103507},
	language = {en},
	urldate = {2022-06-03},
	journal = {Artificial Intelligence},
	author = {Riveiro, Maria and Thill, Serge},
	month = sep,
	year = {2021},
	pages = {103507},
	file = {Riveiro und Thill - 2021 - “That's (not) the output I expected!” On the role .pdf:/home/jacqueline/Zotero/storage/MVRQRXP5/Riveiro und Thill - 2021 - “That's (not) the output I expected!” On the role .pdf:application/pdf},
}

@misc{mohseni_multidisciplinary_2020,
	title = {A {Multidisciplinary} {Survey} and {Framework} for {Design} and {Evaluation} of {Explainable} {AI} {Systems}},
	url = {http://arxiv.org/abs/1811.11839},
	abstract = {The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence applications used in everyday life. Explainable intelligent systems are designed to self-explain the reasoning behind system decisions and predictions, and researchers from different disciplines work together to define, design, and evaluate interpretable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of interpretable machine learning research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this paper presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of interpretable machine learning design goals and evaluation methods to show a mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
	language = {en},
	urldate = {2022-06-03},
	publisher = {arXiv},
	author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
	month = aug,
	year = {2020},
	note = {Number: arXiv:1811.11839
arXiv:1811.11839 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {Mohseni et al. - 2020 - A Multidisciplinary Survey and Framework for Desig.pdf:/home/jacqueline/Zotero/storage/RD73QB9K/Mohseni et al. - 2020 - A Multidisciplinary Survey and Framework for Desig.pdf:application/pdf},
}

@article{vilone_quantitative_2021,
	title = {A {Quantitative} {Evaluation} of {Global}, {Rule}-{Based} {Explanations} of {Post}-{Hoc}, {Model} {Agnostic} {Methods}},
	volume = {4},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2021.717899/full},
	doi = {10.3389/frai.2021.717899},
	abstract = {Understanding the inferences of data-driven, machine-learned models can be seen as a process that discloses the relationships between their input and output. These relationships consist and can be represented as a set of inference rules. However, the models usually do not explicit these rules to their end-users who, subsequently, perceive them as black-boxes and might not trust their predictions. Therefore, scholars have proposed several methods for extracting rules from data-driven machine-learned models to explain their logic. However, limited work exists on the evaluation and comparison of these methods. This study proposes a novel comparative approach to evaluate and compare the rulesets produced by five model-agnostic, post-hoc rule extractors by employing eight quantitative metrics. Eventually, the Friedman test was employed to check whether a method consistently performed better than the others, in terms of the selected metrics, and could be considered superior. Findings demonstrate that these metrics do not provide sufficient evidence to identify superior methods over the others. However, when used together, these metrics form a tool, applicable to every rule-extraction method and machine-learned models, that is, suitable to highlight the strengths and weaknesses of the rule-extractors in various applications in an objective and straightforward manner, without any human interventions. Thus, they are capable of successfully modelling distinctively aspects of explainability, providing to researchers and practitioners vital insights on what a model has learned during its training process and how it makes its predictions.},
	language = {en},
	urldate = {2022-06-03},
	journal = {Front. Artif. Intell.},
	author = {Vilone, Giulia and Longo, Luca},
	month = nov,
	year = {2021},
	pages = {717899},
	file = {Vilone und Longo - 2021 - A Quantitative Evaluation of Global, Rule-Based Ex.pdf:/home/jacqueline/Zotero/storage/4UV8FPWI/Vilone und Longo - 2021 - A Quantitative Evaluation of Global, Rule-Based Ex.pdf:application/pdf},
}

@article{zhang_explainable_2020,
	title = {Explainable {Recommendation}: {A} {Survey} and {New} {Perspectives}},
	volume = {14},
	issn = {1554-0669, 1554-0677},
	shorttitle = {Explainable {Recommendation}},
	url = {http://www.nowpublishers.com/article/Details/INR-066},
	doi = {10.1561/1500000066},
	language = {en},
	number = {1},
	urldate = {2022-06-07},
	journal = {FNT in Information Retrieval},
	author = {Zhang, Yongfeng and Chen, Xu},
	year = {2020},
	pages = {1--101},
	file = {Zhang und Chen - 2020 - Explainable Recommendation A Survey and New Persp.pdf:/home/jacqueline/Zotero/storage/N6GN5MCL/Zhang und Chen - 2020 - Explainable Recommendation A Survey and New Persp.pdf:application/pdf},
}

@article{alharin_reinforcement_2020,
	title = {Reinforcement {Learning} {Interpretation} {Methods}: {A} {Survey}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Reinforcement {Learning} {Interpretation} {Methods}},
	url = {https://ieeexplore.ieee.org/document/9194697/},
	doi = {10.1109/ACCESS.2020.3023394},
	abstract = {Reinforcement Learning (RL) systems achieved outstanding performance in different domains such as Atari games, ﬁnance, healthcare, and self-driving cars. However, their black-box nature complicates their use, especially in critical applications such as healthcare. To solve this problem, researchers have proposed different approaches to interpret RL models. Some of these methods were adopted from machine learning, while others were designed speciﬁcally for RL. The main objective of this paper is to show and explain RL interpretation methods, the metrics used to classify them, and how these metrics were applied to understand the internal details of RL models. We reviewed papers that propose new RL interpretation methods, improve the old ones, or discuss the pros and cons of the existing methods.},
	language = {en},
	urldate = {2022-06-07},
	journal = {IEEE Access},
	author = {Alharin, Alnour and Doan, Thanh-Nam and Sartipi, Mina},
	year = {2020},
	pages = {171058--171077},
	file = {Alharin et al. - 2020 - Reinforcement Learning Interpretation Methods A S.pdf:/home/jacqueline/Zotero/storage/G3LLPZYM/Alharin et al. - 2020 - Reinforcement Learning Interpretation Methods A S.pdf:application/pdf},
}

@article{mott_towards_nodate,
	title = {Towards {Interpretable} {Reinforcement} {Learning} {Using} {Attention} {Augmented} {Agents}},
	abstract = {Inspired by recent work in attention models for image captioning and question answering, we present a soft attention model for the reinforcement learning domain. This model uses a soft, top-down attention mechanism to create a bottleneck in the agent, forcing it to focus on task-relevant information by sequentially querying its view of the environment. The output of the attention mechanism allows direct observation of the information used by the agent to select its actions, enabling easier interpretation of this model than of traditional models. We analyze different strategies that the agents learn and show that a handful of strategies arise repeatedly across different games. We also show that the model learns to query separately about space and content (“where” vs. “what”). We demonstrate that an agent using this mechanism can achieve performance competitive with state-of-the-art models on ATARI tasks while still being interpretable.},
	language = {en},
	author = {Mott, Alex and Zoran, Daniel and Chrzanowski, Mike},
	pages = {10},
	file = {Mott et al. - Towards Interpretable Reinforcement Learning Using.pdf:/home/jacqueline/Zotero/storage/69GHCPWC/Mott et al. - Towards Interpretable Reinforcement Learning Using.pdf:application/pdf},
}

@misc{glanois_survey_2022,
	title = {A {Survey} on {Interpretable} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2112.13112},
	abstract = {Although deep reinforcement learning has become a promising machine learning approach for sequential decision-making problems, it is still not mature enough for high-stake domains such as autonomous driving or medical applications. In such contexts, a learned policy needs for instance to be interpretable, so that it can be inspected before any deployment (e.g., for safety and veriﬁability reasons). This survey provides an overview of various approaches to achieve higher interpretability in reinforcement learning (RL). To that aim, we distinguish interpretability (as a property of a model) and explainability (as a post-hoc operation, with the intervention of a proxy) and discuss them in the context of RL with an emphasis on the former notion. In particular, we argue that interpretable RL may embrace diﬀerent facets: interpretable inputs, interpretable (transition/reward) models, and interpretable decision-making. Based on this scheme, we summarize and analyze recent work related to interpretable RL with an emphasis on papers published in the past 10 years. We also discuss brieﬂy some related research areas and point to some potential promising research directions.},
	language = {en},
	urldate = {2022-06-07},
	publisher = {arXiv},
	author = {Glanois, Claire and Weng, Paul and Zimmer, Matthieu and Li, Dong and Yang, Tianpei and Hao, Jianye and Liu, Wulong},
	month = feb,
	year = {2022},
	note = {Number: arXiv:2112.13112
arXiv:2112.13112 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, I.2.6},
	file = {Glanois et al. - 2022 - A Survey on Interpretable Reinforcement Learning.pdf:/home/jacqueline/Zotero/storage/7VHAJBY9/Glanois et al. - 2022 - A Survey on Interpretable Reinforcement Learning.pdf:application/pdf},
}

@misc{mott_towards_2019,
	title = {Towards {Interpretable} {Reinforcement} {Learning} {Using} {Attention} {Augmented} {Agents}},
	url = {http://arxiv.org/abs/1906.02500},
	abstract = {Inspired by recent work in attention models for image captioning and question answering, we present a soft attention model for the reinforcement learning domain. This model uses a soft, top-down attention mechanism to create a bottleneck in the agent, forcing it to focus on task-relevant information by sequentially querying its view of the environment. The output of the attention mechanism allows direct observation of the information used by the agent to select its actions, enabling easier interpretation of this model than of traditional models. We analyze different strategies that the agents learn and show that a handful of strategies arise repeatedly across different games. We also show that the model learns to query separately about space and content (“where” vs. “what”). We demonstrate that an agent using this mechanism can achieve performance competitive with state-of-the-art models on ATARI tasks while still being interpretable.},
	language = {en},
	urldate = {2022-06-07},
	publisher = {arXiv},
	author = {Mott, Alex and Zoran, Daniel and Chrzanowski, Mike and Wierstra, Daan and Rezende, Danilo J.},
	month = jun,
	year = {2019},
	note = {Number: arXiv:1906.02500
arXiv:1906.02500 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Mott et al. - 2019 - Towards Interpretable Reinforcement Learning Using.pdf:/home/jacqueline/Zotero/storage/5PZTJSJE/Mott et al. - 2019 - Towards Interpretable Reinforcement Learning Using.pdf:application/pdf},
}

@misc{yang_omnixai_2022,
	title = {{OmniXAI}: {A} {Library} for {Explainable} {AI}},
	shorttitle = {{OmniXAI}},
	url = {http://arxiv.org/abs/2206.01612},
	abstract = {We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python library of eXplainable AI (XAI), which offers omni-way explainable AI capabilities and various interpretable machine learning techniques to address the pain points of understanding and interpreting the decisions made by machine learning (ML) in practice. OmniXAI aims to be a one-stop comprehensive library that makes explainable AI easy for data scientists, ML researchers and practitioners who need explanation for various types of data, models and explanation methods at different stages of ML process (data exploration, feature engineering, model development, evaluation, and decision-making, etc). In particular, our library includes a rich family of explanation methods integrated in a unified interface, which supports multiple data types (tabular data, images, texts, time-series), multiple types of ML models (traditional ML in Scikit-learn and deep learning models in PyTorch/TensorFlow), and a range of diverse explanation methods including "model-specific" and "model-agnostic" ones (such as feature-attribution explanation, counterfactual explanation, gradient-based explanation, etc). For practitioners, the library provides an easy-to-use unified interface to generate the explanations for their applications by only writing a few lines of codes, and also a GUI dashboard for visualization of different explanations for more insights about decisions. In this technical report, we present OmniXAI's design principles, system architectures, and major functionalities, and also demonstrate several example use cases across different types of data, tasks, and models.},
	urldate = {2022-06-13},
	publisher = {arXiv},
	author = {Yang, Wenzhuo and Le, Hung and Savarese, Silvio and Hoi, Steven C. H.},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2206.01612
arXiv:2206.01612 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, I.2.6, 68T09, 68T20, 68T01, I.2.5},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/DKCUVTIS/Yang et al. - 2022 - OmniXAI A Library for Explainable AI.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/CTKHQBYE/2206.html:text/html},
}

@misc{buitinck_api_2013,
	title = {{API} design for machine learning software: experiences from the scikit-learn project},
	shorttitle = {{API} design for machine learning software},
	url = {http://arxiv.org/abs/1309.0238},
	abstract = {Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.},
	urldate = {2022-06-13},
	publisher = {arXiv},
	author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and Vanderplas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Gaël},
	month = sep,
	year = {2013},
	note = {Number: arXiv:1309.0238
arXiv:1309.0238 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/945LBWBM/Buitinck et al. - 2013 - API design for machine learning software experien.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/VF2Q9RY5/1309.html:text/html},
}

@article{tjoa_survey_2021-1,
	title = {A {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI}): {Toward} {Medical} {XAI}},
	volume = {32},
	issn = {2162-237X, 2162-2388},
	shorttitle = {A {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	url = {https://ieeexplore.ieee.org/document/9233366/},
	doi = {10.1109/TNNLS.2020.3027314},
	number = {11},
	urldate = {2022-06-21},
	journal = {IEEE Trans. Neural Netw. Learning Syst.},
	author = {Tjoa, Erico and Guan, Cuntai},
	month = nov,
	year = {2021},
	pages = {4793--4813},
	file = {Full Text:/home/jacqueline/Zotero/storage/DD2U4CA9/Tjoa and Guan - 2021 - A Survey on Explainable Artificial Intelligence (X.pdf:application/pdf},
}

@misc{doran_what_2017,
	title = {What {Does} {Explainable} {AI} {Really} {Mean}? {A} {New} {Conceptualization} of {Perspectives}},
	shorttitle = {What {Does} {Explainable} {AI} {Really} {Mean}?},
	url = {http://arxiv.org/abs/1710.00794},
	abstract = {We characterize three notions of explainable AI that cut across research fields: opaque systems that offer no insight into its algo- rithmic mechanisms; interpretable systems where users can mathemat- ically analyze its algorithmic mechanisms; and comprehensible systems that emit symbols enabling user-driven explanations of how a conclusion is reached. The paper is motivated by a corpus analysis of NIPS, ACL, COGSCI, and ICCV/ECCV paper titles showing differences in how work on explainable AI is positioned in various fields. We close by introducing a fourth notion: truly explainable systems, where automated reasoning is central to output crafted explanations without requiring human post processing as final step of the generative process.},
	urldate = {2022-06-21},
	publisher = {arXiv},
	author = {Doran, Derek and Schulz, Sarah and Besold, Tarek R.},
	month = oct,
	year = {2017},
	note = {Number: arXiv:1710.00794
arXiv:1710.00794 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/4LMUDVXH/Doran et al. - 2017 - What Does Explainable AI Really Mean A New Concep.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/26U2P2YA/1710.html:text/html},
}

@misc{yang_omnixai_2022-1,
	title = {{OmniXAI}: {A} {Library} for {Explainable} {AI}},
	shorttitle = {{OmniXAI}},
	url = {http://arxiv.org/abs/2206.01612},
	abstract = {We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python library of eXplainable AI (XAI), which offers omni-way explainable AI capabilities and various interpretable machine learning techniques to address the pain points of understanding and interpreting the decisions made by machine learning (ML) in practice. OmniXAI aims to be a one-stop comprehensive library that makes explainable AI easy for data scientists, ML researchers and practitioners who need explanation for various types of data, models and explanation methods at different stages of ML process (data exploration, feature engineering, model development, evaluation, and decision-making, etc). In particular, our library includes a rich family of explanation methods integrated in a unified interface, which supports multiple data types (tabular data, images, texts, time-series), multiple types of ML models (traditional ML in Scikit-learn and deep learning models in PyTorch/TensorFlow), and a range of diverse explanation methods including "model-specific" and "model-agnostic" ones (such as feature-attribution explanation, counterfactual explanation, gradient-based explanation, etc). For practitioners, the library provides an easy-to-use unified interface to generate the explanations for their applications by only writing a few lines of codes, and also a GUI dashboard for visualization of different explanations for more insights about decisions. In this technical report, we present OmniXAI's design principles, system architectures, and major functionalities, and also demonstrate several example use cases across different types of data, tasks, and models.},
	urldate = {2022-06-21},
	publisher = {arXiv},
	author = {Yang, Wenzhuo and Le, Hung and Savarese, Silvio and Hoi, Steven C. H.},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2206.01612
arXiv:2206.01612 [cs]},
	keywords = {68T09, 68T20, 68T01, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.5, I.2.6},
	file = {arXiv Fulltext PDF:/home/jacqueline/Zotero/storage/98J4Y47Y/Yang et al. - 2022 - OmniXAI A Library for Explainable AI.pdf:application/pdf;arXiv.org Snapshot:/home/jacqueline/Zotero/storage/L2DED2R8/2206.html:text/html},
}
