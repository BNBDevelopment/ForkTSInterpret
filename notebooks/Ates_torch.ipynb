{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import sklearn\n",
    "import pickle\n",
    "import numpy as np \n",
    "import torch \n",
    "from ClassificationModels.CNN_T import ResNetBaseline, get_all_preds, fit, UCRDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "import os "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "dataset='NATOPS'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "X_train,y_train, X_test, y_test=UCR_UEA_datasets().load_dataset(dataset)\n",
    "train_x=X_train.reshape(-1,X_train.shape[-1],X_train.shape[-2])\n",
    "test_x=X_test.reshape(-1,X_train.shape[-1],X_train.shape[-2])\n",
    "train_y = y_train\n",
    "test_y=y_test\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(train_x.shape)\n",
    "print(X_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(180, 24, 51)\n",
      "(180, 51, 24)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "enc1=sklearn.preprocessing.OneHotEncoder(sparse=False).fit(train_y.reshape(-1,1))\n",
    "pickle.dump(enc1,open(f'../ClassificationModels/models/{dataset}/OneHotEncoder.pkl','wb'))\n",
    "train_y=enc1.transform(train_y.reshape(-1,1))\n",
    "test_y=enc1.transform(test_y.reshape(-1,1))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "n_pred_classes =train_y.shape[1]\n",
    "print('n pred classes',n_pred_classes) \n",
    "train_dataset = UCRDataset(train_x.astype(np.float64),train_y.astype(np.int64))\n",
    "test_dataset = UCRDataset(test_x.astype(np.float64),test_y.astype(np.int64))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=1,shuffle=False)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n pred classes 6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model = ResNetBaseline(in_channels= X_train.shape[-1], num_pred_classes=n_pred_classes)\n",
    "fit(model,train_loader,test_loader)\n",
    "if dataset in os.listdir('../ClassificationModels/models/'):\n",
    "    print('Folder exists')\n",
    "else: \n",
    "    os.mkdir(f'../ClassificationModels/models/{dataset}')\n",
    "torch.save(model.state_dict(), f'../ClassificationModels/models/{dataset}/ResNet')\n",
    "\n",
    "test_preds, ground_truth = get_all_preds(model, test_loader)\n",
    "ground_truth=np.argmax(ground_truth,axis=1)\n",
    "#test_preds=np.argmax(test_preds,axis=1)\n",
    "sns.set(rc={'figure.figsize':(5,4)})\n",
    "heatmap=confusion_matrix(ground_truth, test_preds)\n",
    "sns.heatmap(heatmap, annot=True)\n",
    "plt.savefig(f'../ClassificationModels/models/{dataset}/ResNet_confusion_matrix.png')\n",
    "plt.close()\n",
    "acc= accuracy_score(ground_truth, test_preds)\n",
    "a = classification_report(ground_truth, test_preds, output_dict=True)\n",
    "dataframe = pd.DataFrame.from_dict(a)\n",
    "dataframe.to_csv(f'../ClassificationModels/models/{dataset}/classification_report.csv', index = False)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1, Train loss: 1.186, Val loss: 1.367\n",
      "Epoch: 2, Train loss: 0.944, Val loss: 0.959\n",
      "Epoch: 3, Train loss: 0.601, Val loss: 0.59\n",
      "Epoch: 4, Train loss: 0.557, Val loss: 0.474\n",
      "Epoch: 5, Train loss: 0.372, Val loss: 0.415\n",
      "Epoch: 6, Train loss: 0.388, Val loss: 0.659\n",
      "Epoch: 7, Train loss: 0.377, Val loss: 0.596\n",
      "Epoch: 8, Train loss: 0.35, Val loss: 0.504\n",
      "Epoch: 9, Train loss: 0.337, Val loss: 0.424\n",
      "Epoch: 10, Train loss: 0.337, Val loss: 0.553\n",
      "Epoch: 11, Train loss: 0.314, Val loss: 0.409\n",
      "Epoch: 12, Train loss: 0.292, Val loss: 0.424\n",
      "Epoch: 13, Train loss: 0.308, Val loss: 0.397\n",
      "Epoch: 14, Train loss: 0.326, Val loss: 0.356\n",
      "Epoch: 15, Train loss: 0.252, Val loss: 0.427\n",
      "Epoch: 16, Train loss: 0.276, Val loss: 0.361\n",
      "Epoch: 17, Train loss: 0.309, Val loss: 0.331\n",
      "Epoch: 18, Train loss: 0.282, Val loss: 0.352\n",
      "Epoch: 19, Train loss: 0.261, Val loss: 0.34\n",
      "Epoch: 20, Train loss: 0.237, Val loss: 0.339\n",
      "Epoch: 21, Train loss: 0.252, Val loss: 0.491\n",
      "Epoch: 22, Train loss: 0.277, Val loss: 0.345\n",
      "Epoch: 23, Train loss: 0.284, Val loss: 0.371\n",
      "Epoch: 24, Train loss: 0.501, Val loss: 0.528\n",
      "Epoch: 25, Train loss: 0.449, Val loss: 0.483\n",
      "Epoch: 26, Train loss: 0.358, Val loss: 0.363\n",
      "Epoch: 27, Train loss: 0.317, Val loss: 0.29\n",
      "Epoch: 28, Train loss: 0.274, Val loss: 0.396\n",
      "Epoch: 29, Train loss: 0.258, Val loss: 0.371\n",
      "Epoch: 30, Train loss: 0.25, Val loss: 0.366\n",
      "Epoch: 31, Train loss: 0.247, Val loss: 0.347\n",
      "Epoch: 32, Train loss: 0.219, Val loss: 0.373\n",
      "Epoch: 33, Train loss: 0.235, Val loss: 0.388\n",
      "Epoch: 34, Train loss: 0.213, Val loss: 0.36\n",
      "Epoch: 35, Train loss: 0.225, Val loss: 0.528\n",
      "Epoch: 36, Train loss: 0.229, Val loss: 0.311\n",
      "Epoch: 37, Train loss: 0.231, Val loss: 0.602\n",
      "Epoch: 38, Train loss: 0.324, Val loss: 0.529\n",
      "Epoch: 39, Train loss: 0.345, Val loss: 0.401\n",
      "Epoch: 40, Train loss: 0.392, Val loss: 1.162\n",
      "Epoch: 41, Train loss: 0.655, Val loss: 0.647\n",
      "Epoch: 42, Train loss: 0.405, Val loss: 0.396\n",
      "Epoch: 43, Train loss: 0.341, Val loss: 0.279\n",
      "Epoch: 44, Train loss: 0.44, Val loss: 0.546\n",
      "Epoch: 45, Train loss: 0.411, Val loss: 0.33\n",
      "Epoch: 46, Train loss: 0.291, Val loss: 0.307\n",
      "Epoch: 47, Train loss: 0.278, Val loss: 0.327\n",
      "Epoch: 48, Train loss: 0.244, Val loss: 0.351\n",
      "Epoch: 49, Train loss: 0.239, Val loss: 0.379\n",
      "Epoch: 50, Train loss: 0.218, Val loss: 0.41\n",
      "Epoch: 51, Train loss: 0.234, Val loss: 0.404\n",
      "Epoch: 52, Train loss: 0.226, Val loss: 0.401\n",
      "Epoch: 53, Train loss: 0.218, Val loss: 0.433\n",
      "Epoch: 54, Train loss: 0.222, Val loss: 0.391\n",
      "Epoch: 55, Train loss: 0.218, Val loss: 0.406\n",
      "Epoch: 56, Train loss: 0.218, Val loss: 0.377\n",
      "Epoch: 57, Train loss: 0.214, Val loss: 0.433\n",
      "Epoch: 58, Train loss: 0.21, Val loss: 0.374\n",
      "Epoch: 59, Train loss: 0.239, Val loss: 0.461\n",
      "Epoch: 60, Train loss: 0.221, Val loss: 0.345\n",
      "Epoch: 61, Train loss: 0.217, Val loss: 0.381\n",
      "Epoch: 62, Train loss: 0.212, Val loss: 0.39\n",
      "Epoch: 63, Train loss: 0.207, Val loss: 0.6\n",
      "Epoch: 64, Train loss: 0.262, Val loss: 0.35\n",
      "Epoch: 65, Train loss: 0.254, Val loss: 0.315\n",
      "Epoch: 66, Train loss: 0.234, Val loss: 0.375\n",
      "Epoch: 67, Train loss: 0.209, Val loss: 0.514\n",
      "Epoch: 68, Train loss: 0.207, Val loss: 0.42\n",
      "Epoch: 69, Train loss: 0.205, Val loss: 0.425\n",
      "Epoch: 70, Train loss: 0.238, Val loss: 0.349\n",
      "Epoch: 71, Train loss: 0.214, Val loss: 0.483\n",
      "Epoch: 72, Train loss: 0.216, Val loss: 0.45\n",
      "Epoch: 73, Train loss: 0.209, Val loss: 0.58\n",
      "Epoch: 74, Train loss: 0.194, Val loss: 0.401\n",
      "Epoch: 75, Train loss: 0.217, Val loss: 0.439\n",
      "Epoch: 76, Train loss: 0.207, Val loss: 0.547\n",
      "Epoch: 77, Train loss: 0.205, Val loss: 0.536\n",
      "Epoch: 78, Train loss: 0.181, Val loss: 0.882\n",
      "Epoch: 79, Train loss: 0.287, Val loss: 0.549\n",
      "Epoch: 80, Train loss: 0.266, Val loss: 0.462\n",
      "Epoch: 81, Train loss: 0.214, Val loss: 0.363\n",
      "Epoch: 82, Train loss: 0.205, Val loss: 0.514\n",
      "Epoch: 83, Train loss: 0.193, Val loss: 0.399\n",
      "Epoch: 84, Train loss: 0.19, Val loss: 0.502\n",
      "Epoch: 85, Train loss: 0.203, Val loss: 0.44\n",
      "Epoch: 86, Train loss: 0.155, Val loss: 0.557\n",
      "Epoch: 87, Train loss: 0.18, Val loss: 0.476\n",
      "Epoch: 88, Train loss: 0.158, Val loss: 0.457\n",
      "Epoch: 89, Train loss: 0.182, Val loss: 0.619\n",
      "Epoch: 90, Train loss: 0.387, Val loss: 0.642\n",
      "Epoch: 91, Train loss: 0.289, Val loss: 0.39\n",
      "Epoch: 92, Train loss: 0.247, Val loss: 0.378\n",
      "Epoch: 93, Train loss: 0.237, Val loss: 0.487\n",
      "Epoch: 94, Train loss: 0.224, Val loss: 0.455\n",
      "Epoch: 95, Train loss: 0.414, Val loss: 1.081\n",
      "Epoch: 96, Train loss: 0.385, Val loss: 0.558\n",
      "Epoch: 97, Train loss: 0.342, Val loss: 0.412\n",
      "Epoch: 98, Train loss: 0.304, Val loss: 0.326\n",
      "Epoch: 99, Train loss: 0.236, Val loss: 0.626\n",
      "Epoch: 100, Train loss: 0.232, Val loss: 0.623\n",
      "Epoch: 101, Train loss: 0.337, Val loss: 0.526\n",
      "Epoch: 102, Train loss: 0.264, Val loss: 0.388\n",
      "Epoch: 103, Train loss: 0.207, Val loss: 0.438\n",
      "Epoch: 104, Train loss: 0.202, Val loss: 0.49\n",
      "Epoch: 105, Train loss: 0.202, Val loss: 0.38\n",
      "Epoch: 106, Train loss: 0.189, Val loss: 0.375\n",
      "Epoch: 107, Train loss: 0.161, Val loss: 0.524\n",
      "Epoch: 108, Train loss: 0.177, Val loss: 0.591\n",
      "Epoch: 109, Train loss: 0.19, Val loss: 0.574\n",
      "Epoch: 110, Train loss: 0.165, Val loss: 0.525\n",
      "Epoch: 111, Train loss: 0.151, Val loss: 0.391\n",
      "Epoch: 112, Train loss: 0.183, Val loss: 0.603\n",
      "Epoch: 113, Train loss: 0.129, Val loss: 0.404\n",
      "Epoch: 114, Train loss: 0.171, Val loss: 0.568\n",
      "Epoch: 115, Train loss: 0.304, Val loss: 0.659\n",
      "Epoch: 116, Train loss: 0.281, Val loss: 0.491\n",
      "Epoch: 117, Train loss: 0.272, Val loss: 0.624\n",
      "Epoch: 118, Train loss: 0.379, Val loss: 0.634\n",
      "Epoch: 119, Train loss: 0.393, Val loss: 0.533\n",
      "Epoch: 120, Train loss: 0.239, Val loss: 0.351\n",
      "Epoch: 121, Train loss: 0.17, Val loss: 0.376\n",
      "Epoch: 122, Train loss: 0.189, Val loss: 0.452\n",
      "Epoch: 123, Train loss: 0.174, Val loss: 0.625\n",
      "Epoch: 124, Train loss: 0.25, Val loss: 0.332\n",
      "Epoch: 125, Train loss: 0.201, Val loss: 0.385\n",
      "Epoch: 126, Train loss: 0.176, Val loss: 0.337\n",
      "Epoch: 127, Train loss: 0.174, Val loss: 0.586\n",
      "Epoch: 128, Train loss: 0.149, Val loss: 0.37\n",
      "Epoch: 129, Train loss: 0.128, Val loss: 0.453\n",
      "Epoch: 130, Train loss: 0.135, Val loss: 0.557\n",
      "Epoch: 131, Train loss: 0.094, Val loss: 0.553\n",
      "Epoch: 132, Train loss: 0.072, Val loss: 0.948\n",
      "Epoch: 133, Train loss: 0.059, Val loss: 0.723\n",
      "Epoch: 134, Train loss: 0.102, Val loss: 0.76\n",
      "Epoch: 135, Train loss: 0.404, Val loss: 2.523\n",
      "Epoch: 136, Train loss: 0.455, Val loss: 0.389\n",
      "Epoch: 137, Train loss: 0.319, Val loss: 0.364\n",
      "Epoch: 138, Train loss: 0.257, Val loss: 0.411\n",
      "Epoch: 139, Train loss: 0.233, Val loss: 0.365\n",
      "Epoch: 140, Train loss: 0.191, Val loss: 0.361\n",
      "Epoch: 141, Train loss: 0.191, Val loss: 0.372\n",
      "Epoch: 142, Train loss: 0.18, Val loss: 0.361\n",
      "Epoch: 143, Train loss: 0.154, Val loss: 0.372\n",
      "Early stopping!\n",
      "Folder exists\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(X_train.shape[-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(180, 24, 51)\n",
      "(180, 24, 51)\n",
      "24\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explanation Algo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "item=test_x[0].reshape(1,test_x.shape[1],-1)\n",
    "shape=item.shape\n",
    "_item=  torch.from_numpy(item).float()\n",
    "model.eval()\n",
    "#y_target= model(_item).detach().numpy()\n",
    "print(model(_item))\n",
    "y_target = torch.nn.functional.softmax(model(_item)).detach().numpy()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ -2.8414, -44.8772, -88.3838,  92.3523, -28.3477,  -0.6938]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/jacqueline/.local/share/virtualenvs/TSInterpret-NXJYnQDU/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "print(y_target)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[4.5486e-42 0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00 3.8957e-41]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from TSInterpret.InterpretabilityModels.counterfactual.Ates import AtesCF\n",
    "exp_model= AtesCF(model,(train_x,train_y),backend='PYT',mode='feat')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-06-27 15:53:04.039055: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-27 15:53:04.039093: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#TODO Target Calculation  --> Why is opt not working\n",
    "exp = exp_model.explain(item,3, method= 'opt')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Run Brute Force as Backup.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\n",
    "array, label=exp\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "array"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[-0.597549, -1.897467, -0.689874, ..., -0.592066, -1.912729,\n",
       "         -0.702529],\n",
       "        [ 0.599973, -1.917021, -0.838037, ...,  0.597675, -1.918653,\n",
       "         -0.842211],\n",
       "        [-0.594716, -0.835395, -0.121241, ..., -0.599888, -0.82464 ,\n",
       "         -0.119868],\n",
       "        ...,\n",
       "        [ 0.627078, -1.620468, -0.702724, ...,  0.76356 , -1.463721,\n",
       "         -0.630478],\n",
       "        [-0.767244, -1.756   , -0.682146, ..., -0.778381, -1.726662,\n",
       "         -0.68518 ],\n",
       "        [ 0.791354, -1.66717 , -0.84923 , ...,  0.670227, -1.707901,\n",
       "         -0.787852]]])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot Item "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "i=0\n",
    "org_label=0\n",
    "cf_label=label[0]\n",
    "exp=array\n",
    "\n",
    "exp_model.plot(item,org_label,exp,cf_label,figsize=(15,15))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Items are identical\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit ('TSInterpret-NXJYnQDU': pipenv)"
  },
  "interpreter": {
   "hash": "16db99a3fba429d86cb96b96f3cee6d3141a37b91b1d019974802969710b6701"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}