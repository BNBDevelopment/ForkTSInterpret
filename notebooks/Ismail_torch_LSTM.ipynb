{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from TSInterpret.data import load_data\n",
    "import sklearn\n",
    "import pickle\n",
    "import numpy as np \n",
    "import torch \n",
    "from ClassificationModels.CNN_T import ResNetBaseline, get_all_preds, fit, UCRDataset\n",
    "from ClassificationModels.LSTM_T import LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import os \n",
    "from tslearn.datasets import UCR_UEA_datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#dataset='GunPoint'f\n",
    "dataset='BasicMotions'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#TODO include OneHot Encoding Options\n",
    "#train_x,test_x, train_y, test_y = load_data.load_basic_dataset(dataset,scaling=None,mode='time',cwd='/media/jacqueline/Data/UCRArchive_2018/')\n",
    "train_x,train_y, test_x, test_y=UCR_UEA_datasets().load_dataset(dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_x.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(40, 100, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(test_y.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(40,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#enc1=sklearn.preprocessing.OneHotEncoder(sparse=False).fit(np.vstack((train_y.reshape(-1,1),test_y.reshape(-1,1))))\n",
    "#pickle.dump(enc1,open(f'./ClassificationModels/models/{dataset}/OneHotEncoder.pkl','wb'))\n",
    "enc1=sklearn.preprocessing.OneHotEncoder(sparse=False).fit(np.vstack((train_y.reshape(-1,1),test_y.reshape(-1,1))))\n",
    "pickle.dump(enc1,open(f'../ClassificationModels/models/{dataset}/OneHotEncoder.pkl','wb'))\n",
    "\n",
    "train_y=enc1.transform(train_y.reshape(-1,1))\n",
    "test_y=enc1.transform(test_y.reshape(-1,1))\n",
    "    \n",
    "#enc1=pickle.load(open(f'./ClassificationModels/models/{dataset}/OneHotEncoder.pkl','rb'))\n",
    "#train_y=enc1.transform(train_y.reshape(-1,1))\n",
    "#test_y=enc1.transform(test_y.reshape(-1,1))\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "n_pred_classes =train_y.shape[1]\n",
    "train_dataset = UCRDataset(train_x.astype(np.float64),train_y.astype(np.int64))\n",
    "test_dataset = UCRDataset(test_x.astype(np.float64),test_y.astype(np.int64))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "device='cpu'\n",
    "hidden_size=10\n",
    "rnn=0.1\n",
    "model = LSTM(6, hidden_size ,n_pred_classes,rnndropout=rnn).to(device)\n",
    "fit(model,train_loader,test_loader)\n",
    "if dataset in os.listdir('../ClassificationModels/models/'):\n",
    "    print('Folder exists')\n",
    "else: \n",
    "    os.mkdir(f'../ClassificationModels/models/{dataset}')\n",
    "torch.save(model.state_dict(), f'../ClassificationModels/models/{dataset}/LSTM')\n",
    "#model.load_state_dict(torch.load(f'./ClassificationModels/models/{dataset}/ResNet'))\n",
    "#model.eval()\n",
    "\n",
    "test_preds, ground_truth = get_all_preds(model, test_loader)\n",
    "ground_truth=np.argmax(ground_truth,axis=1)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(5,4)})\n",
    "heatmap=confusion_matrix(ground_truth, test_preds)\n",
    "sns.heatmap(heatmap, annot=True)\n",
    "plt.savefig(f'../ClassificationModels/models/{dataset}/LSTM_confusion_matrix.png')\n",
    "plt.close()\n",
    "acc= accuracy_score(ground_truth, test_preds)\n",
    "a = classification_report(ground_truth, test_preds, output_dict=True)\n",
    "dataframe = pd.DataFrame.from_dict(a)\n",
    "dataframe.to_csv(f'../ClassificationModels/models/{dataset}/LSTMclassification_report.csv', index = False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1, Train loss: 1.386, Val loss: 1.395\n",
      "Epoch: 2, Train loss: 1.382, Val loss: 1.393\n",
      "Epoch: 3, Train loss: 1.38, Val loss: 1.392\n",
      "Epoch: 4, Train loss: 1.375, Val loss: 1.39\n",
      "Epoch: 5, Train loss: 1.375, Val loss: 1.388\n",
      "Epoch: 6, Train loss: 1.372, Val loss: 1.386\n",
      "Epoch: 7, Train loss: 1.372, Val loss: 1.385\n",
      "Epoch: 8, Train loss: 1.367, Val loss: 1.383\n",
      "Epoch: 9, Train loss: 1.36, Val loss: 1.381\n",
      "Epoch: 10, Train loss: 1.359, Val loss: 1.38\n",
      "Epoch: 11, Train loss: 1.351, Val loss: 1.378\n",
      "Epoch: 12, Train loss: 1.353, Val loss: 1.376\n",
      "Epoch: 13, Train loss: 1.355, Val loss: 1.374\n",
      "Epoch: 14, Train loss: 1.344, Val loss: 1.373\n",
      "Epoch: 15, Train loss: 1.337, Val loss: 1.371\n",
      "Epoch: 16, Train loss: 1.343, Val loss: 1.369\n",
      "Epoch: 17, Train loss: 1.344, Val loss: 1.367\n",
      "Epoch: 18, Train loss: 1.34, Val loss: 1.365\n",
      "Epoch: 19, Train loss: 1.334, Val loss: 1.364\n",
      "Epoch: 20, Train loss: 1.338, Val loss: 1.362\n",
      "Epoch: 21, Train loss: 1.338, Val loss: 1.36\n",
      "Epoch: 22, Train loss: 1.322, Val loss: 1.358\n",
      "Epoch: 23, Train loss: 1.32, Val loss: 1.356\n",
      "Epoch: 24, Train loss: 1.326, Val loss: 1.354\n",
      "Epoch: 25, Train loss: 1.316, Val loss: 1.352\n",
      "Epoch: 26, Train loss: 1.309, Val loss: 1.35\n",
      "Epoch: 27, Train loss: 1.315, Val loss: 1.347\n",
      "Epoch: 28, Train loss: 1.303, Val loss: 1.345\n",
      "Epoch: 29, Train loss: 1.315, Val loss: 1.342\n",
      "Epoch: 30, Train loss: 1.298, Val loss: 1.34\n",
      "Epoch: 31, Train loss: 1.299, Val loss: 1.337\n",
      "Epoch: 32, Train loss: 1.293, Val loss: 1.334\n",
      "Epoch: 33, Train loss: 1.285, Val loss: 1.331\n",
      "Epoch: 34, Train loss: 1.292, Val loss: 1.328\n",
      "Epoch: 35, Train loss: 1.283, Val loss: 1.325\n",
      "Epoch: 36, Train loss: 1.289, Val loss: 1.322\n",
      "Epoch: 37, Train loss: 1.273, Val loss: 1.318\n",
      "Epoch: 38, Train loss: 1.269, Val loss: 1.315\n",
      "Epoch: 39, Train loss: 1.264, Val loss: 1.311\n",
      "Epoch: 40, Train loss: 1.246, Val loss: 1.308\n",
      "Epoch: 41, Train loss: 1.263, Val loss: 1.304\n",
      "Epoch: 42, Train loss: 1.248, Val loss: 1.3\n",
      "Epoch: 43, Train loss: 1.236, Val loss: 1.296\n",
      "Epoch: 44, Train loss: 1.246, Val loss: 1.292\n",
      "Epoch: 45, Train loss: 1.23, Val loss: 1.287\n",
      "Epoch: 46, Train loss: 1.236, Val loss: 1.283\n",
      "Epoch: 47, Train loss: 1.221, Val loss: 1.279\n",
      "Epoch: 48, Train loss: 1.225, Val loss: 1.275\n",
      "Epoch: 49, Train loss: 1.225, Val loss: 1.27\n",
      "Epoch: 50, Train loss: 1.213, Val loss: 1.266\n",
      "Epoch: 51, Train loss: 1.222, Val loss: 1.262\n",
      "Epoch: 52, Train loss: 1.194, Val loss: 1.258\n",
      "Epoch: 53, Train loss: 1.183, Val loss: 1.253\n",
      "Epoch: 54, Train loss: 1.207, Val loss: 1.249\n",
      "Epoch: 55, Train loss: 1.182, Val loss: 1.244\n",
      "Epoch: 56, Train loss: 1.169, Val loss: 1.24\n",
      "Epoch: 57, Train loss: 1.174, Val loss: 1.235\n",
      "Epoch: 58, Train loss: 1.161, Val loss: 1.23\n",
      "Epoch: 59, Train loss: 1.162, Val loss: 1.226\n",
      "Epoch: 60, Train loss: 1.135, Val loss: 1.221\n",
      "Epoch: 61, Train loss: 1.154, Val loss: 1.216\n",
      "Epoch: 62, Train loss: 1.142, Val loss: 1.211\n",
      "Epoch: 63, Train loss: 1.121, Val loss: 1.206\n",
      "Epoch: 64, Train loss: 1.119, Val loss: 1.201\n",
      "Epoch: 65, Train loss: 1.125, Val loss: 1.195\n",
      "Epoch: 66, Train loss: 1.117, Val loss: 1.19\n",
      "Epoch: 67, Train loss: 1.111, Val loss: 1.185\n",
      "Epoch: 68, Train loss: 1.085, Val loss: 1.18\n",
      "Epoch: 69, Train loss: 1.083, Val loss: 1.175\n",
      "Epoch: 70, Train loss: 1.07, Val loss: 1.17\n",
      "Epoch: 71, Train loss: 1.062, Val loss: 1.165\n",
      "Epoch: 72, Train loss: 1.069, Val loss: 1.16\n",
      "Epoch: 73, Train loss: 1.057, Val loss: 1.155\n",
      "Epoch: 74, Train loss: 1.065, Val loss: 1.15\n",
      "Epoch: 75, Train loss: 1.029, Val loss: 1.145\n",
      "Epoch: 76, Train loss: 1.03, Val loss: 1.141\n",
      "Epoch: 77, Train loss: 1.035, Val loss: 1.136\n",
      "Epoch: 78, Train loss: 1.028, Val loss: 1.132\n",
      "Epoch: 79, Train loss: 1.015, Val loss: 1.128\n",
      "Epoch: 80, Train loss: 1.018, Val loss: 1.123\n",
      "Epoch: 81, Train loss: 1.001, Val loss: 1.12\n",
      "Epoch: 82, Train loss: 1.004, Val loss: 1.116\n",
      "Epoch: 83, Train loss: 0.984, Val loss: 1.112\n",
      "Epoch: 84, Train loss: 0.977, Val loss: 1.109\n",
      "Epoch: 85, Train loss: 0.972, Val loss: 1.105\n",
      "Epoch: 86, Train loss: 0.969, Val loss: 1.101\n",
      "Epoch: 87, Train loss: 0.964, Val loss: 1.098\n",
      "Epoch: 88, Train loss: 0.964, Val loss: 1.094\n",
      "Epoch: 89, Train loss: 0.967, Val loss: 1.091\n",
      "Epoch: 90, Train loss: 0.956, Val loss: 1.093\n",
      "Epoch: 91, Train loss: 0.936, Val loss: 1.093\n",
      "Epoch: 92, Train loss: 0.938, Val loss: 1.087\n",
      "Epoch: 93, Train loss: 0.927, Val loss: 1.086\n",
      "Epoch: 94, Train loss: 0.94, Val loss: 1.079\n",
      "Epoch: 95, Train loss: 0.919, Val loss: 1.078\n",
      "Epoch: 96, Train loss: 0.915, Val loss: 1.077\n",
      "Epoch: 97, Train loss: 0.905, Val loss: 1.077\n",
      "Epoch: 98, Train loss: 0.901, Val loss: 1.074\n",
      "Epoch: 99, Train loss: 0.908, Val loss: 1.071\n",
      "Epoch: 100, Train loss: 0.895, Val loss: 1.069\n",
      "Epoch: 101, Train loss: 0.894, Val loss: 1.068\n",
      "Epoch: 102, Train loss: 0.885, Val loss: 1.068\n",
      "Epoch: 103, Train loss: 0.884, Val loss: 1.058\n",
      "Epoch: 104, Train loss: 0.88, Val loss: 1.05\n",
      "Epoch: 105, Train loss: 0.879, Val loss: 1.048\n",
      "Epoch: 106, Train loss: 0.871, Val loss: 1.043\n",
      "Epoch: 107, Train loss: 0.874, Val loss: 1.04\n",
      "Epoch: 108, Train loss: 0.864, Val loss: 1.037\n",
      "Epoch: 109, Train loss: 0.856, Val loss: 1.034\n",
      "Epoch: 110, Train loss: 0.868, Val loss: 1.031\n",
      "Epoch: 111, Train loss: 0.867, Val loss: 1.029\n",
      "Epoch: 112, Train loss: 0.865, Val loss: 1.026\n",
      "Epoch: 113, Train loss: 0.852, Val loss: 1.024\n",
      "Epoch: 114, Train loss: 0.844, Val loss: 1.022\n",
      "Epoch: 115, Train loss: 0.848, Val loss: 1.019\n",
      "Epoch: 116, Train loss: 0.844, Val loss: 1.017\n",
      "Epoch: 117, Train loss: 0.841, Val loss: 1.015\n",
      "Epoch: 118, Train loss: 0.84, Val loss: 1.018\n",
      "Epoch: 119, Train loss: 0.835, Val loss: 1.04\n",
      "Epoch: 120, Train loss: 0.83, Val loss: 1.046\n",
      "Epoch: 121, Train loss: 0.829, Val loss: 1.064\n",
      "Epoch: 122, Train loss: 0.831, Val loss: 1.056\n",
      "Epoch: 123, Train loss: 0.829, Val loss: 1.044\n",
      "Epoch: 124, Train loss: 0.829, Val loss: 1.044\n",
      "Epoch: 125, Train loss: 0.822, Val loss: 1.047\n",
      "Epoch: 126, Train loss: 0.827, Val loss: 1.052\n",
      "Epoch: 127, Train loss: 0.819, Val loss: 1.043\n",
      "Epoch: 128, Train loss: 0.812, Val loss: 1.02\n",
      "Epoch: 129, Train loss: 0.813, Val loss: 1.009\n",
      "Epoch: 130, Train loss: 0.816, Val loss: 1.007\n",
      "Epoch: 131, Train loss: 0.81, Val loss: 1.009\n",
      "Epoch: 132, Train loss: 0.815, Val loss: 1.013\n",
      "Epoch: 133, Train loss: 0.807, Val loss: 1.017\n",
      "Epoch: 134, Train loss: 0.807, Val loss: 1.017\n",
      "Epoch: 135, Train loss: 0.808, Val loss: 1.014\n",
      "Epoch: 136, Train loss: 0.804, Val loss: 1.011\n",
      "Epoch: 137, Train loss: 0.801, Val loss: 1.007\n",
      "Epoch: 138, Train loss: 0.8, Val loss: 1.002\n",
      "Epoch: 139, Train loss: 0.8, Val loss: 1.002\n",
      "Epoch: 140, Train loss: 0.8, Val loss: 1.006\n",
      "Epoch: 141, Train loss: 0.805, Val loss: 0.981\n",
      "Epoch: 142, Train loss: 0.819, Val loss: 0.98\n",
      "Epoch: 143, Train loss: 0.817, Val loss: 0.979\n",
      "Epoch: 144, Train loss: 0.821, Val loss: 0.978\n",
      "Epoch: 145, Train loss: 0.817, Val loss: 0.978\n",
      "Epoch: 146, Train loss: 0.835, Val loss: 0.976\n",
      "Epoch: 147, Train loss: 0.834, Val loss: 0.975\n",
      "Epoch: 148, Train loss: 0.833, Val loss: 0.974\n",
      "Epoch: 149, Train loss: 0.81, Val loss: 0.973\n",
      "Epoch: 150, Train loss: 0.811, Val loss: 0.972\n",
      "Epoch: 151, Train loss: 0.813, Val loss: 0.971\n",
      "Epoch: 152, Train loss: 0.81, Val loss: 0.971\n",
      "Epoch: 153, Train loss: 0.81, Val loss: 0.97\n",
      "Epoch: 154, Train loss: 0.793, Val loss: 0.945\n",
      "Epoch: 155, Train loss: 0.787, Val loss: 0.943\n",
      "Epoch: 156, Train loss: 0.791, Val loss: 0.941\n",
      "Epoch: 157, Train loss: 0.788, Val loss: 0.939\n",
      "Epoch: 158, Train loss: 0.786, Val loss: 0.937\n",
      "Epoch: 159, Train loss: 0.789, Val loss: 0.936\n",
      "Epoch: 160, Train loss: 0.785, Val loss: 0.935\n",
      "Epoch: 161, Train loss: 0.785, Val loss: 0.934\n",
      "Epoch: 162, Train loss: 0.784, Val loss: 0.933\n",
      "Epoch: 163, Train loss: 0.786, Val loss: 0.931\n",
      "Epoch: 164, Train loss: 0.785, Val loss: 0.93\n",
      "Epoch: 165, Train loss: 0.786, Val loss: 0.929\n",
      "Epoch: 166, Train loss: 0.785, Val loss: 0.928\n",
      "Epoch: 167, Train loss: 0.785, Val loss: 0.927\n",
      "Epoch: 168, Train loss: 0.782, Val loss: 0.926\n",
      "Epoch: 169, Train loss: 0.782, Val loss: 0.925\n",
      "Epoch: 170, Train loss: 0.783, Val loss: 0.924\n",
      "Epoch: 171, Train loss: 0.781, Val loss: 0.924\n",
      "Epoch: 172, Train loss: 0.784, Val loss: 0.923\n",
      "Epoch: 173, Train loss: 0.778, Val loss: 0.922\n",
      "Epoch: 174, Train loss: 0.779, Val loss: 0.921\n",
      "Epoch: 175, Train loss: 0.779, Val loss: 0.92\n",
      "Epoch: 176, Train loss: 0.779, Val loss: 0.919\n",
      "Epoch: 177, Train loss: 0.778, Val loss: 0.918\n",
      "Epoch: 178, Train loss: 0.776, Val loss: 0.917\n",
      "Epoch: 179, Train loss: 0.777, Val loss: 0.916\n",
      "Epoch: 180, Train loss: 0.776, Val loss: 0.916\n",
      "Epoch: 181, Train loss: 0.775, Val loss: 0.915\n",
      "Epoch: 182, Train loss: 0.777, Val loss: 0.914\n",
      "Epoch: 183, Train loss: 0.776, Val loss: 0.913\n",
      "Epoch: 184, Train loss: 0.774, Val loss: 0.912\n",
      "Epoch: 185, Train loss: 0.774, Val loss: 0.912\n",
      "Epoch: 186, Train loss: 0.774, Val loss: 0.911\n",
      "Epoch: 187, Train loss: 0.774, Val loss: 0.91\n",
      "Epoch: 188, Train loss: 0.775, Val loss: 0.909\n",
      "Epoch: 189, Train loss: 0.772, Val loss: 0.908\n",
      "Epoch: 190, Train loss: 0.773, Val loss: 0.908\n",
      "Epoch: 191, Train loss: 0.772, Val loss: 0.907\n",
      "Epoch: 192, Train loss: 0.772, Val loss: 0.906\n",
      "Epoch: 193, Train loss: 0.771, Val loss: 0.905\n",
      "Epoch: 194, Train loss: 0.77, Val loss: 0.904\n",
      "Epoch: 195, Train loss: 0.771, Val loss: 0.903\n",
      "Epoch: 196, Train loss: 0.769, Val loss: 0.902\n",
      "Epoch: 197, Train loss: 0.77, Val loss: 0.9\n",
      "Epoch: 198, Train loss: 0.769, Val loss: 0.898\n",
      "Epoch: 199, Train loss: 0.769, Val loss: 0.895\n",
      "Epoch: 200, Train loss: 0.769, Val loss: 0.892\n",
      "Epoch: 201, Train loss: 0.767, Val loss: 0.891\n",
      "Epoch: 202, Train loss: 0.767, Val loss: 0.89\n",
      "Epoch: 203, Train loss: 0.768, Val loss: 0.889\n",
      "Epoch: 204, Train loss: 0.767, Val loss: 0.888\n",
      "Epoch: 205, Train loss: 0.767, Val loss: 0.887\n",
      "Epoch: 206, Train loss: 0.766, Val loss: 0.886\n",
      "Epoch: 207, Train loss: 0.768, Val loss: 0.886\n",
      "Epoch: 208, Train loss: 0.767, Val loss: 0.885\n",
      "Epoch: 209, Train loss: 0.766, Val loss: 0.884\n",
      "Epoch: 210, Train loss: 0.765, Val loss: 0.883\n",
      "Epoch: 211, Train loss: 0.767, Val loss: 0.882\n",
      "Epoch: 212, Train loss: 0.765, Val loss: 0.881\n",
      "Epoch: 213, Train loss: 0.766, Val loss: 0.88\n",
      "Epoch: 214, Train loss: 0.766, Val loss: 0.879\n",
      "Epoch: 215, Train loss: 0.764, Val loss: 0.878\n",
      "Epoch: 216, Train loss: 0.766, Val loss: 0.877\n",
      "Epoch: 217, Train loss: 0.764, Val loss: 0.876\n",
      "Epoch: 218, Train loss: 0.765, Val loss: 0.875\n",
      "Epoch: 219, Train loss: 0.764, Val loss: 0.875\n",
      "Epoch: 220, Train loss: 0.764, Val loss: 0.875\n",
      "Epoch: 221, Train loss: 0.765, Val loss: 0.874\n",
      "Epoch: 222, Train loss: 0.763, Val loss: 0.874\n",
      "Epoch: 223, Train loss: 0.764, Val loss: 0.873\n",
      "Epoch: 224, Train loss: 0.764, Val loss: 0.873\n",
      "Epoch: 225, Train loss: 0.764, Val loss: 0.872\n",
      "Epoch: 226, Train loss: 0.763, Val loss: 0.872\n",
      "Epoch: 227, Train loss: 0.763, Val loss: 0.871\n",
      "Epoch: 228, Train loss: 0.762, Val loss: 0.87\n",
      "Epoch: 229, Train loss: 0.762, Val loss: 0.87\n",
      "Epoch: 230, Train loss: 0.762, Val loss: 0.869\n",
      "Epoch: 231, Train loss: 0.761, Val loss: 0.869\n",
      "Epoch: 232, Train loss: 0.761, Val loss: 0.868\n",
      "Epoch: 233, Train loss: 0.761, Val loss: 0.867\n",
      "Epoch: 234, Train loss: 0.762, Val loss: 0.867\n",
      "Epoch: 235, Train loss: 0.762, Val loss: 0.866\n",
      "Epoch: 236, Train loss: 0.761, Val loss: 0.866\n",
      "Epoch: 237, Train loss: 0.762, Val loss: 0.865\n",
      "Epoch: 238, Train loss: 0.761, Val loss: 0.865\n",
      "Epoch: 239, Train loss: 0.762, Val loss: 0.864\n",
      "Epoch: 240, Train loss: 0.762, Val loss: 0.864\n",
      "Epoch: 241, Train loss: 0.761, Val loss: 0.863\n",
      "Epoch: 242, Train loss: 0.76, Val loss: 0.862\n",
      "Epoch: 243, Train loss: 0.761, Val loss: 0.862\n",
      "Epoch: 244, Train loss: 0.76, Val loss: 0.861\n",
      "Epoch: 245, Train loss: 0.76, Val loss: 0.861\n",
      "Epoch: 246, Train loss: 0.76, Val loss: 0.86\n",
      "Epoch: 247, Train loss: 0.76, Val loss: 0.86\n",
      "Epoch: 248, Train loss: 0.76, Val loss: 0.859\n",
      "Epoch: 249, Train loss: 0.761, Val loss: 0.859\n",
      "Epoch: 250, Train loss: 0.759, Val loss: 0.858\n",
      "Epoch: 251, Train loss: 0.759, Val loss: 0.858\n",
      "Epoch: 252, Train loss: 0.76, Val loss: 0.857\n",
      "Epoch: 253, Train loss: 0.759, Val loss: 0.857\n",
      "Epoch: 254, Train loss: 0.758, Val loss: 0.856\n",
      "Epoch: 255, Train loss: 0.759, Val loss: 0.856\n",
      "Epoch: 256, Train loss: 0.759, Val loss: 0.855\n",
      "Epoch: 257, Train loss: 0.758, Val loss: 0.855\n",
      "Epoch: 258, Train loss: 0.758, Val loss: 0.855\n",
      "Epoch: 259, Train loss: 0.759, Val loss: 0.854\n",
      "Epoch: 260, Train loss: 0.758, Val loss: 0.854\n",
      "Epoch: 261, Train loss: 0.757, Val loss: 0.853\n",
      "Epoch: 262, Train loss: 0.758, Val loss: 0.853\n",
      "Epoch: 263, Train loss: 0.758, Val loss: 0.853\n",
      "Epoch: 264, Train loss: 0.758, Val loss: 0.852\n",
      "Epoch: 265, Train loss: 0.757, Val loss: 0.852\n",
      "Epoch: 266, Train loss: 0.758, Val loss: 0.852\n",
      "Epoch: 267, Train loss: 0.757, Val loss: 0.851\n",
      "Epoch: 268, Train loss: 0.757, Val loss: 0.851\n",
      "Epoch: 269, Train loss: 0.757, Val loss: 0.851\n",
      "Epoch: 270, Train loss: 0.758, Val loss: 0.85\n",
      "Epoch: 271, Train loss: 0.757, Val loss: 0.85\n",
      "Epoch: 272, Train loss: 0.756, Val loss: 0.85\n",
      "Epoch: 273, Train loss: 0.757, Val loss: 0.849\n",
      "Epoch: 274, Train loss: 0.758, Val loss: 0.849\n",
      "Epoch: 275, Train loss: 0.757, Val loss: 0.849\n",
      "Epoch: 276, Train loss: 0.756, Val loss: 0.849\n",
      "Epoch: 277, Train loss: 0.757, Val loss: 0.848\n",
      "Epoch: 278, Train loss: 0.757, Val loss: 0.848\n",
      "Epoch: 279, Train loss: 0.757, Val loss: 0.848\n",
      "Epoch: 280, Train loss: 0.757, Val loss: 0.847\n",
      "Epoch: 281, Train loss: 0.757, Val loss: 0.847\n",
      "Epoch: 282, Train loss: 0.756, Val loss: 0.847\n",
      "Epoch: 283, Train loss: 0.755, Val loss: 0.846\n",
      "Epoch: 284, Train loss: 0.756, Val loss: 0.846\n",
      "Epoch: 285, Train loss: 0.755, Val loss: 0.846\n",
      "Epoch: 286, Train loss: 0.756, Val loss: 0.846\n",
      "Epoch: 287, Train loss: 0.755, Val loss: 0.845\n",
      "Epoch: 288, Train loss: 0.757, Val loss: 0.845\n",
      "Epoch: 289, Train loss: 0.755, Val loss: 0.845\n",
      "Epoch: 290, Train loss: 0.756, Val loss: 0.845\n",
      "Epoch: 291, Train loss: 0.755, Val loss: 0.844\n",
      "Epoch: 292, Train loss: 0.755, Val loss: 0.844\n",
      "Epoch: 293, Train loss: 0.756, Val loss: 0.844\n",
      "Epoch: 294, Train loss: 0.755, Val loss: 0.844\n",
      "Epoch: 295, Train loss: 0.755, Val loss: 0.843\n",
      "Epoch: 296, Train loss: 0.754, Val loss: 0.843\n",
      "Epoch: 297, Train loss: 0.755, Val loss: 0.843\n",
      "Epoch: 298, Train loss: 0.755, Val loss: 0.843\n",
      "Epoch: 299, Train loss: 0.755, Val loss: 0.842\n",
      "Epoch: 300, Train loss: 0.755, Val loss: 0.842\n",
      "Epoch: 301, Train loss: 0.754, Val loss: 0.842\n",
      "Epoch: 302, Train loss: 0.754, Val loss: 0.842\n",
      "Epoch: 303, Train loss: 0.754, Val loss: 0.841\n",
      "Epoch: 304, Train loss: 0.755, Val loss: 0.841\n",
      "Epoch: 305, Train loss: 0.755, Val loss: 0.841\n",
      "Epoch: 306, Train loss: 0.755, Val loss: 0.841\n",
      "Epoch: 307, Train loss: 0.754, Val loss: 0.841\n",
      "Epoch: 308, Train loss: 0.755, Val loss: 0.84\n",
      "Epoch: 309, Train loss: 0.755, Val loss: 0.84\n",
      "Epoch: 310, Train loss: 0.754, Val loss: 0.84\n",
      "Epoch: 311, Train loss: 0.754, Val loss: 0.84\n",
      "Epoch: 312, Train loss: 0.754, Val loss: 0.839\n",
      "Epoch: 313, Train loss: 0.754, Val loss: 0.839\n",
      "Epoch: 314, Train loss: 0.754, Val loss: 0.839\n",
      "Epoch: 315, Train loss: 0.753, Val loss: 0.839\n",
      "Epoch: 316, Train loss: 0.754, Val loss: 0.839\n",
      "Epoch: 317, Train loss: 0.754, Val loss: 0.838\n",
      "Epoch: 318, Train loss: 0.754, Val loss: 0.838\n",
      "Epoch: 319, Train loss: 0.753, Val loss: 0.838\n",
      "Epoch: 320, Train loss: 0.753, Val loss: 0.838\n",
      "Epoch: 321, Train loss: 0.754, Val loss: 0.838\n",
      "Epoch: 322, Train loss: 0.753, Val loss: 0.837\n",
      "Epoch: 323, Train loss: 0.753, Val loss: 0.837\n",
      "Epoch: 324, Train loss: 0.755, Val loss: 0.837\n",
      "Epoch: 325, Train loss: 0.754, Val loss: 0.837\n",
      "Epoch: 326, Train loss: 0.754, Val loss: 0.837\n",
      "Epoch: 327, Train loss: 0.753, Val loss: 0.837\n",
      "Epoch: 328, Train loss: 0.753, Val loss: 0.837\n",
      "Epoch: 329, Train loss: 0.753, Val loss: 0.836\n",
      "Epoch: 330, Train loss: 0.753, Val loss: 0.836\n",
      "Epoch: 331, Train loss: 0.753, Val loss: 0.836\n",
      "Epoch: 332, Train loss: 0.753, Val loss: 0.836\n",
      "Epoch: 333, Train loss: 0.752, Val loss: 0.836\n",
      "Epoch: 334, Train loss: 0.753, Val loss: 0.836\n",
      "Epoch: 335, Train loss: 0.753, Val loss: 0.836\n",
      "Epoch: 336, Train loss: 0.753, Val loss: 0.835\n",
      "Epoch: 337, Train loss: 0.753, Val loss: 0.835\n",
      "Epoch: 338, Train loss: 0.753, Val loss: 0.835\n",
      "Epoch: 339, Train loss: 0.753, Val loss: 0.835\n",
      "Epoch: 340, Train loss: 0.752, Val loss: 0.835\n",
      "Epoch: 341, Train loss: 0.752, Val loss: 0.835\n",
      "Epoch: 342, Train loss: 0.752, Val loss: 0.834\n",
      "Epoch: 343, Train loss: 0.753, Val loss: 0.834\n",
      "Epoch: 344, Train loss: 0.752, Val loss: 0.834\n",
      "Epoch: 345, Train loss: 0.752, Val loss: 0.834\n",
      "Epoch: 346, Train loss: 0.752, Val loss: 0.834\n",
      "Epoch: 347, Train loss: 0.752, Val loss: 0.834\n",
      "Epoch: 348, Train loss: 0.752, Val loss: 0.833\n",
      "Epoch: 349, Train loss: 0.752, Val loss: 0.833\n",
      "Epoch: 350, Train loss: 0.752, Val loss: 0.833\n",
      "Epoch: 351, Train loss: 0.752, Val loss: 0.833\n",
      "Epoch: 352, Train loss: 0.752, Val loss: 0.833\n",
      "Epoch: 353, Train loss: 0.752, Val loss: 0.833\n",
      "Epoch: 354, Train loss: 0.752, Val loss: 0.833\n",
      "Epoch: 355, Train loss: 0.752, Val loss: 0.832\n",
      "Epoch: 356, Train loss: 0.752, Val loss: 0.832\n",
      "Epoch: 357, Train loss: 0.751, Val loss: 0.832\n",
      "Epoch: 358, Train loss: 0.752, Val loss: 0.832\n",
      "Epoch: 359, Train loss: 0.751, Val loss: 0.832\n",
      "Epoch: 360, Train loss: 0.752, Val loss: 0.833\n",
      "Epoch: 361, Train loss: 0.752, Val loss: 0.833\n",
      "Epoch: 362, Train loss: 0.752, Val loss: 0.834\n",
      "Epoch: 363, Train loss: 0.751, Val loss: 0.834\n",
      "Epoch: 364, Train loss: 0.752, Val loss: 0.835\n",
      "Epoch: 365, Train loss: 0.752, Val loss: 0.836\n",
      "Epoch: 366, Train loss: 0.752, Val loss: 0.836\n",
      "Epoch: 367, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 368, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 369, Train loss: 0.752, Val loss: 0.837\n",
      "Epoch: 370, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 371, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 372, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 373, Train loss: 0.752, Val loss: 0.837\n",
      "Epoch: 374, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 375, Train loss: 0.75, Val loss: 0.837\n",
      "Epoch: 376, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 377, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 378, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 379, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 380, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 381, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 382, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 383, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 384, Train loss: 0.751, Val loss: 0.837\n",
      "Epoch: 385, Train loss: 0.751, Val loss: 0.836\n",
      "Epoch: 386, Train loss: 0.751, Val loss: 0.836\n",
      "Epoch: 387, Train loss: 0.75, Val loss: 0.836\n",
      "Epoch: 388, Train loss: 0.75, Val loss: 0.836\n",
      "Epoch: 389, Train loss: 0.75, Val loss: 0.836\n",
      "Epoch: 390, Train loss: 0.75, Val loss: 0.836\n",
      "Epoch: 391, Train loss: 0.751, Val loss: 0.836\n",
      "Epoch: 392, Train loss: 0.75, Val loss: 0.835\n",
      "Epoch: 393, Train loss: 0.751, Val loss: 0.835\n",
      "Epoch: 394, Train loss: 0.751, Val loss: 0.835\n",
      "Epoch: 395, Train loss: 0.75, Val loss: 0.835\n",
      "Epoch: 396, Train loss: 0.751, Val loss: 0.835\n",
      "Epoch: 397, Train loss: 0.751, Val loss: 0.835\n",
      "Epoch: 398, Train loss: 0.75, Val loss: 0.834\n",
      "Epoch: 399, Train loss: 0.75, Val loss: 0.834\n",
      "Epoch: 400, Train loss: 0.75, Val loss: 0.834\n",
      "Epoch: 401, Train loss: 0.75, Val loss: 0.834\n",
      "Epoch: 402, Train loss: 0.75, Val loss: 0.834\n",
      "Epoch: 403, Train loss: 0.75, Val loss: 0.834\n",
      "Epoch: 404, Train loss: 0.75, Val loss: 0.833\n",
      "Epoch: 405, Train loss: 0.75, Val loss: 0.833\n",
      "Epoch: 406, Train loss: 0.75, Val loss: 0.833\n",
      "Epoch: 407, Train loss: 0.75, Val loss: 0.833\n",
      "Epoch: 408, Train loss: 0.75, Val loss: 0.833\n",
      "Epoch: 409, Train loss: 0.75, Val loss: 0.833\n",
      "Epoch: 410, Train loss: 0.749, Val loss: 0.832\n",
      "Epoch: 411, Train loss: 0.75, Val loss: 0.832\n",
      "Epoch: 412, Train loss: 0.75, Val loss: 0.832\n",
      "Epoch: 413, Train loss: 0.75, Val loss: 0.832\n",
      "Epoch: 414, Train loss: 0.749, Val loss: 0.832\n",
      "Epoch: 415, Train loss: 0.749, Val loss: 0.832\n",
      "Epoch: 416, Train loss: 0.749, Val loss: 0.831\n",
      "Epoch: 417, Train loss: 0.75, Val loss: 0.831\n",
      "Epoch: 418, Train loss: 0.749, Val loss: 0.831\n",
      "Epoch: 419, Train loss: 0.749, Val loss: 0.831\n",
      "Epoch: 420, Train loss: 0.749, Val loss: 0.831\n",
      "Epoch: 421, Train loss: 0.749, Val loss: 0.831\n",
      "Epoch: 422, Train loss: 0.749, Val loss: 0.831\n",
      "Epoch: 423, Train loss: 0.749, Val loss: 0.831\n",
      "Epoch: 424, Train loss: 0.749, Val loss: 0.83\n",
      "Epoch: 425, Train loss: 0.75, Val loss: 0.83\n",
      "Epoch: 426, Train loss: 0.749, Val loss: 0.83\n",
      "Epoch: 427, Train loss: 0.75, Val loss: 0.83\n",
      "Epoch: 428, Train loss: 0.749, Val loss: 0.83\n",
      "Epoch: 429, Train loss: 0.749, Val loss: 0.83\n",
      "Epoch: 430, Train loss: 0.75, Val loss: 0.83\n",
      "Epoch: 431, Train loss: 0.75, Val loss: 0.829\n",
      "Epoch: 432, Train loss: 0.749, Val loss: 0.829\n",
      "Epoch: 433, Train loss: 0.749, Val loss: 0.829\n",
      "Epoch: 434, Train loss: 0.749, Val loss: 0.829\n",
      "Epoch: 435, Train loss: 0.749, Val loss: 0.829\n",
      "Epoch: 436, Train loss: 0.749, Val loss: 0.828\n",
      "Epoch: 437, Train loss: 0.749, Val loss: 0.828\n",
      "Epoch: 438, Train loss: 0.749, Val loss: 0.828\n",
      "Epoch: 439, Train loss: 0.75, Val loss: 0.828\n",
      "Epoch: 440, Train loss: 0.749, Val loss: 0.828\n",
      "Epoch: 441, Train loss: 0.749, Val loss: 0.827\n",
      "Epoch: 442, Train loss: 0.749, Val loss: 0.827\n",
      "Epoch: 443, Train loss: 0.749, Val loss: 0.827\n",
      "Epoch: 444, Train loss: 0.749, Val loss: 0.827\n",
      "Epoch: 445, Train loss: 0.749, Val loss: 0.827\n",
      "Epoch: 446, Train loss: 0.749, Val loss: 0.826\n",
      "Epoch: 447, Train loss: 0.75, Val loss: 0.826\n",
      "Epoch: 448, Train loss: 0.75, Val loss: 0.826\n",
      "Epoch: 449, Train loss: 0.749, Val loss: 0.826\n",
      "Epoch: 450, Train loss: 0.748, Val loss: 0.826\n",
      "Epoch: 451, Train loss: 0.749, Val loss: 0.825\n",
      "Epoch: 452, Train loss: 0.749, Val loss: 0.825\n",
      "Epoch: 453, Train loss: 0.749, Val loss: 0.825\n",
      "Epoch: 454, Train loss: 0.749, Val loss: 0.825\n",
      "Epoch: 455, Train loss: 0.749, Val loss: 0.825\n",
      "Epoch: 456, Train loss: 0.75, Val loss: 0.825\n",
      "Epoch: 457, Train loss: 0.749, Val loss: 0.824\n",
      "Epoch: 458, Train loss: 0.748, Val loss: 0.824\n",
      "Epoch: 459, Train loss: 0.749, Val loss: 0.824\n",
      "Epoch: 460, Train loss: 0.748, Val loss: 0.824\n",
      "Epoch: 461, Train loss: 0.749, Val loss: 0.824\n",
      "Epoch: 462, Train loss: 0.748, Val loss: 0.824\n",
      "Epoch: 463, Train loss: 0.749, Val loss: 0.823\n",
      "Epoch: 464, Train loss: 0.749, Val loss: 0.823\n",
      "Epoch: 465, Train loss: 0.749, Val loss: 0.823\n",
      "Epoch: 466, Train loss: 0.748, Val loss: 0.823\n",
      "Epoch: 467, Train loss: 0.749, Val loss: 0.823\n",
      "Epoch: 468, Train loss: 0.748, Val loss: 0.823\n",
      "Epoch: 469, Train loss: 0.748, Val loss: 0.822\n",
      "Epoch: 470, Train loss: 0.748, Val loss: 0.822\n",
      "Epoch: 471, Train loss: 0.748, Val loss: 0.822\n",
      "Epoch: 472, Train loss: 0.748, Val loss: 0.822\n",
      "Epoch: 473, Train loss: 0.749, Val loss: 0.822\n",
      "Epoch: 474, Train loss: 0.748, Val loss: 0.821\n",
      "Epoch: 475, Train loss: 0.748, Val loss: 0.821\n",
      "Epoch: 476, Train loss: 0.749, Val loss: 0.821\n",
      "Epoch: 477, Train loss: 0.749, Val loss: 0.821\n",
      "Epoch: 478, Train loss: 0.748, Val loss: 0.821\n",
      "Epoch: 479, Train loss: 0.748, Val loss: 0.821\n",
      "Epoch: 480, Train loss: 0.748, Val loss: 0.82\n",
      "Epoch: 481, Train loss: 0.749, Val loss: 0.82\n",
      "Epoch: 482, Train loss: 0.748, Val loss: 0.82\n",
      "Epoch: 483, Train loss: 0.748, Val loss: 0.82\n",
      "Epoch: 484, Train loss: 0.748, Val loss: 0.82\n",
      "Epoch: 485, Train loss: 0.748, Val loss: 0.82\n",
      "Epoch: 486, Train loss: 0.748, Val loss: 0.82\n",
      "Epoch: 487, Train loss: 0.749, Val loss: 0.819\n",
      "Epoch: 488, Train loss: 0.748, Val loss: 0.819\n",
      "Epoch: 489, Train loss: 0.748, Val loss: 0.819\n",
      "Epoch: 490, Train loss: 0.748, Val loss: 0.819\n",
      "Epoch: 491, Train loss: 0.748, Val loss: 0.819\n",
      "Epoch: 492, Train loss: 0.748, Val loss: 0.819\n",
      "Epoch: 493, Train loss: 0.748, Val loss: 0.819\n",
      "Epoch: 494, Train loss: 0.748, Val loss: 0.818\n",
      "Epoch: 495, Train loss: 0.748, Val loss: 0.818\n",
      "Epoch: 496, Train loss: 0.748, Val loss: 0.818\n",
      "Epoch: 497, Train loss: 0.748, Val loss: 0.818\n",
      "Epoch: 498, Train loss: 0.748, Val loss: 0.818\n",
      "Epoch: 499, Train loss: 0.748, Val loss: 0.817\n",
      "Epoch: 500, Train loss: 0.748, Val loss: 0.817\n",
      "Epoch: 501, Train loss: 0.748, Val loss: 0.817\n",
      "Epoch: 502, Train loss: 0.748, Val loss: 0.817\n",
      "Epoch: 503, Train loss: 0.748, Val loss: 0.817\n",
      "Epoch: 504, Train loss: 0.748, Val loss: 0.817\n",
      "Epoch: 505, Train loss: 0.748, Val loss: 0.816\n",
      "Epoch: 506, Train loss: 0.748, Val loss: 0.816\n",
      "Epoch: 507, Train loss: 0.748, Val loss: 0.816\n",
      "Epoch: 508, Train loss: 0.748, Val loss: 0.816\n",
      "Epoch: 509, Train loss: 0.748, Val loss: 0.816\n",
      "Epoch: 510, Train loss: 0.748, Val loss: 0.816\n",
      "Epoch: 511, Train loss: 0.748, Val loss: 0.815\n",
      "Epoch: 512, Train loss: 0.748, Val loss: 0.815\n",
      "Epoch: 513, Train loss: 0.748, Val loss: 0.815\n",
      "Epoch: 514, Train loss: 0.748, Val loss: 0.815\n",
      "Epoch: 515, Train loss: 0.748, Val loss: 0.815\n",
      "Epoch: 516, Train loss: 0.748, Val loss: 0.815\n",
      "Epoch: 517, Train loss: 0.748, Val loss: 0.814\n",
      "Epoch: 518, Train loss: 0.747, Val loss: 0.814\n",
      "Epoch: 519, Train loss: 0.747, Val loss: 0.814\n",
      "Epoch: 520, Train loss: 0.748, Val loss: 0.814\n",
      "Epoch: 521, Train loss: 0.747, Val loss: 0.814\n",
      "Epoch: 522, Train loss: 0.747, Val loss: 0.814\n",
      "Epoch: 523, Train loss: 0.748, Val loss: 0.813\n",
      "Epoch: 524, Train loss: 0.747, Val loss: 0.813\n",
      "Epoch: 525, Train loss: 0.747, Val loss: 0.813\n",
      "Epoch: 526, Train loss: 0.748, Val loss: 0.813\n",
      "Epoch: 527, Train loss: 0.748, Val loss: 0.813\n",
      "Epoch: 528, Train loss: 0.747, Val loss: 0.813\n",
      "Epoch: 529, Train loss: 0.747, Val loss: 0.813\n",
      "Epoch: 530, Train loss: 0.747, Val loss: 0.812\n",
      "Epoch: 531, Train loss: 0.747, Val loss: 0.812\n",
      "Epoch: 532, Train loss: 0.747, Val loss: 0.812\n",
      "Epoch: 533, Train loss: 0.747, Val loss: 0.812\n",
      "Epoch: 534, Train loss: 0.748, Val loss: 0.812\n",
      "Epoch: 535, Train loss: 0.747, Val loss: 0.812\n",
      "Epoch: 536, Train loss: 0.748, Val loss: 0.811\n",
      "Epoch: 537, Train loss: 0.748, Val loss: 0.811\n",
      "Epoch: 538, Train loss: 0.748, Val loss: 0.811\n",
      "Epoch: 539, Train loss: 0.747, Val loss: 0.811\n",
      "Epoch: 540, Train loss: 0.747, Val loss: 0.811\n",
      "Epoch: 541, Train loss: 0.747, Val loss: 0.811\n",
      "Epoch: 542, Train loss: 0.748, Val loss: 0.811\n",
      "Epoch: 543, Train loss: 0.748, Val loss: 0.81\n",
      "Epoch: 544, Train loss: 0.748, Val loss: 0.81\n",
      "Epoch: 545, Train loss: 0.747, Val loss: 0.81\n",
      "Epoch: 546, Train loss: 0.747, Val loss: 0.81\n",
      "Epoch: 547, Train loss: 0.747, Val loss: 0.81\n",
      "Epoch: 548, Train loss: 0.747, Val loss: 0.81\n",
      "Epoch: 549, Train loss: 0.747, Val loss: 0.81\n",
      "Epoch: 550, Train loss: 0.747, Val loss: 0.809\n",
      "Epoch: 551, Train loss: 0.747, Val loss: 0.809\n",
      "Epoch: 552, Train loss: 0.747, Val loss: 0.809\n",
      "Epoch: 553, Train loss: 0.747, Val loss: 0.809\n",
      "Epoch: 554, Train loss: 0.747, Val loss: 0.809\n",
      "Epoch: 555, Train loss: 0.747, Val loss: 0.809\n",
      "Epoch: 556, Train loss: 0.747, Val loss: 0.809\n",
      "Epoch: 557, Train loss: 0.747, Val loss: 0.809\n",
      "Epoch: 558, Train loss: 0.747, Val loss: 0.809\n",
      "Epoch: 559, Train loss: 0.748, Val loss: 0.808\n",
      "Epoch: 560, Train loss: 0.748, Val loss: 0.808\n",
      "Epoch: 561, Train loss: 0.747, Val loss: 0.808\n",
      "Epoch: 562, Train loss: 0.747, Val loss: 0.808\n",
      "Epoch: 563, Train loss: 0.747, Val loss: 0.808\n",
      "Epoch: 564, Train loss: 0.747, Val loss: 0.808\n",
      "Epoch: 565, Train loss: 0.747, Val loss: 0.808\n",
      "Epoch: 566, Train loss: 0.747, Val loss: 0.808\n",
      "Epoch: 567, Train loss: 0.747, Val loss: 0.807\n",
      "Epoch: 568, Train loss: 0.747, Val loss: 0.807\n",
      "Epoch: 569, Train loss: 0.747, Val loss: 0.807\n",
      "Epoch: 570, Train loss: 0.747, Val loss: 0.807\n",
      "Epoch: 571, Train loss: 0.747, Val loss: 0.807\n",
      "Epoch: 572, Train loss: 0.747, Val loss: 0.807\n",
      "Epoch: 573, Train loss: 0.747, Val loss: 0.807\n",
      "Epoch: 574, Train loss: 0.747, Val loss: 0.807\n",
      "Epoch: 575, Train loss: 0.747, Val loss: 0.807\n",
      "Epoch: 576, Train loss: 0.747, Val loss: 0.806\n",
      "Epoch: 577, Train loss: 0.747, Val loss: 0.806\n",
      "Epoch: 578, Train loss: 0.747, Val loss: 0.806\n",
      "Epoch: 579, Train loss: 0.747, Val loss: 0.806\n",
      "Epoch: 580, Train loss: 0.747, Val loss: 0.806\n",
      "Epoch: 581, Train loss: 0.747, Val loss: 0.806\n",
      "Epoch: 582, Train loss: 0.747, Val loss: 0.806\n",
      "Epoch: 583, Train loss: 0.747, Val loss: 0.806\n",
      "Epoch: 584, Train loss: 0.747, Val loss: 0.806\n",
      "Epoch: 585, Train loss: 0.747, Val loss: 0.805\n",
      "Epoch: 586, Train loss: 0.747, Val loss: 0.805\n",
      "Epoch: 587, Train loss: 0.747, Val loss: 0.805\n",
      "Epoch: 588, Train loss: 0.747, Val loss: 0.805\n",
      "Epoch: 589, Train loss: 0.747, Val loss: 0.805\n",
      "Epoch: 590, Train loss: 0.747, Val loss: 0.805\n",
      "Epoch: 591, Train loss: 0.747, Val loss: 0.805\n",
      "Epoch: 592, Train loss: 0.747, Val loss: 0.805\n",
      "Epoch: 593, Train loss: 0.747, Val loss: 0.805\n",
      "Epoch: 594, Train loss: 0.747, Val loss: 0.805\n",
      "Epoch: 595, Train loss: 0.747, Val loss: 0.804\n",
      "Epoch: 596, Train loss: 0.747, Val loss: 0.804\n",
      "Epoch: 597, Train loss: 0.747, Val loss: 0.804\n",
      "Epoch: 598, Train loss: 0.747, Val loss: 0.804\n",
      "Epoch: 599, Train loss: 0.747, Val loss: 0.804\n",
      "Epoch: 600, Train loss: 0.747, Val loss: 0.804\n",
      "Epoch: 601, Train loss: 0.747, Val loss: 0.804\n",
      "Epoch: 602, Train loss: 0.746, Val loss: 0.804\n",
      "Epoch: 603, Train loss: 0.747, Val loss: 0.804\n",
      "Epoch: 604, Train loss: 0.746, Val loss: 0.804\n",
      "Epoch: 605, Train loss: 0.747, Val loss: 0.804\n",
      "Epoch: 606, Train loss: 0.747, Val loss: 0.803\n",
      "Epoch: 607, Train loss: 0.746, Val loss: 0.803\n",
      "Epoch: 608, Train loss: 0.746, Val loss: 0.803\n",
      "Epoch: 609, Train loss: 0.746, Val loss: 0.803\n",
      "Epoch: 610, Train loss: 0.747, Val loss: 0.803\n",
      "Epoch: 611, Train loss: 0.747, Val loss: 0.803\n",
      "Epoch: 612, Train loss: 0.746, Val loss: 0.803\n",
      "Epoch: 613, Train loss: 0.747, Val loss: 0.803\n",
      "Epoch: 614, Train loss: 0.746, Val loss: 0.803\n",
      "Epoch: 615, Train loss: 0.746, Val loss: 0.803\n",
      "Epoch: 616, Train loss: 0.747, Val loss: 0.803\n",
      "Epoch: 617, Train loss: 0.747, Val loss: 0.803\n",
      "Epoch: 618, Train loss: 0.746, Val loss: 0.802\n",
      "Epoch: 619, Train loss: 0.746, Val loss: 0.802\n",
      "Epoch: 620, Train loss: 0.747, Val loss: 0.802\n",
      "Epoch: 621, Train loss: 0.746, Val loss: 0.802\n",
      "Epoch: 622, Train loss: 0.746, Val loss: 0.802\n",
      "Epoch: 623, Train loss: 0.746, Val loss: 0.802\n",
      "Epoch: 624, Train loss: 0.747, Val loss: 0.802\n",
      "Epoch: 625, Train loss: 0.747, Val loss: 0.802\n",
      "Epoch: 626, Train loss: 0.747, Val loss: 0.802\n",
      "Epoch: 627, Train loss: 0.747, Val loss: 0.802\n",
      "Epoch: 628, Train loss: 0.747, Val loss: 0.802\n",
      "Epoch: 629, Train loss: 0.747, Val loss: 0.801\n",
      "Epoch: 630, Train loss: 0.746, Val loss: 0.801\n",
      "Epoch: 631, Train loss: 0.747, Val loss: 0.801\n",
      "Epoch: 632, Train loss: 0.746, Val loss: 0.801\n",
      "Epoch: 633, Train loss: 0.747, Val loss: 0.801\n",
      "Epoch: 634, Train loss: 0.746, Val loss: 0.801\n",
      "Epoch: 635, Train loss: 0.746, Val loss: 0.801\n",
      "Epoch: 636, Train loss: 0.746, Val loss: 0.801\n",
      "Epoch: 637, Train loss: 0.746, Val loss: 0.801\n",
      "Epoch: 638, Train loss: 0.746, Val loss: 0.801\n",
      "Epoch: 639, Train loss: 0.746, Val loss: 0.8\n",
      "Epoch: 640, Train loss: 0.746, Val loss: 0.8\n",
      "Epoch: 641, Train loss: 0.746, Val loss: 0.8\n",
      "Epoch: 642, Train loss: 0.747, Val loss: 0.8\n",
      "Epoch: 643, Train loss: 0.746, Val loss: 0.8\n",
      "Epoch: 644, Train loss: 0.746, Val loss: 0.8\n",
      "Epoch: 645, Train loss: 0.746, Val loss: 0.8\n",
      "Epoch: 646, Train loss: 0.746, Val loss: 0.8\n",
      "Epoch: 647, Train loss: 0.746, Val loss: 0.8\n",
      "Epoch: 648, Train loss: 0.746, Val loss: 0.8\n",
      "Epoch: 649, Train loss: 0.746, Val loss: 0.8\n",
      "Epoch: 650, Train loss: 0.746, Val loss: 0.8\n",
      "Epoch: 651, Train loss: 0.746, Val loss: 0.799\n",
      "Epoch: 652, Train loss: 0.747, Val loss: 0.799\n",
      "Epoch: 653, Train loss: 0.746, Val loss: 0.799\n",
      "Epoch: 654, Train loss: 0.746, Val loss: 0.799\n",
      "Epoch: 655, Train loss: 0.746, Val loss: 0.799\n",
      "Epoch: 656, Train loss: 0.746, Val loss: 0.799\n",
      "Epoch: 657, Train loss: 0.746, Val loss: 0.799\n",
      "Epoch: 658, Train loss: 0.746, Val loss: 0.799\n",
      "Epoch: 659, Train loss: 0.746, Val loss: 0.799\n",
      "Epoch: 660, Train loss: 0.746, Val loss: 0.799\n",
      "Epoch: 661, Train loss: 0.746, Val loss: 0.798\n",
      "Epoch: 662, Train loss: 0.746, Val loss: 0.798\n",
      "Epoch: 663, Train loss: 0.746, Val loss: 0.798\n",
      "Epoch: 664, Train loss: 0.746, Val loss: 0.798\n",
      "Epoch: 665, Train loss: 0.746, Val loss: 0.798\n",
      "Epoch: 666, Train loss: 0.746, Val loss: 0.798\n",
      "Epoch: 667, Train loss: 0.746, Val loss: 0.798\n",
      "Epoch: 668, Train loss: 0.746, Val loss: 0.798\n",
      "Epoch: 669, Train loss: 0.746, Val loss: 0.798\n",
      "Epoch: 670, Train loss: 0.746, Val loss: 0.798\n",
      "Epoch: 671, Train loss: 0.746, Val loss: 0.798\n",
      "Epoch: 672, Train loss: 0.746, Val loss: 0.798\n",
      "Epoch: 673, Train loss: 0.747, Val loss: 0.797\n",
      "Epoch: 674, Train loss: 0.746, Val loss: 0.797\n",
      "Epoch: 675, Train loss: 0.746, Val loss: 0.797\n",
      "Epoch: 676, Train loss: 0.746, Val loss: 0.797\n",
      "Epoch: 677, Train loss: 0.746, Val loss: 0.797\n",
      "Epoch: 678, Train loss: 0.746, Val loss: 0.797\n",
      "Epoch: 679, Train loss: 0.746, Val loss: 0.797\n",
      "Epoch: 680, Train loss: 0.746, Val loss: 0.797\n",
      "Epoch: 681, Train loss: 0.746, Val loss: 0.797\n",
      "Epoch: 682, Train loss: 0.746, Val loss: 0.797\n",
      "Epoch: 683, Train loss: 0.746, Val loss: 0.796\n",
      "Epoch: 684, Train loss: 0.746, Val loss: 0.796\n",
      "Epoch: 685, Train loss: 0.746, Val loss: 0.796\n",
      "Epoch: 686, Train loss: 0.746, Val loss: 0.796\n",
      "Epoch: 687, Train loss: 0.746, Val loss: 0.796\n",
      "Epoch: 688, Train loss: 0.746, Val loss: 0.796\n",
      "Epoch: 689, Train loss: 0.746, Val loss: 0.796\n",
      "Epoch: 690, Train loss: 0.746, Val loss: 0.796\n",
      "Epoch: 691, Train loss: 0.746, Val loss: 0.796\n",
      "Epoch: 692, Train loss: 0.746, Val loss: 0.796\n",
      "Epoch: 693, Train loss: 0.746, Val loss: 0.795\n",
      "Epoch: 694, Train loss: 0.746, Val loss: 0.795\n",
      "Epoch: 695, Train loss: 0.746, Val loss: 0.795\n",
      "Epoch: 696, Train loss: 0.746, Val loss: 0.795\n",
      "Epoch: 697, Train loss: 0.746, Val loss: 0.795\n",
      "Epoch: 698, Train loss: 0.746, Val loss: 0.795\n",
      "Epoch: 699, Train loss: 0.746, Val loss: 0.795\n",
      "Epoch: 700, Train loss: 0.746, Val loss: 0.795\n",
      "Epoch: 701, Train loss: 0.746, Val loss: 0.795\n",
      "Epoch: 702, Train loss: 0.746, Val loss: 0.795\n",
      "Epoch: 703, Train loss: 0.746, Val loss: 0.794\n",
      "Epoch: 704, Train loss: 0.746, Val loss: 0.794\n",
      "Epoch: 705, Train loss: 0.746, Val loss: 0.794\n",
      "Epoch: 706, Train loss: 0.746, Val loss: 0.794\n",
      "Epoch: 707, Train loss: 0.746, Val loss: 0.794\n",
      "Epoch: 708, Train loss: 0.746, Val loss: 0.794\n",
      "Epoch: 709, Train loss: 0.746, Val loss: 0.794\n",
      "Epoch: 710, Train loss: 0.746, Val loss: 0.794\n",
      "Epoch: 711, Train loss: 0.746, Val loss: 0.793\n",
      "Epoch: 712, Train loss: 0.746, Val loss: 0.793\n",
      "Epoch: 713, Train loss: 0.746, Val loss: 0.793\n",
      "Epoch: 714, Train loss: 0.746, Val loss: 0.793\n",
      "Epoch: 715, Train loss: 0.746, Val loss: 0.793\n",
      "Epoch: 716, Train loss: 0.746, Val loss: 0.793\n",
      "Epoch: 717, Train loss: 0.746, Val loss: 0.793\n",
      "Epoch: 718, Train loss: 0.746, Val loss: 0.793\n",
      "Epoch: 719, Train loss: 0.746, Val loss: 0.793\n",
      "Epoch: 720, Train loss: 0.746, Val loss: 0.792\n",
      "Epoch: 721, Train loss: 0.746, Val loss: 0.792\n",
      "Epoch: 722, Train loss: 0.746, Val loss: 0.792\n",
      "Epoch: 723, Train loss: 0.746, Val loss: 0.792\n",
      "Epoch: 724, Train loss: 0.746, Val loss: 0.792\n",
      "Epoch: 725, Train loss: 0.746, Val loss: 0.792\n",
      "Epoch: 726, Train loss: 0.746, Val loss: 0.792\n",
      "Epoch: 727, Train loss: 0.746, Val loss: 0.792\n",
      "Epoch: 728, Train loss: 0.746, Val loss: 0.792\n",
      "Epoch: 729, Train loss: 0.746, Val loss: 0.791\n",
      "Epoch: 730, Train loss: 0.746, Val loss: 0.791\n",
      "Epoch: 731, Train loss: 0.746, Val loss: 0.791\n",
      "Epoch: 732, Train loss: 0.745, Val loss: 0.791\n",
      "Epoch: 733, Train loss: 0.746, Val loss: 0.791\n",
      "Epoch: 734, Train loss: 0.746, Val loss: 0.791\n",
      "Epoch: 735, Train loss: 0.746, Val loss: 0.791\n",
      "Epoch: 736, Train loss: 0.746, Val loss: 0.791\n",
      "Epoch: 737, Train loss: 0.746, Val loss: 0.791\n",
      "Epoch: 738, Train loss: 0.746, Val loss: 0.791\n",
      "Epoch: 739, Train loss: 0.746, Val loss: 0.791\n",
      "Epoch: 740, Train loss: 0.746, Val loss: 0.79\n",
      "Epoch: 741, Train loss: 0.745, Val loss: 0.79\n",
      "Epoch: 742, Train loss: 0.745, Val loss: 0.79\n",
      "Epoch: 743, Train loss: 0.745, Val loss: 0.79\n",
      "Epoch: 744, Train loss: 0.745, Val loss: 0.79\n",
      "Epoch: 745, Train loss: 0.745, Val loss: 0.79\n",
      "Epoch: 746, Train loss: 0.745, Val loss: 0.79\n",
      "Epoch: 747, Train loss: 0.745, Val loss: 0.79\n",
      "Epoch: 748, Train loss: 0.745, Val loss: 0.79\n",
      "Epoch: 749, Train loss: 0.745, Val loss: 0.79\n",
      "Epoch: 750, Train loss: 0.746, Val loss: 0.79\n",
      "Epoch: 751, Train loss: 0.745, Val loss: 0.79\n",
      "Epoch: 752, Train loss: 0.745, Val loss: 0.789\n",
      "Epoch: 753, Train loss: 0.746, Val loss: 0.789\n",
      "Epoch: 754, Train loss: 0.745, Val loss: 0.789\n",
      "Epoch: 755, Train loss: 0.746, Val loss: 0.789\n",
      "Epoch: 756, Train loss: 0.745, Val loss: 0.789\n",
      "Epoch: 757, Train loss: 0.745, Val loss: 0.789\n",
      "Epoch: 758, Train loss: 0.745, Val loss: 0.789\n",
      "Epoch: 759, Train loss: 0.746, Val loss: 0.789\n",
      "Epoch: 760, Train loss: 0.745, Val loss: 0.789\n",
      "Epoch: 761, Train loss: 0.745, Val loss: 0.789\n",
      "Epoch: 762, Train loss: 0.746, Val loss: 0.789\n",
      "Epoch: 763, Train loss: 0.745, Val loss: 0.789\n",
      "Epoch: 764, Train loss: 0.746, Val loss: 0.788\n",
      "Epoch: 765, Train loss: 0.745, Val loss: 0.788\n",
      "Epoch: 766, Train loss: 0.745, Val loss: 0.788\n",
      "Epoch: 767, Train loss: 0.745, Val loss: 0.788\n",
      "Epoch: 768, Train loss: 0.745, Val loss: 0.788\n",
      "Epoch: 769, Train loss: 0.746, Val loss: 0.788\n",
      "Epoch: 770, Train loss: 0.745, Val loss: 0.788\n",
      "Epoch: 771, Train loss: 0.746, Val loss: 0.788\n",
      "Epoch: 772, Train loss: 0.745, Val loss: 0.787\n",
      "Epoch: 773, Train loss: 0.745, Val loss: 0.787\n",
      "Epoch: 774, Train loss: 0.745, Val loss: 0.787\n",
      "Epoch: 775, Train loss: 0.745, Val loss: 0.787\n",
      "Epoch: 776, Train loss: 0.745, Val loss: 0.787\n",
      "Epoch: 777, Train loss: 0.745, Val loss: 0.787\n",
      "Epoch: 778, Train loss: 0.745, Val loss: 0.786\n",
      "Epoch: 779, Train loss: 0.745, Val loss: 0.786\n",
      "Epoch: 780, Train loss: 0.745, Val loss: 0.786\n",
      "Epoch: 781, Train loss: 0.745, Val loss: 0.786\n",
      "Epoch: 782, Train loss: 0.745, Val loss: 0.786\n",
      "Epoch: 783, Train loss: 0.745, Val loss: 0.785\n",
      "Epoch: 784, Train loss: 0.745, Val loss: 0.785\n",
      "Epoch: 785, Train loss: 0.745, Val loss: 0.785\n",
      "Epoch: 786, Train loss: 0.745, Val loss: 0.785\n",
      "Epoch: 787, Train loss: 0.745, Val loss: 0.784\n",
      "Epoch: 788, Train loss: 0.745, Val loss: 0.784\n",
      "Epoch: 789, Train loss: 0.745, Val loss: 0.784\n",
      "Epoch: 790, Train loss: 0.745, Val loss: 0.784\n",
      "Epoch: 791, Train loss: 0.745, Val loss: 0.784\n",
      "Epoch: 792, Train loss: 0.745, Val loss: 0.784\n",
      "Epoch: 793, Train loss: 0.745, Val loss: 0.783\n",
      "Epoch: 794, Train loss: 0.745, Val loss: 0.783\n",
      "Epoch: 795, Train loss: 0.745, Val loss: 0.783\n",
      "Epoch: 796, Train loss: 0.745, Val loss: 0.783\n",
      "Epoch: 797, Train loss: 0.745, Val loss: 0.783\n",
      "Epoch: 798, Train loss: 0.745, Val loss: 0.783\n",
      "Epoch: 799, Train loss: 0.745, Val loss: 0.783\n",
      "Epoch: 800, Train loss: 0.745, Val loss: 0.783\n",
      "Epoch: 801, Train loss: 0.745, Val loss: 0.782\n",
      "Epoch: 802, Train loss: 0.745, Val loss: 0.782\n",
      "Epoch: 803, Train loss: 0.745, Val loss: 0.782\n",
      "Epoch: 804, Train loss: 0.745, Val loss: 0.782\n",
      "Epoch: 805, Train loss: 0.745, Val loss: 0.782\n",
      "Epoch: 806, Train loss: 0.745, Val loss: 0.782\n",
      "Epoch: 807, Train loss: 0.745, Val loss: 0.782\n",
      "Epoch: 808, Train loss: 0.745, Val loss: 0.782\n",
      "Epoch: 809, Train loss: 0.745, Val loss: 0.781\n",
      "Epoch: 810, Train loss: 0.745, Val loss: 0.781\n",
      "Epoch: 811, Train loss: 0.745, Val loss: 0.781\n",
      "Epoch: 812, Train loss: 0.745, Val loss: 0.781\n",
      "Epoch: 813, Train loss: 0.745, Val loss: 0.781\n",
      "Epoch: 814, Train loss: 0.745, Val loss: 0.781\n",
      "Epoch: 815, Train loss: 0.745, Val loss: 0.781\n",
      "Epoch: 816, Train loss: 0.745, Val loss: 0.781\n",
      "Epoch: 817, Train loss: 0.745, Val loss: 0.781\n",
      "Epoch: 818, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 819, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 820, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 821, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 822, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 823, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 824, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 825, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 826, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 827, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 828, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 829, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 830, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 831, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 832, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 833, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 834, Train loss: 0.745, Val loss: 0.78\n",
      "Epoch: 835, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 836, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 837, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 838, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 839, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 840, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 841, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 842, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 843, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 844, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 845, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 846, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 847, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 848, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 849, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 850, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 851, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 852, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 853, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 854, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 855, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 856, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 857, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 858, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 859, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 860, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 861, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 862, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 863, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 864, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 865, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 866, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 867, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 868, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 869, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 870, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 871, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 872, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 873, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 874, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 875, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 876, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 877, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 878, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 879, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 880, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 881, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 882, Train loss: 0.745, Val loss: 0.779\n",
      "Epoch: 883, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 884, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 885, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 886, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 887, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 888, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 889, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 890, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 891, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 892, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 893, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 894, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 895, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 896, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 897, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 898, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 899, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 900, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 901, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 902, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 903, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 904, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 905, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 906, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 907, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 908, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 909, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 910, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 911, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 912, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 913, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 914, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 915, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 916, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 917, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 918, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 919, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 920, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 921, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 922, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 923, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 924, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 925, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 926, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 927, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 928, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 929, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 930, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 931, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 932, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 933, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 934, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 935, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 936, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 937, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 938, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 939, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 940, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 941, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 942, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 943, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 944, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 945, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 946, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 947, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 948, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 949, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 950, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 951, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 952, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 953, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 954, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 955, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 956, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 957, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 958, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 959, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 960, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 961, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 962, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 963, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 964, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 965, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 966, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 967, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 968, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 969, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 970, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 971, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 972, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 973, Train loss: 0.744, Val loss: 0.778\n",
      "Epoch: 974, Train loss: 0.745, Val loss: 0.778\n",
      "Epoch: 975, Train loss: 0.745, Val loss: 0.777\n",
      "Epoch: 976, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 977, Train loss: 0.745, Val loss: 0.777\n",
      "Epoch: 978, Train loss: 0.745, Val loss: 0.777\n",
      "Epoch: 979, Train loss: 0.745, Val loss: 0.777\n",
      "Epoch: 980, Train loss: 0.745, Val loss: 0.777\n",
      "Epoch: 981, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 982, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 983, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 984, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 985, Train loss: 0.745, Val loss: 0.777\n",
      "Epoch: 986, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 987, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 988, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 989, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 990, Train loss: 0.745, Val loss: 0.777\n",
      "Epoch: 991, Train loss: 0.745, Val loss: 0.777\n",
      "Epoch: 992, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 993, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 994, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 995, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 996, Train loss: 0.745, Val loss: 0.777\n",
      "Epoch: 997, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 998, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 999, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 1000, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 1001, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 1002, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 1003, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 1004, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 1005, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 1006, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 1007, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 1008, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 1009, Train loss: 0.745, Val loss: 0.777\n",
      "Epoch: 1010, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 1011, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 1012, Train loss: 0.745, Val loss: 0.776\n",
      "Epoch: 1013, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1014, Train loss: 0.745, Val loss: 0.776\n",
      "Epoch: 1015, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1016, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1017, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1018, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1019, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1020, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1021, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1022, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1023, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1024, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1025, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1026, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1027, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1028, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1029, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1030, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1031, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1032, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1033, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1034, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1035, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1036, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1037, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1038, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1039, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1040, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1041, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1042, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1043, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1044, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1045, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1046, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1047, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1048, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1049, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1050, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1051, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1052, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1053, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1054, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1055, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1056, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1057, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1058, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1059, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1060, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1061, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1062, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1063, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1064, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1065, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1066, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1067, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1068, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1069, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1070, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1071, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1072, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1073, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1074, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1075, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1076, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1077, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1078, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1079, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1080, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1081, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1082, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1083, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1084, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1085, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1086, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1087, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1088, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1089, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1090, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1091, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1092, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1093, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1094, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1095, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1096, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1097, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1098, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1099, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1100, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1101, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1102, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1103, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1104, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1105, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1106, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1107, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1108, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1109, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1110, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1111, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1112, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1113, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1114, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1115, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1116, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1117, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1118, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1119, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1120, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1121, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1122, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1123, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1124, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1125, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1126, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1127, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1128, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1129, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1130, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1131, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1132, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1133, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1134, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1135, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1136, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1137, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1138, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1139, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1140, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1141, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1142, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1143, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1144, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1145, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1146, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1147, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1148, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1149, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1150, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1151, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1152, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1153, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1154, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1155, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1156, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1157, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1158, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1159, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1160, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1161, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1162, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1163, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1164, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1165, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1166, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1167, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1168, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1169, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1170, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1171, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1172, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1173, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1174, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1175, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1176, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1177, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1178, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1179, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1180, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1181, Train loss: 0.744, Val loss: 0.771\n",
      "Epoch: 1182, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1183, Train loss: 0.744, Val loss: 0.772\n",
      "Epoch: 1184, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1185, Train loss: 0.744, Val loss: 0.773\n",
      "Epoch: 1186, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1187, Train loss: 0.744, Val loss: 0.774\n",
      "Epoch: 1188, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1189, Train loss: 0.744, Val loss: 0.775\n",
      "Epoch: 1190, Train loss: 0.744, Val loss: 0.776\n",
      "Epoch: 1191, Train loss: 0.744, Val loss: 0.777\n",
      "Epoch: 1192, Train loss: 0.744, Val loss: 0.778\n",
      "Epoch: 1193, Train loss: 0.744, Val loss: 0.779\n",
      "Epoch: 1194, Train loss: 0.744, Val loss: 0.78\n",
      "Epoch: 1195, Train loss: 0.744, Val loss: 0.781\n",
      "Epoch: 1196, Train loss: 0.744, Val loss: 0.782\n",
      "Epoch: 1197, Train loss: 0.744, Val loss: 0.783\n",
      "Epoch: 1198, Train loss: 0.744, Val loss: 0.784\n",
      "Epoch: 1199, Train loss: 0.744, Val loss: 0.784\n",
      "Epoch: 1200, Train loss: 0.744, Val loss: 0.785\n",
      "Epoch: 1201, Train loss: 0.744, Val loss: 0.786\n",
      "Epoch: 1202, Train loss: 0.744, Val loss: 0.787\n",
      "Epoch: 1203, Train loss: 0.744, Val loss: 0.787\n",
      "Epoch: 1204, Train loss: 0.744, Val loss: 0.788\n",
      "Epoch: 1205, Train loss: 0.744, Val loss: 0.788\n",
      "Epoch: 1206, Train loss: 0.744, Val loss: 0.789\n",
      "Epoch: 1207, Train loss: 0.744, Val loss: 0.789\n",
      "Epoch: 1208, Train loss: 0.744, Val loss: 0.79\n",
      "Epoch: 1209, Train loss: 0.744, Val loss: 0.79\n",
      "Epoch: 1210, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1211, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1212, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1213, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1214, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1215, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1216, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1217, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1218, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1219, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1220, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1221, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1222, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1223, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1224, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1225, Train loss: 0.744, Val loss: 0.793\n",
      "Epoch: 1226, Train loss: 0.744, Val loss: 0.793\n",
      "Epoch: 1227, Train loss: 0.744, Val loss: 0.793\n",
      "Epoch: 1228, Train loss: 0.744, Val loss: 0.793\n",
      "Epoch: 1229, Train loss: 0.744, Val loss: 0.793\n",
      "Epoch: 1230, Train loss: 0.744, Val loss: 0.793\n",
      "Epoch: 1231, Train loss: 0.744, Val loss: 0.793\n",
      "Epoch: 1232, Train loss: 0.744, Val loss: 0.793\n",
      "Epoch: 1233, Train loss: 0.744, Val loss: 0.793\n",
      "Epoch: 1234, Train loss: 0.744, Val loss: 0.793\n",
      "Epoch: 1235, Train loss: 0.744, Val loss: 0.793\n",
      "Epoch: 1236, Train loss: 0.744, Val loss: 0.793\n",
      "Epoch: 1237, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1238, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1239, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1240, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1241, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1242, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1243, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1244, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1245, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1246, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1247, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1248, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1249, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1250, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1251, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1252, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1253, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1254, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1255, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1256, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1257, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1258, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1259, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1260, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1261, Train loss: 0.744, Val loss: 0.792\n",
      "Epoch: 1262, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1263, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1264, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1265, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1266, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1267, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1268, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1269, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1270, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1271, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1272, Train loss: 0.744, Val loss: 0.791\n",
      "Epoch: 1273, Train loss: 0.744, Val loss: 0.791\n",
      "Early stopping!\n",
      "Folder exists\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explanation Algo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "item=test_x[0].reshape(1,-1,6)\n",
    "shape=item.shape\n",
    "_item=  torch.from_numpy(item).float()\n",
    "model.eval()\n",
    "y_target= model(_item).detach().numpy()\n",
    "#y_target = torch.nn.functional.softmax(model(_item)).detach().numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from TSInterpret.InterpretabilityModels.Saliency.SaliencyMethods_PTY import Saliency_PTY\n",
    "int_mod=Saliency_PTY(model, train_x.shape[-1],train_x.shape[-2], method='IG', mode ='time')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-06-14 09:41:14.732760: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jacqueline/.local/share/virtualenvs/TSInterpret-NXJYnQDU/lib/python3.7/site-packages/cv2/../../lib64:\n",
      "2022-06-14 09:41:14.732778: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(test_x[0,:,:].shape)\n",
    "np.argmax(y_target,axis=1)[0] "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(100, 6)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "exp=int_mod.explain(np.array([test_x[0,:,:]]),labels =1 ,TSR = True)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32585/741997550.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mTSR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/jacqueline/Data/TSInterpret/TSInterpret/InterpretabilityModels/Saliency/SaliencyMethods_PTY.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(self, item, labels, TSR)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTSR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#TODO differntiat between CNN and LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumTimeSteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mfeatureMask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumTimeSteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureMask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "exp.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "int_mod.plot(np.array([test_x[0,:,:]]),exp)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Saliency_PTY' object has no attribute 'mode'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31847/1520502381.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mint_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/jacqueline/Data/TSInterpret/TSInterpret/InterpretabilityModels/FeatureAttribution.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, item, exp, figsize, heatmap, normelize_saliency, save)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m#index= np.where(np.any(item))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'time'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time mode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Saliency_PTY' object has no attribute 'mode'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "int_mod.plot(np.array([test_x[0,:,:]]),exp, heatmap = True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time mode\n",
      "(1, 100, 6)\n",
      "(1, 6, 100)\n",
      "(100, 6)\n",
      "(6, 100)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit ('TSInterpret-NXJYnQDU': pipenv)"
  },
  "interpreter": {
   "hash": "16db99a3fba429d86cb96b96f3cee6d3141a37b91b1d019974802969710b6701"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}